['feat(readme): add coveralls badge to readme. closes #728summary of changes:adds coveralls badge', 'add coveralls badge to readme. addresses #729 summary of changes: add the badge!', 'it was done. so i just deleted!', 'gkatsev is there any way to run the `coveralls` test locally? as i said. i have my own browserstack credentials but if i attempt to run it i get the following output:```running "coveralls:all" (coveralls) task>> no src files could be found for grunt-coverallswarning: task "coveralls:all" failed. use --force to continue.aborted due to warnings.```just a little information on why we\'re wanting this feature: this has great value. especially for broadcasters (we are a news station and are working to integrate this player as our new video player) who. by law. have to provide closed captions for live streams. which is currently done through rtmp in our system.', 'lol go home coveralls you so drunk', 'should running tests with coverage fail if coveralls command fails?. i have run into issues today where my tests pass. but coveralls is down. should this actually fail the build or is there a better way of handling an issue such as this?', "that's an interesting question.i'm not sure of the answer.  my general tendency is to favor failing if anything didn't go as expected. since otherwise you'll probably not notice. and then be surprised later.you can always re-try the build later when coveralls comes back. right?  maybe this could be a cli option or something?", 'sure. retrying is no problem and i guess every developer will have his or her own preference on it? was just curious. i may take a look into adding an option in the future. but see no reason for this to stay open. thanks for your response!', "wait. so. coveralls can figure out the location without the `coveralls_repo_token`??  that's awesome. i had no idea!is it enough to check if `process.env.travis === '1'` then?  how does it figure it out?", '> i still find the coveralls_repo_token support usefulyeah. sure. it\'s presence should indicate you want it to upload. but it would be nice if that wasn\'t the only way to trigger it. use cases to cover:- the build is running on travis. but they don\'t want coverage uploaded (i.e. not using coveralls).- the build is not on travis. but they do want coverage uploaded. the presence of `coveralls_repo _token` is sufficient here.ideally you could reuse the same test script on travis and local dev machines. i\'ve never had a use for uploading to travis from my local machine. so i definitely want `tap` to detect the environment. perhaps the best way to handle this would be to add a `--coveralls-if=travis` (a string flag that conditionally uploads if the specified environment variable is set). `coveralls_repo_token` continues to automatically trigger an upload.> i thought that travis always ran tests with the environment vars you set?"secure" environment variables are not available to pr\'s. otherwise i could just submit a pr with `console.log(process.env.xxx)` to get your token', "there's basically no downside to uploading to coveralls from ci.the hazard i could see would be if a repo isn't set up with coveralls. what will the failure look like?", "that is why i marked it as wip and said that it is only a first step.my concerns was not only about publishing process.you may take a note that this approach doesn't involve long cross linking process that was happen with lerna bootstrap. here we have native module resolution mechanism.script to run tests was removed only because it was used lerna to run and coveralls is not. tests itself not gone. of course tests must be restored.i will prepare a list on what must be restored and improved. and i hope you guys could help with it. really i see a big improvements we can get with this approach.", 'change coveralls token', 'lgtm. waiting for coveralls', "while it's interesting to see the current coverage of the cli code base. i'm not going to take this pull request. this is because. in my judgment. i don't believe that advertising the coverage level is currently useful or actionable for the cli's users. coverage has been at around 85% pretty much since we finished fixing up the test suite to run under travis. and that's not super great. this is even despite the fact that we have several thousand individual test cases in the test suite. and pretty extensive integration test coverage.the team insists that new changes come with tests. even if writing them is fairly complex. this process frequently needs a lot of support from the team. because npm's internal architecture is unwieldy in a way that makes writing traditional unit tests difficult - you either have to do a significant amount of redesign of the relevant cli code. or you end up having to do fairly tricksy mocking within the module cache. also. as a cli tool.  a lot of the tests are written to drive npm from the command line. which makes the test suite slow (and. because a lot of them end up doing similar things. makes it tough to safely parallelize tests. as you suggest in #14431).because writing good tests for npm requires an intimate familiarity with the cli's internals. the core team pretty much needs to be in the loop to review any sizable changes to the test suite. also. given the idiosyncrasies of the code base (which the team is working diligently to reduce over time). new tests need a certain amount of vetting to ensure that they're testing the right things. this makes improving the coverage of the test suite a slow. incremental process. and one that requires a lot of time and attention from the core contributors.as a result. the coverage number is stubbornly slow to increase. and the project isn't in a place where we can really delegate to process of improving that coverage (which is not to say that the team wouldn't _tremendously_ appreciate it if contributors took the initiative to write the missing tests - we would :heart:love:heart: this. were it to happen). as such. having a crappy coverage number at the head of the readme would be more of a discouraging reminder of how many different ways our time and attention are split right now more than any kind of encouragement. we do very much care about the direction the coverage arrow is pointing. but the actual number itself is less interesting / helpful right now.thanks for your time and attention. and please. if you are interested in helping us get the coverage number up. by all means contribute new tests! we're happy to work with you. time permitting. to understand the bits of the architecture that make it challenging to write tests. and we appreciate all help in making the code easier to test. see #14431 for more thoughts on switching test frameworks.", "> if coverage isn't a concern. then why have it at all? it's either important. or not important.it's important to the cli team. but maybe not so important or relevant to people not on the team. is what i'm saying.", "as to the rest. improving the contributors' documentation is high on my list of things to do. but so are many other things. i'll try to get to that before the end of the year. if at all possible. it's absolutely important.", 'bump.despite #2778. coveralls is still not working.', 'mxstbr no problem. excited for the move to jest/enzyme!', "hmm. looks like coveralls isn't picking up the error handling code as being covered. probably because it is throwing an exception. not sure how to work around that.", "don't worry about coveralls. you provided plenty of test coverage - the fact that you figured out how to add the the documentation test is impressive. coveralls is just looking at unit test coverage which isn't necessarily the most important to me.", 'added some tests to comfort coveralls.']
["you're right that's odd. as-cii did we miss something when we pulled #91?anyway. artemave this looks good. i'd be happy to merge a pr. i wonder if we could eliminate the `firstrun` variable though? the `ready-to-show` event should fire just once initially. not on subsequent reloads i believe (we can use `.once` instead of `.on` to express this expectation more clearly. too) so we might not need it... (but i can look at that later). also. let's use the `mocha-` prefix for the event name. i.e.. `mocha-ready-to-start` or something like that?)", "trigger 'mocha-start' on page reload. otherwise tests only run once on the initial window open and nothing happens on page reload.fixes #96", 'create mocha test for example on in readme', 'created mocha test for title and author. issue #49', 'put all mocha tests in spec folder', 'moved mocha test to spec folder. issue #53', "the issue is that you are trying to read it with english grammar. when it was written like that so that it's read as a hierarchy.this is best showcased by using mocha's spec reporter.`mocha -r spec`", 'hah. well that was easy. thanks for the pointer on where to look for code changes! saved me several hours of dissecting the code base.pull request submitted with an extra few test cases. mocha output:```$ mocha  ........................................................  .........................................................  .........................  138 passing (3 seconds)```', "can we have the readme change in a separate pull request?i think this is probably the wrong place to teach people to use `git` but also. it's simpler than you make out (and more consistent across other node.js modules).install dependencies:```npm install```run tests:```npm test```mocha gets automatically installed locally and therefore becomes available to npm's scripts (even though it won't work if you just type `mocha`).", "> chai's should/expect syntax will not work in all browsers as it depends on es5 getters/setters.ah okay. so that is how `expect(blah).to.be.null` works without a function call at the end! i always thought that it was just returning class instances... my bad.> the tests themselves are written with the mocha framework which has a tap reporter.> > chai is an assertion library and does not know anything about the state of tests. in an over-simplification. it throws semantic errors to be interpreted by a testing framework (if present).ah gotcha. so this pull request is just about getting the test suite for chai working in testling. not chai itself. so for myself. i'd just have to make sure whatever test runner i use supports tap. got it.---so all in all. if we want ie support. we got to look elsewhere. or do something radical like rewriting chai to use function calls instead of getters/setters. so `expect(blah).to().be().null()`", 'should "foo".should.be.djlksajdkla throw an error?. at the moment. all of the tests using invalid assertions like `"foo".should.be.djlksajdkla`  silently succeed (i\'m using karma + mocha). is this expected behaviour?', 'unfortunately yes... see #147', "i ran node's mocha which passes the tests but i see travis balks on the browser tests. i'm on windows so i cannot run make to verify this (no grunt love here? :).", 'actually travis/make is configured to handle all the build output files itself and you should not be committing them in prs. i think mocha or mocha-phantomjs is having issues... we had to downgrade our mocha recently due to incompatibility but perhaps it has resurfaced. i will look into it further.', "a) if you are referring to the visual display (which i am assuming you are). this is a problem with mocha's object diff display mechanics ... likely not displaying `undefined` properties. you would search through [visionmedia/mocha](/visionmedia/mocha) issue queue or open one there with this report.b) if you are referring to the fact that this assertion should have passed. it is correct in not passing as you have a `innererror` property in one of the objects but not the other. that works as expected.closing this issue as it is either unrelated to chai (a) or works as expected (b).", "bridgear thought i'd try out the `--cache` flag for performance. the actual important update is getting rid of:`./node_modules/.bin/_mocha`. the new library we're using for spawning doesn't like absolute paths on windows.", 'are you using mocha as the test runner? does the issue still happen if you try using the built-in runner?', 'i am using mocha as the test runner. set in the `nightwatch.json` file (copied down below). and i\'m using `browser.expect()` and it\'s still not showing up in the total count when it fails. is that what you mean by the built-in runner?``` json{..."test_runner" : {    "type" : "mocha".    "options" : {      "ui" : "bdd".      "reporter" : "list"    }...}```', 'unable to run with mocha. i\'ve narrowed my test case down to:```var neo4j = require("neo4j");var db = new neo4j.graphdatabase(":7474");db.query("match (n) optional match (n)-[r]->() delete n. r". {}. function(err. res) {    console.log("response");});console.log("done");```when run with `node test/test.js` it logs two lines of output. when run with `mocha` it only logs the latter.any help is appreciated!', "need to do it async style. it's not waiting for the test to finish.", "wfreeman what's a good practice for doing that with mocha?", 'never mind. i found the done() parameter. thanks for the help!', 'add ability to use mocha --watch flag without constantly file overwrite. for now if tests are running with --watch flag. new report overwrites old one every time. even if no changes was made in tests. it would be great if you add an ability to watch file changes and compose new report only if there was some.', "that's just how the `--watch` flag works. it scans for changes to any js file and then reruns your tests. thus regenerating any reports. it's not behavior specific to mochawesome and not something that can or should be changed.", "it's a while since i looked at this but i suspect jasmine doesn't provide the option to programmatically skip tests. this was only added relatively recently in mocha which is why you needed to upgrade for your other package.", "calvinmetcalf then there's definitely an error.  every single one of those tests times out instead of showing an error in mocha.", 'use gulp-plumber for catching jscs error. like catching mocha error. if jscs raise error. then plumber catches this.', 'i had jshint errors when building a release. i disabled the jshint check. built the library and then tested in our project that uses requirejs and it works. here are the errors i received with `grunt release`:```running "jshint:grunt" (jshint) tasklinting gruntfile.js...error[l142:c36] script url.  grunt.registertask("test-unit". ["mocha:unit"]);[l144:c35] script url.  "test-build". ["concat:test". "mocha:exportsamd". "mocha:exportsglobal"][l144:c55] script url.  "test-build". ["concat:test". "mocha:exportsamd". "mocha:exportsglobal"]```', "thanks for the input! i'll see about taking a crack at it sometime next week if i can make the time. it does spawn a process to run the tests within electron. but mocha is only required within that process. i for the hell of it tried manually requiring in coffee-coverage directly where mocha would expect it to be. but still get 0%. i'm sure i'll come up with something.", 'this turned out to be an issue on electron mochas end. will be putting up a pr there. thanks again.', 'michapixel i think that is exactly what i posted in pr #1012 a couple days ago. it breaks the mocha/travis test.', 'mikermcneil did you read what fyockm wrote about the mocha-test?', "hmm. when i run `gulp browsertest` or even `mocha`. as that is what it looks like travis-ci is doing. locally. the tests complete successfully. perhaps i don't have something configured properly for travis.", ':+1: hard to understand where we call this --v8 flag...what happen if you simply run `mocha .`? the same result?', 'we dont - its in the npmwill check `mocha .`', 'i usually use:```> cd test> mocha .```i will ask mathiasrw about this `npm test` problem', 'update "ember-mocha" and locked dependencies', "code: ack.mocha: 3164 passing (5s)index.html: passes: 3136 failures: 0let's wait for another devs ack and i'll merge", 'agreed. pageobjects are part of test design strategy and wd is just a browser driver component. wd  is meant to be used with other testing frameworks. some framework on top of mocha or jasmine would be the proper candidate.', 'update mocha integration example. mocha-as-promised is deprecated', "it's in mocha. just need to remove the require.", "you seem to have installed `types/mocha`. and it's typings are conflicting with `types/jasmine`. uninstall one of them and you should be fine.", 'timing out today. i\'ve run the "mocha rest" tests in the tests folder with valid credentials. and it just hangs and eventually times out. i\'m not finding any indications of problems on twitter\'s end. but absolutely no love for mtwitter today. i wonder if they got stricter about something today?', 'it works here. does this still happen on your end?', 'if you go in the tests and increase or even remove the timeouts. it might help. i run the tests on "clean" credentials (so there\'s zero rate limits. zero interference from other apps). and from a fairly well-connected box. and i set timeouts mostly because otherwise travis ci just timeouts the entire thing if one get test takes a long time (and we want the rest of the results. too. right?).the way mtwitter works. if it re-queues the requests they\'ll generally run **minutes** or more later. thus blowing out of the timeouts. regarding other libraries:- if you care about getting the content. no matter what time it takes. that how mtwitter is designed. (it\'s not _fully_ there yet. though.)- if you prefer to not get some content. and have the library get back to you (e.g. with an error) as soon as possible. you should either use other libraries. or specify the `queue: false` option.', '> failing pretty quickly (a few seconds).the timeouts are set to 10s per test. and here requests complete within about half that.', 'share src property to all target?. most of the time when using `mochatest:test` and `mochatest:coverage` (even travis one) the `src` prop points to the same location.it would be nice if we could define the option at the task level as a default value for all targets.', 'thanks for the update', 'test coverage value in grunt task. is there a way to have the overall coverage value reported to the grunt task so that rather than looking like this:```running "mochatest:coverage" (mochatest) taskdone. without errors.```it would look like this:```running "mochatest:coverage" (mochatest) task    93% overall coveragedone. without errors.```or even better. pass / fail based on threshold and allow the threshold to be set within the gruntfile rather than via `travis-cov`.```running "mochatest:coverage" (mochatest) task    73% overall coverage: failedwarning: task "coverage" failed. use --force to continue.aborted due to warnings.```', 'the mocha `html-cov` reporter just generates html output which is why i usually capture it to a file (it would be pretty unreadable in the console). it is the `travis-cov` reporter that prints and checks the coverage level. neither of these components are maintained in this project. the `html-cov` reporter is part of the `mocha` project and the `travis-cov` reporter comes from the `travis-cov` project.i have to admit i am struggling to see the problem you have with using `travis-cov`', "it's true that if i fully integrated blanket and travis-cov i could probably provide the same functionality out of the box but it would be a change in direction/focus for this project... i'll continue to give it some thought. but no promises ;)", "sorry for the delay in looking at this. the reason is you didn't include the testem script: `/testem.js`. you should set up a path for it and then load it right after mocha and then it should go.", 'require.js addition to the "mocha+chai" framework.. ideally i\'d like to add this capability to all of the views. i think this would require a requirejs section in the config file though so that mustache placeholders can be used in the views..is there an irc channel or google group for discussion?', 'hmm. i\'d don\'t like the direction this is going. so i am not going to take this patch "as is". mocha. chai. sinon. require.js. and other test frameworks. some of these are mutually exclusive but some aren\'t. we need a more flexible way to specify these libraries.', 'okay. i did reproduce it. and you did make me go "wtf?". the issue is this: you have to insert the testem script `/testem.js` directly below the mocha script. and _not_ after you call `mocha.run()`.', "i am done with the refactoring now. take a look at `lib/ci/test_reporter.js` if anyone wants to take a crack at it. i won't be able to help until next week. i'd like to have a command line option `-r` like mocha to specify the reporter to run. obviously only relevant for ci mode.", 'i cleanup in `before` hooks personally. that way even if you have a fatal error that mocha cant handle you just end up doing it next run', 'update mocha.css. add "z-index: 1;" to #mocha-stats div so it lives above the rest of the output (makes sure you can always click on the "passes"/"fails" links at the top of the page) .', 'change in xunit reporter in test case time when the test fails. if we use xunit format as the mocha reporter. when the test case fails. the time attribute in test case will be nan. so when sonar wants to parse this xunit report. it raises parse exception which means it can not parse nan value of time.  so we replaced part related to create time value of test case from :    time: test.duration / 1000to :         time: test.duration ? test.duration / 1000 : 0', "there's a ton of ways to handle this. depending on if you want the data to stream or not etc. so i'd like to leave this out. people can just tap into the runner like mocha-cloud does", 'add a feature to optionally direct the test output to a file.. when using mocha from within a framework like grunt. it can be useful to direct the test output to a file. for example. when using the xunit reporter. then all of the output from various test suites could be collected and parsed after the fact.the implementation creates a new output library and changes all of the test reporters to call output.log or output.write instead of directly calling console.log or process.stdout.write.it also adds a command line option "-o <file>" to enable the redirection.also added a test rule to the makefile that lists all of the reporters and verifies that the output is properly generated when the option is enabled.', 'try starting bash.exe as a login. interactive shell. in the console settings. set the shell property to something like:c:\\cygwin\\bin\\bash.exe --login -ithis should make bash behave exactly as in the default cygwin terminal', '+1 commonjs promises/a should be first-class citizens in mocharstacruz your helper eats the correct error location. how about``` coffee-scriptpt = (fn) -> (done) -> fn().then((-> done()). done)```this is basically what mocha-as-promised does. but it does it on everything.', 'this would be awesome. not just to redirect to files. but to any stream. for example. we\'re trying to stream the result of our test suite to a browser and now the only way it to spawn a child process. which is kind of a lot of work just to "hijack" the stdout!', "why not just redirect stdio? seems like a lot of hacky-ish changes for little gain. julien51 i wouldn't consider that bad practice at all personally", "but if we redirect stdio that will apply to all other stdout statement in our app. which has a lot more than mocha in it :( ok. we'll try that then.", "oops -- sorry about that - i didn't intend to close the request.", "--compilers only compile specs. should also compile source. i'm running mocha like this``` bashmocha --reporter spec --compilers coffee:coffee-script --require should```i keep my specs in ./test and my source in ./src. both are coffeescript. when i run mocha. the specs are compiled correctly. but the source seems cached or not run through the coffeescript compiler at all. i can verify this by introducing an error in the source code and re-run the tests. but they will still pass.funny side note: when using `--watch test`. the source files are also watched. even though they are not in the test directory. but they are still not compiled before the specs are run.", "mocha is awesome. visionmedia you may not think promises are also awesome. but a lot of mocha's users do and need to test their promise based code. please provide better extensibility than the black magic required in mocha-as-promised. or add first class support for promises to mocha.", "there are also cases where you can't control what's written to stdout by third party libraries that you include in your tests. any random output mixed in with an xunit report will break parsers trying to read it.this change would be immensely useful for feeding test results into ci systems.", "in general i don't really think third-party libs should even write to stdio. that's generally bad practice. with some exceptions of course :d as for debugging with console.log()s you could always just write to stderr instead", "nodeunit-like expect assertions. nodeunit's expect feature:> expect(amount) - specify how many assertions are expected to run within a test. very useful for ensuring that all your callbacks and assertions are run.does indeed seem useful. if `mocha` has developer docs i'd see about hooking this in as a plugin.", 'expose the browser shim process as mocha.process. expose for mocha phantomjs to use.', 'no sorry', "definitely not mocha. not sure what's going onthere sorry", "if we do something similar i'd definitely prefer to just pass an arbitrary output `stream`", "julien51 you'd want to run mocha in a sub process for isolation anyway. so reading from stdio there is pretty natural imo. no reason to dump it to a file (you could just redirect anyway)", "superstructor mocha works just fine with promises. especially with generators around the corner this won't matter at all. if anything the fact that promises are annoying here. illustrates that they are annoying :p", "we can't do this sort of thing since we dont couple with any assertion library. but it's pretty easy to do little counters if you need to and do `assert(n == 3)` for callback checks etc. pretty much the same thing maybe an extra line or two", "visionmedia yeah i know. and this is what i've been doing for testing streams but such tests are verbose enough. and every little line added is getting painful.i just need to find a way to make a nice little util that does this automatically. but that requires knowing the internals of mocha and an assertion lib. and hope i can make them cooperate. right now mocha api/internals aren't documented so not likely to do this soon...", "i originally started down this road where each reporter would get to the output stream through the base class. the problem is that a number of reporters emit output from callbacks. obviously we could bind 'self' before calling the output but it seemed that using a module would be simpler than forcing callers to go through the reporter instance.however. as jbowes requested it would be trivial to also pass the stream to the reporter. i will work on a patch to change it to do this as well.", 'visionmedia thanks for the references', "thanpolas not quite. if the promise fulfills with a value it will call done(value) which makes mocha complain. i'm not sure what you mean by `partial`?", "thanpolas oh i see. yes that works. you could create `ok` and `nok` wrappers for done that do the right thing. but the point was that it's much nicer to use mocha-as-promised :grinning:", 'updated the branch as requested.of course. by changing the contract of the reporter constructor. any reporters defined outside of the source tree will need to be modified accordingly. but then they will have simpler access to the output stream.', 'visionmedia is there a global `beforeeach` and `aftereach` hook in `mocha`? my thinking is that if i can have mocha use a plugin that hooked on like that. i would have the `expect` function setup for all my tests as long as `mocha` is run with that plugin. i.e. on the cli via `--require`.that would be ideal.', 'yup. root-level ones outside of describes() are attached to the root suite named `""`', 'npm test command failing due to mocha throw err. investigating what the cause is.', 'updated mocha link.', 'no. there is currently no nicely boxed-up way to do this. if i were going to try to do something like this. i would probably try to implement a test-only ipc listener in the main process that can receive messages from the renderer processes and run code that simulates clicking on menu items.`tests/ember-electron/main.js` is used in place of `ember-electron/main.js` when running `ember electron:test`. so you can put test-only main process code there. if you register an ipc listener for some kind of an event that you can send from your tests specifying the menu item you want clicked. then the main process code could call `menu.getapplicationmenu()`. look up the menu item specified by the renderer process. make sure it\'s enabled or whatever other validation logic you want. and then invoke its click handler manually.so you\'d still have to do the work to implement the "click this menu item for me" protocol. and the logic to look up and validate the menu item and call its click handler.i haven\'t actually implemented this myself. but that\'s the best way i can currently think of to achieve this inside ember tests. there might be a better way to achieve what you want using `electron-mocha`. but there isn\'t a good way to run any ember code under `electron-mocha` now. so you\'d really just be testing the main process.', 'add a create-e2e command. add a create-e2e command to create the e2e folder and the init.js. mocha.opts and myfirsttest.spec.js files inside it automatically', 'are you running ```embark test``` or ```mocha test/ --no-timeouts``` with the no-timeouts option?', "i'm unit testing the framework itself via `grunt mochatest`", "i would like to write-up a proposal for generators that can easily consume or build-on other generators. i feel like this is parallel to the unit-testing story as we ideally want generator-mocha and so on to be able to work more closely with framework generators.people want to be able to yo chromeapp angular karma and so on with it just working. right now that's far off from what is capable of doing cleanly.", "spurious test failures due to weird mocha supports. also. it's not reallyascoltatori responsibilityto do it. normally. you might want a domain foreach subscriber. not a general one.", 'same issue for me. running mocha-phantomjs on an elcapitan osx:```> mocha-phantomjs ./test/client/index.htmlphantomjs terminated with signal sigkillnpm err! test failed.  see above for more details.```', "es6 code doesn't work?. i'm having some issues running my tests that contain es6 code? they run just fine if i open up the `test.js` file in say chrome where v8 is definitely ready to handle es6 but not in the interpreter packaged with `phantomjs`. is there anyway to make `mocha-phantomjs` compatible with es6 besides pre-`babelify`ing the test.js file?", 'lgtm. :+1:', 'is there a way to use the mocha --compiler flag while running through mocha-phantomjs? this would make it simple to use babel. as with other mocha code:`mocha --compilers js:babel-core/register`', "that's for when mocha runs in node.js. mocha runs in the browser in phantomjs with `mocha-phantomjs`. if you want to bring in babel on the client side and compile your code there. that works.", 'yup that works. #hipstertax (i pay it too sometimes :)', "trigger method optimizations. this pr adds a special internal function``` jsmarionette._triggermethod```how should i benchmark this. the only benchmark i've used so far is the reported running time of the test-suite. according to `mocha` post these changes the tests run `300ms - 400ms` faster (with a couple extra tests). note: `triggermethod` is called 7790 times in the specsfeel free to reject as this really dirties the code. but i feel it may be worth it for such a hot function", 'if `_triggermethod` allows passing a context does this then remove the need for `triggermethodon`?', 'megawac. want to add jshint exceptions to the top of the file so that travis passes?', 'hey megawac. i just rebases master onto minor. mind doing a quick rebase and re-push.', "duncanbeevers - we're investigating. but good news. this has nothing to do with your change :) looks like a package dependency broke for mochatest", 'megawac just cherrypick this whenever and i will :eyes: thanks again :heart:', 'if you put this at the top of the file it should fix those.``` js/* jshint maxstatements: 14. maxcomplexity: 7 */```', "yeah. working on another project at the moment. i'll come back to this later to night probably", 'we do have access to the dom with `electron-mocha` for testing. `setcellhoverstate` seems like the most likely candidate for testing.', "need to pass --web-security=[true|false] to phantomjs. hey!i'm building an api module that is supposed to work both in node and in privileged browser environments such as phonegap/cordova. it contains integration tests making cross-origin requests to a url that is not cors-enabled. this is fine for my use-case since the xhr in phonegap/cordova is not restricted by the same-origin policy.these integration tests work with mocha (since node too can make cross-origin requests). but fail in mochify since it's not currently possible to pass the necessary `--web-security=false` flag into mochify and on to phantomic and phantomjs. do you think we could add support for it somehow? if so. i can do the implementation. but we'd of course have to agree on a sane solution.i guess we could either make some kind of subargs thingy that lets you specify arbitrary arguments to be passed on from mochify to phantomic and then on to phantomjs. or we could add an explicit --web-security flag to mochify and phantomic. do you prefer any of them. or do you have any other ideas?thanks!", 'it should be straight forward to add an explicit -- option for this. use the phantomjs option name and just pass it through.i gave up looking into a generic solution for options. you need to know which one affects which library. assign default etc...thanks!', 'good work :+1:', "there are a lot of different testing methods. karma.js. mocha-phantomjs. browserling. i don't think we can easily group them", 'so you want basically `brunch test` that will be a simple alias to `mocha-phantomjs`?', "i've updated the pull request with a minor variation of the original idea to illustrate why it's necessary. this allows you to use only and ignore options in a .babelrc when using babel-register with something like mocha where you're using babel-register as such: `mocha --compilers js:babel-register`", 'there are mocha based integration tests in the project. please check the `test/cli.test.js`. during test execution feature files and step definitions are created. the result is executed with nightwatch-cucumber.']
['added lambda properties. refactored. wrote mocha test', 'original mocha works well in intellij console.', "i agree with jprichardson that we should be careful not to add too much extra functionality to electron-mocha; most users just want to effortlessly use a regular mocha setup with electron after all. having said that. i understand that this is a pretty cool feature for unit testing code for the web. perhaps we can simplify the approach a little bit. though. to ease the maintenance burden?just thinking out loud. but could you. for example. `--require` a module that adds script tags to the head of the page? (for this to work we'd probably also need to make sure `mocha.run` waits for dom ready.)if this works. we could add the `--scripts` option but simply have it add the script tags (it could run right after the current requires). in particular. i would much prefer to keep the static index.html as is and not to open all the script files ourselves (using script tags with a src attribute instead) as it makes debugging much easier. if this works. we'd be able to expose the same feature with very little code. orbitbot do you think that would work or am i missing something?", "i'd gladly make this simpler if there's a reasonable way of doing it. a separate module didn't come to mind initially. but i can't immediately see how something outside electron-mocha itself would work.> just thinking out loud. but could you. for example. --require a module that adds script tags to the head of the page? (for this to work we'd probably also need to make sure mocha.run waits for dom ready.)> > if this works. we could add the --scripts option but simply have it add the script tags (it could run right after the current requires). i'm not sure on how electron handles scripts with source attributes. but it's worth testing. if the source attributes don't work then it should still work by basically using the same logic as in the modified `index.js` in this pr on the page itself. but in that case nothing much has been achieved with the changes.> in particular. i would much prefer to keep the static index.html as is and not to open all the script files ourselves (using script tags with a src attribute instead) as it makes debugging much easier. well. hopefully your existing tests would explode at reasonable places. but i guess this way it might be possible to get better stack traces for errors.i can have a go and open a separate pr or report back with results.", 'orbitbot ah. i just tried the same thing over at inukshuk/electron-mocha#scripts -- would you mind calling the option `preload` and have it work exactly like `require`. i.e.. you can use it multiple times instead of a comma separated list?', 'add tests for mocha.opts. --require. and --preload. reorganize test files to make them easier to call.add tests to run scripts.', "hi madarche is this still something you would like to see merged? i am sure this can be helpful. but i think it might be a better idea to put this into a separate repository where it can be maintained separately (or at least pinned to a specific electron-mocha release so we don't have to worry about breaking anything).(also. i'm not sure what you mean by 'renderer view' exactly)", "by 'renderer view' i mean a part of the ui of an electron app. that is an html page as spawn by the main process through a renderer process (typically like `main_window.loadurl(an_html_page_path)`.for example atom is an electron app. and a developer could want to test the menus of this app (for example to test that when the user clicks on the save menu item the application doesn't crash).ideally i would like to be able to test the whole ui of the electron demo app (that is the main.js of the electron demo app could be loaded). but i don't see how to do it at least now. a less powerful and easier to develop. but still useful. approach would be to provide something like a `--view an_html_page_path` option to electron-mocha that would  `main_window.loadurl(an_html_page_path)`. what do you think?", "i think that providing a framework to test electron views in this sense is probably out of scope for electron-mocha (we just want to 'run mocha tests in electron'). however. we should definitely allow for such tests to be built on top of electron-mocha.your second suggestion (providing an option to an html file) is the one i was initially thinking about as well. in theory. you could achieve something very similar now. by using `--require` or `--preload` to run a script which replaces the body of the page with the one you would like to test (you can also inject css and so on). right? alternatively. if we had the proposed `--view` option. we'd have to run the mocha setup in a preload script. while that should work. it wouldn't be a silver bullet either (e.g.. what if your electron view also depends on a preload script?) what i'm trying to say is that testing your view like this probably works already. but it requires you to set everything up before the tests are run; would adding a `--view` option really help that much? my feeling is no. but i'm happy to be convinced otherwise. just taking the app i'm working on at the moment. i'd still have to write a lot of scaffolding to set everything up. i'm not sure that i'd really benefit from having the `--view` option. what do you think?the first option is intriguing. too. but i doubt that we could set this up in a generic way either. for instance. what if your app depends on command line parameters to be called? regarding the examples. i don't know what jprichardson thinks. but i'd prefer this to go into a separate repo or into a gist. we could add a link to the readme to make sure people looking for examples find it.", "thanks a lot inukshuk for your detailed answer. at first i thought that electron-mocha aim could be to test electron apps too. but as you reminded me: electron-mocha is 'just run mocha tests in electron' (which is good too :-) )reading you i reach the same conclusions as you. so this pr can safely be closed and i will look for another project that makes it possible to test electron apps.thanks again!", 'as explained in details in #38 by inukshuk. electron-mocha is just to run mocha tests in electron. not to test electron apps.', 'unit tests: use a test runner to run the tests. something like karma or mocha. in fact. we should again look at what espree has done here.', "how's this different than #1004?", 'sorry. missed it during a brain dump!', "ariya. then i'll take this one and get mocha set up. expect pr tomorrow.", "mikesherov shouldn't you start with a test runner first (karma or venus.js)? last time i checked. maybe it's changing already these days. the standard `mocha` runner doesn't automatically run the tests in (different) browsers.", "> it's one less thing people have to learn about before they can contribute.this. i don't think we need to reinvent the wheel here. as long as the resulting tests are small. easily identified when failing. and can be run on node and browsers both locally and in ci. we should be good.i think using mocha. chai. leche. and karma address this issue while providing contributors familiarity.", 'yup. all comes free with mocha/karma.', 'addition of sc5-serverless-boilerplate and serverless-mocha-plugin to the documentation. sc5-serverless-boilerplate is a project template for test driven development of rest backends (installed using sls project install -n projectname sc5-serverless-boilerplateserverless-mocha-plugin is a plugin to ease test driven development with serverless', 'mocha broke the build', ":)my use case is a little bit different. i want to do this on a very global level. where before i run _any_ test. the assertion counter (or flag) is reset. and after the test has run the counter/flag will be checked to see if it was hit at all. i use chai with mocha. and it's very easy to hook into before and after each test. but well in there i have no idea what to do with chai. i suppose solving that problem will inherently give you a correct counter as well. since you'd have to know whenever _any_ type of assertion were made. am i making any sort of sense? i feel like i might be confusing things.**edit**: the benefit of having a global counter or flag is of course that i can then make sure to fail any tests that didn't make any assertions at all. it happens a little bit too often that i misspell the code and mocha happily reports these tests as a-ok. which is clearly not right.", 'tmp.dirsync and tmp.dir behave differently on invalid tries option. found during refactoring tests with mocha #137 #141dirsync will handle -1. undefined. 0 and null as expected while dir will only handle -1 as expected. failing on 0. undefined and null.', 'in germany we say `messen kommt von mist`. same goes for testing.', "raszi you're welcome. the existing tests that spawn child processes are still missing. i need to go through the code to find out how to do it with mocha...", "hey lutovich .i have been using mocha so far. but would be more than happy to checkout jest.if you have code by hand please be kind to submit a pr and i'll take a look.cheers!", 'same thing happens with mocha which is why the mocha task is weird btw', 'i hope that the `--testcase` cli argument is good enough for now? otherwise re-open and state your use case.', 'line numbers in mocha are all incorrect. .. this is quite annoying for debugging tests ...', 'try `node_modules\\.bin\\mocha test\\*-test.js`', 'nevermind. got it working a bit better than in v9. actually.  just had to drop the `t.current()` dependence.', "good point about console.log or other statements interspersed with the tests.  i personally haven't realized the usefulness of synchronous relationship between code inside and outside of the tests. (and i would love to see the cases where it is important... really and sincerely mean that.  i'm not doubting that it is super useful. just saying i haven't seen the need yet in my testing.) having the bodies of the tests themselves synchronous in relation to each other. has been all i needed so far:so for the example above. i would do:```javascriptvar t = require('tap')t.test('child'. function (t) {  console.log('before')  t.pass('this is fine')  t.end()  console.log('after')})```but i would be happy to enrich test-kit to allow synchronous statements outside the tests:```javascriptt.log('before')t.test('child'. function (t) {  t.pass('this is fine')  t.end()})t.log('after')```... where <code>t.log(..)</code> could just be a special case of a general hook we could make:    t.sync(function() {...})that executes statements sequentially/synchronously with respect to the tests.  (i agree very much with your earlier point that messing with console.log is a *bad* idea).as for the point about settimeout()... i'm not sure i would worry about that case because the cost of solving it exceeds the handiness of .only(). which is just there to save time when i want to quickly hone in on a test.  but if we wanted a way to hone in on certain tests. we could add another method to t that turns tests on and off by their name and/or number:t.only(1). would run only the first test.  in fact. i might want to put a statement like this at the top of the test file and eliminate all the async stuff in test-kit (make it opt-in).... hadn't thought of that until just now.  i mean it's nice to switch tests on and off right at the test. but putting the statement early in the module might be 'good enough'.", 'no. this is something else. it would allow you to avoid creating a register-handlers.js file in each project for default configurations (i.e. just coffeescript. no streamline etc.).', 'devongovett thanks for pointing this out. since the options are hard coded. how about reading these from a coffeecoverage.conf json configuration file from the cwd?in this file one could specify multiple different configurations. e.g.```{    "default" : { ... }.    "mocha" : { ... }}```with the above register script then looking for the key mocha and the command line utility looking for the key default or some other user specified key.', 'updating user model tests for synchronous test and fixing done() calls. refactoring the async nature in the user model tests to account for mocha 2 second timeouts causing travis-ci build fails', 'ilanbiala i updated and squashed. this should solve still the async/sync issue with that one test.', "thanks. i'll merge.", 'running "server" taskmean.js - test environmentenvironment:                    testport:                           3001database:                                "mochatest:src" (mochatest) task  article model unit tests:    method save      [?] should be able to save without problems      [?] should be able to show an error when try to save without title  article crud tests    [?] should be able to save an article if logged in (127ms)    [?] should not be able to save an article if not logged in    [?] should not be able to save an article if no title is provided (55ms)    [?] should be able to update an article if signed in (81ms)    [?] should be able to get a list of articles if not signed in    [?] should be able to get a single article if not signed in    [?] should return proper error for single article with an invalid id. if not signed in    [?] should return proper error for single article which doesnt exist. if not signed in    [?] should be able to delete an article if signed in (70ms)    [?] should not be able to delete an article if not signed in  user model unit tests:    method save      [?] should begin with no users      [?] should be able to save without problems (43ms)      [?] should fail to save an existing user again (45ms)      [?] should be able to show an error when try to save without first name      [?] should confirm that saving user model doesnt change the password      [?] should be able to save 2 different users (51ms)      1) should not be able to save different user with the same email address  user crud tests    [?] should not be able to retrieve a list of users if not admin (57ms)    [?] should be able to retrieve a list of users if admin (59ms)  20 passing (3s)  1 failing  1) user model unit tests: method save should not be able to save different user with the same email address:     error: timeout of 2000ms exceeded at null.<anonymous> (c:\\users\\rmlopes\\meanprojects\\meancegi\\node_modules\\mocha\\lib\\runnable.js:139:19)      at timer.listontimeout (timers.js:119:15)there you go. as you can see i am running this on a windows machine (even though it should not make a difference in this case). the remaining phantomjs tests are all marked with success.', "fixing user model test's last test which fail or report a false positive. fixing up a user model test which was not setup correctly without the async done() callback. which led to false postivies. adding timeouts to the test ensures that the test completes in time. otherwise mocha's 2s timeout will fail the test", 'ilanbiala  mleanos review please?', "ilanbiala we're good?", "i'd like to push this fix in as well as other tests i have worked on in another pr asap so we can get that tests safety next as wide as possible.anything else codydaig ilanbiala mleanos ?", 'yes lgtm.', 'tests are failing with "cannot read property \'prototype\' of undefined". seems like something is wrong with servicebusmanagement.the failure is described in the azure_error file:```wed dec 05 2012 17:39:37 gmt-0800 (pacific standard time):[typeerror: cannot read property \'prototype\' of undefined]typeerror: cannot read property \'prototype\' of undefined    at object.exports.inherits (util.js:538:43)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\servicebusmanagement\\servicebusmanagementservice.js:60:6)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\servicebusmanagement\\index.js:18:35)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\commands\\sb.js:19:28)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\cli.js:788:14    at array.map (native)    at scan (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\cli.js:787:23)    at harvestplugins (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\cli.js:800:17)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\lib\\cli.js:918:1)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\test\\cli.js:1:80)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at object.<anonymous> (c:\\temp\\projects\\azure-sdk-tools-xplat\\test\\commands\\cli.site-deploymentscript-tests.js:18:11)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.require (module.js:362:17)    at require (module.js:378:17)    at mocha.loadfiles (c:\\users\\amitap\\appdata\\roaming\\npm\\node_modules\\mocha\\lib\\mocha.js:137:27)    at array.foreach (native)    at mocha.loadfiles (c:\\users\\amitap\\appdata\\roaming\\npm\\node_modules\\mocha\\lib\\mocha.js:134:14)    at mocha.run (c:\\users\\amitap\\appdata\\roaming\\npm\\node_modules\\mocha\\lib\\mocha.js:290:31)    at object.<anonymous> (c:\\users\\amitap\\appdata\\roaming\\npm\\node_modules\\mocha\\bin\\_mocha:329:7)    at module._compile (module.js:449:26)    at object.module._extensions..js (module.js:467:10)    at module.load (module.js:356:32)    at function.module._load (module.js:312:12)    at module.runmain (module.js:492:10)    at process.startup.processnexttick.process._tickcallback (node.js:244:9)```', 'so how do i run tests (now during development)?', 'clone the azure-sdk-for-node repo and check out the dev branch. put it next to your clone of the tools-xplat repo. then. from your sdk-tools-xplat directory. do:    npm install ../azure-sdk-for-nodethis will grab the azure bits from that other directory.', 'im working on implementing mocha so that its a more standard format.', 'updated the memory info with more readable data. updated the parser. please pull and `npm install`if you run node with the `--expose-gc` flag garbage collection will be invoked between each test file runs. ```node --expose-gc run > results.log ```next step: let mocha do the output job', "trying to run 'sanity test' using mocha per reference in package.js:```node_env=mocha mocha --reporter spec -t 5000 -s 500```it returns:```calipso.logging.configurelogging(loggingconfig);                ^typeerror: object #<object> has no method 'configurelogging'```", 'sounds good to me! would also be nice if you used a test framework like mocha so you could have pretty outputs', 'did you insert the following snippet just below your mocha.js include?```<script src="/testem.js"></script>```', "i believe mocha supports this level of granularity using the `describe.only` inside the test blocks. i think testem is not the right place for something like this to be supported; instead we should submit pr's to testing frameworks that don't have logic in place to limit tests to an exclusive set when running a suite.", 'missing \'ms\' in mocha.js. i recently built my own mocha.js (for the browser) using make. i am now getting \'uncaught error: failed to require "ms"\' from chrome. i noticed that \'ms\' just became a new dep. was this not factored into the build for mocha.js ?', 'correct. i think tj will refactor mocha to use components which would gracefully include this.', "my bad yeah i'll have to chuck it in master for now unless i get the component part done first", "oh. if it's global that wont really work. npm doesn't really work that way. so you should install it for that project and do ./node_modules/.bin/mocha and add `make test` or similar", 'yeah mocha currently assumes that if you have an issue in before hooks then subsequent things will fail. though we could limit the scope of that assumption to only those test-cases nested within the same describe()', 'it seems to only be a problem for mongoose model schema definitions. though.  maybe mongoose is refusing to redefine an existing model when the file is re-required?', "i have  mocha.tmbundle: no such file or directory error and there is not nested folder in library/application support/textmate/bundles. any ideas?update: npm didn't include editors folder. weird... i wonder what else is missing?", 'any ideas why i get  cp: editors/javascript mocha.tmbundle: no such file or directorymake: **\\* [tm] error 1?', 'update bin/mocha. v8 also has an optional --harmony flag available.', "doesn't need to be in mocha core. there's no reason to couple it", "we did just use closures in the past. but that also encouraged accidentalpollution within the closure.then we moved to only sharing via the context. which ensure that anytimethe setup is done it will be in a beforeeach. it. etc.now that we have a spec suite that just broke through 2000 tests. and have3 development teams adding specs steadily. manually clearing this contextjust isn't maintainable.we are loving mocha. by the way. as it runs through those 2k+ specs inabout 40 seconds!", "yeah it's not actually easy right now especially using mocha(1)", "assert libs shouldn't assign `actual` & `expected` props to the error object for these kinds of asserts. when they do this. it should mean that they want mocha to display a diff. which might not be suitable for all kinds of asserts.speaking of which. i guess my commit for expect.js should leave `.equal()` along. which shouldn't display a diff.", "well those props aren't even specifically for mocha. they just hold the expected/actual values so you can choose to do other things with them. going mocha-specific wouldn't really be too great. ideally i guess we have alternative messages or something. bit of a tough thing abstraction-wise i guess", "do you think a mocha option for diff methods still stands? not sure how to access mocha options in the base reporter. if you could shed some light. i'd like to take a stab at it.", 'allow to skip() test  inside the function. in my tests some times test need to run only if certain conditions are met. for example previous test passed (where not skipped like load and save consequent tests). it would be great if inside test function i could do something like that:<code>if (!this.fit) this.skip() // and mocha would skip the test.</code>', "skip() inside of a test-case can't automagically jump out of it. just do `if (something) return done();` or if it's sync `if (something) return;`", "-1 from me for a few reasons. mainly being that mocha currently loads everything all at once and this would cause a bit gap in the server / client-side implementation. secondly i can't think of anything that is unmock-able without this feature. what's an example? i'm not a huge proponent of mocking in general but this would be a large overhaul if it's mostly for convenience", "thanks. but just as idea:it still would be good to have such ability maybe supply skip function as third parameter:it('should skip if not fit'. function(done. skip){  if (fit) done() else (skip())}or maybe do it with done function by supplying some paramenters done({skip: true}). by the way done('some message') output is huge and like error out. i wonder why?", "? there's no reason for skip functionality. just `done()` early that's all there is to it", "``` jsvar runner = mocha.run();runner.on('end'. function(){});```", 'exposed stats via runner to allow access via "end" callback. note: made the single-line change in `lib/reporters/base.js`; however. also ran the compile step which changes multple `mocha.js` lines.', "`.concat['xmlhttprequest']`", 'add -f to specify path to mocha.opts. allows you to store your configuration file anywhere you wanted.**-f. --fileconf <path>**the --fileconf option allows you to specify the configuration file that will be used. by default "/test/mocha.opts".', 'imo this is an anti-pattern. projects should be as self-contained as possible including test related stuff', 'add option to run mocha only in the master cluster process', "in fact. we've got many test directories for each part of our project. we use the same configuration file in each test directory. it would be usefull to store this file in a unique place. so that any modifications of the file would immediatly impact all tests.", 'fair enough. few changes though', "is there a fix for this planned at all?  browser-side async testing is pretty ugly at the moment.can't browser-side mocha use window.onerror in the same way that server-side mocha uses process.on('uncaughtexception') ?it may be ugly. but it's less ugly than the current state of things.", "lightsofapollo the behaviors of the handlers associated with onerror and uncaughtexception appear to be different.  on server-side. it appears to rely on the stack trace provided in the exception (see #482)what i'm looking for is to have browser-side mocha treat the exception as an assertion failure (like it does server-side). rather than as an uncaught exception (which it does browser-side).does that make sense?", 'alexkwolfe thanks!!!', 'another thing. can you please try to type `mocha` in the rn-example project directory and see if mocha is in the path?', '-bash: mocha: command not found', "i pushed a small fix that will use the local mocha explicitly. so if you pull you won't need to `npm install mocha -g` anymore", "zombie browser throws exception when a status code of 401 is received. currently i cannot use zombie to check the status codes of particular urls or paths in my application. if i get a status code of 401 the zombie browser throws an exception. is there any way to suppress this behaviour with a setting? i have looked at the code and haven't found one. here is a test case: this mocha test should pass but it doesn't :  is a mocha test file so you need to run it with mocha. i have come up with a workaround for this problem but it makes it harder to write tests so it is not optimum.", "ok so... does this mean that the recommended way to do browser tests in mocha is to do the browser navigation in the before function and then just a whole series of asserts in the it() tests? i looked back over the documentation and i didn't see that point made anywhere and there is no example test file (that i can see) so i didn't notice this as a suggestion. i think maybe it is implied but some people (like myself :p) like to have things spelled out in black and white. btw. i see your point about the 401 and looking for it. makes sense.", "if we introduce this api. `warnon` might become obsolete. i don't know.this file menu would be tied to the file system api provided in the generators api (write. copy. template methods etc.). and would show up only when we detect that a file is about to be overwritten and that this file is not identical.we might be able to implement the differences output by checking out how it's done within mocha. it has a really nice diff feature.", "great feedback. i definitely agree on trying to push auto-generation of tests with generators.hooks can be used to help us here. and provide us some flexibility to easily switch to another framework (like switching mocha to a jasmine setup).in every generators where generating tests make sense. `this.hookfor('test-framework');` can be used (this for now defaults internally to `mocha`. but it can be changed via the `--test-framework` cli option).- when done from a controller generator. the resolved generator is `mocha:controller`.- when done from a model generator. the resolved generator is `mocha:model`- and so on. for view. route. service etc.then. one can decide to implement each `mocha:*` generators. that generates the appropriate tests based on what kind of generators have invoked them. and probably add them to the test/index.html runner for both the specs and sources files.because. `mocha:controller` generated tests will be different when generating from an angular or a backbone generator. i'm starting to think that they should be considered mutually exclusive. and that each set of generators for a framework. will have to implement their own test-framework generators (mocha. jasmine. etc.)hope it makes sense. let me know if it doesn't =/", '(if you want to carry on using the pipe. you could use $pipestatus to make sure the errors from mocha are propagated)', "less files were just an example use case.  the real fix here is targeting what to watch. for instance. it's common to have an app or src directory that has a build step that builds to a dist or build folder. there can be files of all types in both. the goal is. you only want to reload when the build / dist directory is updated. not based on file types. there are similar patterns to this feature in webpack's loader config. you can 'exclude' patterns from being run through the loader. but you can also 'include' patterns. the include is the recommended approach in the docs because it is explicit. there is a finite and well defined list of things you want to parse opposed to an infinite list of things you want to exclude (everything else).babel has a similar option with its register hook. you can compile 'only' certain patterns or 'ignore'.  or what if mocha had you ignore every directory that did not have tests in it?  i guess what i'm getting at is that when you know exactly what you want to watch. it is easier and makes more sense to explicitly define it that  try to ignore every other possible thing.that's my whole hearted attempt at a persuading you :) whether the update makes it or not. still love the module and will probably keep using it.", 'any update on this?', 'why? why would you not just send an event to `window`? why would you make a test depdendent on being run in `mocha-phantomjs`?', 'sounds like a good idea. any chance you would like to help implement this?', "well. i tested dozens of times. it never worked and just after your response. it works!you are a shaman :)at the beginning. i was trying with:```mocha --compilers js:babel/register```and sure this syntax doesn't work.thank you."]
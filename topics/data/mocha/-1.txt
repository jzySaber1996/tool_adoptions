["improve testing situation. moving `mocha` into `devdependencies` allows people to install this in their own projects without having to download `mocha`. since `mocha` is only needed to develop and test this module itself.i also tried to run `npm test` out of habit and noticed the script wasn't defined. it looked like you needed to `cd` into the `test` folder before running mocha. so now. with help from `path`. you can run the tests regardless of your current working directory.thanks for this project! it looks great", "thanks for this. it's much cleaner. merging!", "on wiki page you pointed the first item said that `http` and `jq` assertions had moved to separate  repositories (so it is documented. maybe need to add which exact methods was moved?).`.header` now in `should-http`. you can use it with mocha to require itself or just `require('should-http')` somewhere (one time will be enough).", 'the issue is resolved in the current build! not sure exactly what has resolved the issue. but this is safe to close now.', 'orm is leaking a global: lodash "_". this error occurs every time i start running the `mocha` tests from scratch: `error: global leak detected: _`the test to reproduce:```describe(\'orm\'. function() {    it(\'will leak\'. function() {        var orm = require(\'orm\');    });});```i went one step further to see if `lodash` was the problem.```describe(\'lodash\'. function() {    it(\'will not leak\'. function() {        var _ = require(\'lodash\');    });});```only the `require(\'orm\')` produced the leak.', 'found the source of the leak in utilities.js.  putting together a pr.', "i see this was resolved a few weeks ago... it's just not available via `npm install orm` yet.", "should be resolve imho. right now the console.logs are extremely distracting and the test output is impossible to read (just the result). there's already a mocha pr which tries to solve this.", "forge was initially written before typedarrays were prevalent. we definitely want to switch to them (and to buffers in node as juliangruber suggested). i just haven't had the time to implement a plan to switch over. the plan is to include switching over as part (perhaps in steps) of implementing the webcrypto api so that forge can be used as a polyfill. we want to clean up and consolidate the test suite (and use mocha) before this work happens though. to ensure everything keeps running properly.", 'hi san650. some time ago i added the support for the addon\'s default blueprint. maybe i missed something there. will have a look! in the meantime you may want to run your tests with setting the debug environment var to "ember-cli-addon-tests" like this:    debug=ember-cli-addon-tests mocha --recursive test/ this will give you some detailed debug messages about what is happening...', 'move test framework to mocha. the jasmine tests are begining to creak a bit. and are running into a lot of basic problems that dont seem to be easily resolvablepersonally i prefer mocha for testing npm modules. so i vote for moving the whole test suite over to mocha', 'can you pull latest master into this? is it just the fact travisci is failing that this is still pending?', "hey jerairrest .yeh i'll rebase master asap. tests are failing due to some sort of misconfiguration on ci.however those are working quite nice locally...need to figure out why the heck it's not working as it's supposed to.i hope to get onto this over weekend.cheers.", 'hey jerairrest .if you have time these days i would appreciate if you could try this branch out a bit.i have rebased master.cheers.goran', 'will do!', "everything is looking good to me. i'm going to do a little bit more testing but for now", "i'm going to merge this in jerairrest", 'get travis running & tests passing. noticed while helping tcbyrd out with #84 that `mocha` is the dependency in the tests. but `jest` was being used to run the tests.we can migrate to jest in a separate pr. but this should get the tests running on travis.', "this looks good to me! we'll have to reimplement it in the new mocha test suite that we're working on. but i don't see any reason why this shouldn't be merged in the interim.", "thats great erinspice! i'll be happy to help out with implementing this into the new test suite. as you can see it was not very many changes. the new test suite is in the `mocha-test-suite-no-grunt` branch right?i reverted the skip socket test as i found a reliable way to share the redis unix socket between two containers. so this change should be good to merge.", "nice work with the mocha tests and removing grunt. one less thing to deal with. i've been a big fan of using npm as a build tool.", 'bengotow no problem man. just a little caution though we\'re making some internal improvements on a branch called "mocha" (as well as switching over to mocha+chai testing). we\'re pretty much 99% there actually. just need sdepold \'s final say/check and we should be able to switch over to  travis with it.. anyway. it _may_ cause with that pr but i do plan on rebasing and updating that pr asap after the switch over :)', 'tests run as currently user rather than postgres. trying to run the new mocha tests and it seems to insist on running as my current user. rather than as postgres with a local connection.', 'will look into why that is. for the time being you can do `seq_user=postgres make pgsql` if it really makes a difference', 'default for me works just fine as : `user postgres undefined sequelize_test`i just run "make pgsql". are you sure that you\'re not running as postgres by default?', 'seth-admittedly do you by any chance have a user environment variable set?', "yeah. unfortunately this is because brianc from node-postgres made the user environment take into affect (we technically speaking. can't go to a lower level than node-postgres obviously :p) i'll try to make a workaround.", 'alright dude. you should be good to go! :) just pull in the #master :)', "it's amazing the things that end up in production! jokes aside. i'm in total agreement here. i can't speak for jedwatson or dcousens. but i would love to see a pr for this (especially before #227). i don't have any experience with jest. only karma+mocha+chai. tests are tests though - jedwatson / dcousens how do you want to proceed here?", 'thanks craigdallimore. this is brilliant. i really appreciate the work you put into providing example of both jest and mocha.my personal preference is leaning towards the jest pr. it is simple to follow and quite explicit. not that the mocha one is much less so... does anybody else have a specific preference or input before i merge one of them?', '+1 to running nightwatch from mocha', 'i am interested in this integration as well. we use mocha for all of our tests. even ones where we utilize gherkin (interpreted by yadda) to describe and drive behaviors. it would be fantastic to be able to interact with the nightwatch api from our extensive. existing test code rather than having to expand our tool chain for this one class of tests. also. mocha integrates very well with our local development environments as well as our ci set up. it would be far easier to leverage that than have to also beef up our ci setup and train developers on a new tool for a subset of tests.', "vows.js won't make it to release. but it's out now (will stick to expresso until rewritten tests for mocha.js)", 'now that we\'re using mocha. we can do async tests:``` jsit("should work". function (done) {  model.on(\'change:message\'. function (value) {    expect(value).to.eql(\'hi\');    done();  });  model.set(\'message\'. \'hi\');});```can you see if you can update the tests where it fails?also. would you get the same benefit if you set `debounceinterval` to 0? hypothetically. if all the changes are done within the same process tick. deferring the processing to the next tick should be enough.', "yes. i've googled for mocha async. we have to `ractive.observe` for test and `done()`. lots of rewriting.`debounceinterval=0` -- great idea. looks promising. i'll check.", "oh no. i've said about `defer` as idea to replace your `wait` in test. `_.defer` more predictable. i thought it is some `mocha`s feature until i've seen definition in the end of file.maybe just rename it to `nexttick` and forget about `defer`)but inside main code they have huge difference. `defer` just runs all this 1500 handlers later. and `debounce` merges several sequential calls into one. this is the goal of this pr", "hi mreinstein. thanks for the pull request. alexa-app is running with mocha and chai as the test stack. which means tap tests won't be supported. i agree adding comprehensive test coverage is a great idea and any contributions on improving coverage using mocha and chai would be a great addition. we will also be getting a travis integration set up soon. marking this as closed.", "cool! :icecream: i'll test it. our use-case was unit testing. in this case you usually don't have one single entry module. but a whole directory of tests. a test runner like mocha then just executes all modules within test.", "probably worth a better explanation...the simplest way to invoke yadda is with an array of strings. each string is a 'step' and the array is a 'scenario. after the scenario has executed yadda will return if the step/scenario is sync. or callback if it's async. scenario and step events will be emitted. so you can listen for lifecycle events as the scenario executes.what you do to get the array of strings is up to you. you can parse them from files. or html. from a rest response. etc. etc. etcyadda doesn't offer any output - that's also up to you. typically is left to the test runner. e.g. when using mocha if 'done' is called with no errors then the test is successful and mocha handles the output", 'would you in general recommend wrapping yadda inside something like qunit or mocha? or is "raw yadda"  an equally valid approach?', "i added an example for creating reusable fragments. it could be made nicer by using a template loaded from the file system. rather than creating the scenario in code.also because the fragment isn't run directly from mocha. you only get one line of output for the entire scenario rather than the individual steps. i.e.```  reusable scenario example    100 green bottles       sing 100 green bottles are standing on the wall     100 blue bottles       sing 100 blue bottles are standing on the wall ```", 'rewrote helpers tests for mocha', 'rewrote utils tests for mocha', 'saschagehlich moved tests to mocha. should work well now.', 'add testing. test: add minimum viable testing framework - include mocha and enzyme for unit tests- add nyc for code coverage- add tests to ensure token and key are passed to stripe', 'as for testing frameworks. i beleive travisperson prefers mocha', 'i really like this idea. i would like to go in this direction. can we do it for qunit first? we could experiment by doing everything for qunit first. we could support mocha in the future.please. start working on this and let us know if you have any question. you can open a workinprogress pull request if you want early feedback.', 'great. will try to put something (at least a quick spike) together before xmas if time allows! :)as a mocha user myself. will try to bring this also to the mocha world asap! ;)', "because nodeunit doesn't provide test timeout and or ability to finish test immediately on crash i suggest to migrate test to mocha library.", 'extremely confusing. this is failing in node for me locally. but travis passes+1000 on the mocha change. qunit is being crap again', "this actually works now. it has mocha in node and qunit on the browser. going to see if we can't get mocha in the browser too.  also noticed i have it set on nyan reporter. we probably want to change that to dot before we merge. or do we?", "so i'd merge in the mocha tests first before trying to fix this", '(#1321) - increased timeout limit on mocha tests from 2s to 5s. a small number of tests still time out after 5s (at least on my machine). perhaps those tests should have their timeouts individually set.', "(#1321) - increased global test time limit. increased global limit for mocha testing (they didn't pass on my computer earlier)", 'this is niiiiiiice!', "the node tests succeed and the new nyan cat is definitely a :thumbsup: . but `npm run dev-server` is broken now for me.  all the mocha tests fail in the browser on every browser i try; apparently the global pouchdb object is not present.  not sure what i'm doing wrong.", 'you probably want to do `git pull --rebase origin master` the main one got merged already but the test specific ones are still needed', 'nick-thompson see #1261 which was waiting on the mocha stuff so i could debug. that is honestly step 1 for  #1250 as it should hopefully remove all the fs stuff in leveldb', "i've found that switching to mocha style `done` is helpful (given as a can argument to the wrapper function) as it can be called with an error if you need to fail the test.", 'if we switch to starting the tests with something like mocha --ui qunit-whatever --exclude those.tests tests/tests.*.js that might help', 'travis only tests node. not the browser. you are only testing the browser. also you should use done syntax now that were using mocha. basically change start() to done() and pass done as an argument to the function that wraps the test', "have we looked into testling? pretty solid browser support ( and now that we have mocha it's super easy to provide tap output..", 'yeh firefox is currently disabled as there was an intermittent / getting regular failure. its passing herebut yup. we are switching to mocha style tests so best not to add new tests in the old style. thank', '(#1354) - add flags for mocha reporter and grep', 'switched the default reported to spec as well. i find it easiest to work with and given tests can crash / stall i think its the most sensible default', "could you modify the contributors.md at the same time?  lots of changes going on recently (ch-ch-ch-changes). and i'm having trouble keeping up with the new unit test options.", 'once #1342 gets done we can call mocha directly. though this is good for now (about to test)', 'also grep browser tests. since i read in #1354 that you use `grep` for filtering node tests. why not also add it to the browser tests (now that we use mocha).', "yes that's true. but this is meant mainly for automation because you cannot change the location during the automated browser test (it exits with something like 'page reload not supported with async tests').", 'mocha ui. work in progress migrating the files over to default mocha ui and chai. covers #1342 and #1341', 'ping daleharvey', 'i think we will still want to have the mocha target call ./bin/test-mocha so we can pass arguments to it. but i think we can discuss after landing. its awesome to have cleaner tests and get rid of a bunch of scafollding. however its a very disruptive commit with a bunch of new contributors so lets land asap. the pouch.shim rename not majorly fussed about. think removeing bluebird_debug is needed. once theres a green travis i am :+1: on mergingthis is a massive amount of work. great job', 'so for changes. get. and replication i just put in shims for the qunit tests instead of fully chaifying it. as i wanted to finish this today so it could get merged', 'my idea was that if you want to do custom stuff. you can just call mocha yourself', 'green so merged 4307d35e94674eaf1bd9eb7c70ab58930098a009', 'changed test to mocha.', "of course. dale. i wouldn't forget to do anything as dumb as that.  :frowning: :palm_tree:", "so this isn't a hang its a timeout. mocha tests only end when done is called or the timeout happens not an error. closing this as its more an annoyance then a bug.", "i've been noticing that any exceptions seems to cause a timeout. even the desired exceptions (when should.js throws them). this hasn't been a problem for me in other projects using a lot of nested async tests before. mocha usually fails when an exception is thrown. similar to calling `done(err);`. the way i've worked around it for now. so the tests at least can give some better feedback than 'timed out'. is wrapping the tests in a try/catch like this:```it('foo'.function(done){  try {    bar.should.equal(baz);  } catch(e){ return done(e); }});```this was the correct error is propagated up and reported properly and it's faster as it won't wait for a timeout. but as i mentioned. this isn't the expected behavior of mocha as far as i know and shouldn't be necessary. so it might be something in the test utils or somewhere in pouch which causes this?", "+1000 for that. i'd rather not have to rewrite all the unit tests. even the ugly pyramid-of-doom ones.you're right that this is just an annoyance. but it's a huge one for me. i spend most of my time with pouch fixing failed tests in exotic browsers. and it kills my workflow if i can't even see which tests failed. qunit at least did that. so mocha sorta feels like a step down.reopening to remind myself to monkey-patch a fix onto this until we replace mocha with chai as promised. a simple `console.log` will suffice.", "i suggest to replace jest with mocha or jasmine. because we don't really use jest automocking feature.", 'isaacs i wonder if it would be possible to run `tap` in a dedicated only mode. in which it would only execute tests with a `{only: true}` to it. basically the opposite of `{skip: true}`. by default. `{only: true}` would be ignored. but running with `tap --only` or something similar would be as if i added `{skip: true}` to all over tests. would something like that be possible?', 'huh.  gr2m that\'s a pretty good idea.while we\'re at it. maybe we could add a way to say "run the skipped tests. too". so you could mark things as skipped in general. but run them while you\'re developing?  or skip really slow tests. but still run them on ci?', 'i guess you can already do stuff like `{ skip: process.env.ci ? \'skip in ci\' : false }`. so maybe "ignore skip" isn\'t such a valuable addition.', "i'd be happy to try send a pull request for the `--only` cli option if you like? i could use some pointers on where the code would need to be touched. how test and where to update documentation. just a bit to get started. we can then discuss details in the pull request itself. this would be tremendously helpful to have in `tap` :) i also like that it would allow to run multiple tests with `only` option but without the risk to accidentally push a `test.only` change which would run the single test in ci. this bit me lots of time in the past. i think the `{only: true}` option with `--only` has less potential for developers to shoot themselves in the foot", "> add a t.only(...) commandare you sure you want to add the method? there is no `t.skip` is there? i'd suggest we add the `{only: true}` option first and leave adding a `t.only` and maybe a `t.skip` method to a follow up issue?", 'i was thinking it\'d be nice to maybe implement this as a filter function on the root tap object.then it can also be leveraged to do `tap --filter="some pattern"` to filter tests based on their name.', "also that sails-mysql thing.. could that be obscuring the assets error for me? i haven't done any testing in node and mocha before so this is definitely new for me.my intuition was that the mysql issue is unrelated to the assets issue. i figure i'm probably missing a dependency somewhere. have you seen this before?", "+1 for generating unit test whenever something is scaffolded out. test coverage matters right? right? also: if tests are going to be _a thing_ i would vote for the karma/mocha/chai combo. from my perspective it's theoretical awesomeness. everybody likes easy cross platform testing. no?", "this works though:```    node_path='' nodemon \\      -e 'coffee|pegjs' \\      --watch test \\      --watch . \\      node_modules/.bin/mocha \\    $(arg)```", "this doesn't work (from docs `-e coffee. pegjs`)```    node_path='' nodemon \\      -e 'coffee. pegjs' \\      --watch test \\      --watch . \\      node_modules/.bin/mocha \\    $(arg)```", "how to default to filesystemloader even when window is present?. we're testing our components with `mocha-phantomjs` and are pointing it to an html file.due to this setup. `window` is present. which forces nunjucks to use `webloader`.the html file we are loading is however _not on a server_. but merely in a filesystem. webloader renders its template via `ajax`. and _this_ leads to issues. ajax calls return a `network_error: xmlhttprequest exception 101` error. as ajax calls are not allowed to local files.to address this we'd need to default to the filesystemloader. how would we achieve this?thank you in advance.", "i'm not seeing these. i presume the move to `mocha` either solved them or worked around them. please re-open if they pop back up however.", "we've moved to mocha now. closing.", 'fix npm start command again. since the new build changed some path. this commit is needed to fix `npm start`.finally it introduces a test case in node+mocha to make sure that the command keeps working.it removes the `watch` feature as it was hogging the cpu.', 'marten-de-vries: this isn\'t a burning issue for me. i did the work because it was insanely useful in debugging a perf issue i was having with pouchdb replication on node.js. so i\'d rather see us do this \'right\'. however one way or another we have to at least have a line of code in express-pouchdb that says "if req has proxy then set something on opts".as for pouchdb... that\'s harder. the issue is that so far as i\'m aware there is no way to programmatically set a proxy location in the browser. so a \'proxy\' option at the pouchdb layer would have to detect that we are in the browser and complain. the way pouchdb deals with this today is by side stepping the issue. they just say "whatever you stick in ajax we send down the stack. no validation". which is why they wouldn\'t have a test.the real test i want to have is that we are correctly parsing the request and passing it to pouchdb correctly. i wouldn\'t even care if we tested that the functionality was working. this really screams for a mock. and honestly. shouldn\'t we have mocks for all the express-pouchdb requests? i realize that is a ton of work but at least we could create a mocha framework and start with one test.i have very little experience with mocha or mocks. so it would be educational for me to actually set up mocha and write a mock for this test. if you\'re o.k. with it i\'m happy to submit a test framework for express-pouchdb with one test. :)what do you think?', 'dfasdjfdskajf;ladsfjklsdfj mocha!!!! >: [', 'douglasduteil sure definitely. this would be amazing.', 'add the first ever mocha tests!. with this pr mocha is now officially a devdep of roosevelt. and that can only mean one thing... tests!starting us off is some simple tests to ensure that params are set properly. this comes in 3 flavors:- constructor: test that params sent to the constructor are instantiated in a new app. tossing a package.json into the mix also ensures that they take priority.- package: test that params in rooseveltconfig of a local package.json are set properly.- defaults: test that default params are set when they are not set via the constructor or the package.some things to note:- there are some exceptions which break the conventions of these test cases which will either directly or indirectly be tested in the future:  - setting suppresslogs in every test case defies the default test.  - implicit `generatefolderstructure: false` in default tests as a result of no package.json.- to mitigate the possibility of race conditions polluting the isolation of app environments between tests each `appdir` is a different directory specific to that test suite.', 'folder structure mocha test', 'we should add another test to ensure that no extra unexpected directories are generated in the appdir.', 'okay. it was an issue with mocha haha. my apologies. mocha was moving onto the next test case before the event listener could finish processing.', 'i know time has passed since this thread was active but i would like to supplement sabov answer for the benefits of future reader.in addition to call `done()` callback but also increase timeout on your mocha test to allow test to wait for nodemailer to connect and send email. in my case i set timeout equal to 10000 and it has worked like a charm.', 'move mocha opts to test/mocha.opts. less cluttered package.json. this has the additional benefit of being able to directly run `mocha` without `npm test`.', 'thanks!', "added mocha tests. started test structure.  whitlockjc see what you think of the following structure for testing.  i haven't replaced the build system yet. but we can kick it off with `cake bake && npm test`", 'intended to address #180', 'everything looks fine.', 'i opened a pull request for initial field  #306 ;)', 'turn fields test into mocha unit test. no idea if this is desired or not at this stage. but i wanted to get more familiar with the way the project is tested and this was a way to do it.', "good call. this field test file was written before we'd integrated grunt / mocha / must. thanks for cleaning it up.", 'improve automated tests for keystone. we need to have a look at how to easily and automatically test keystone so that changes and releases can be made with confidence across a wide number of use cases and configurations.some of the functionality can be covered by expanding the automated test coverage now that we have gulp and mocha / must integrated. and this will help considerably.it might also. i think. be a good idea to set up a sample project (or two?) based on the yeoman generator that cover a wide range of route and model configurations. with test processes that will catch things that may be more difficult to cover using the automated keystone code tests. including the ui.any feedback or help getting this right would be welcome. it will be a really valuable addition to the project.', "lbdremy just did a simple test with figc outside this project. works like a charm.even does overwriting of nested properties in the config (which we would need)however: $ npm test --over.write=me will not pass them down. so figc will not do it's magicon the plus side. mocha does. so this works fine:$ mocha test/mytest.json --over.writeso it's not 100% but it still seems useful enough to throw figc in for this. right?", '/me (being sorry for pushing the wrong button)', "my mistake:  for some reason the group-query-test fails in this branch: also when ran individually.hadn't ran it individually before.group-query-test was new and merged in from the previous pullreq #46 that file just didn't give a conflict so i forgot about it. so sorry for all this noise.will adapt our expectations to this test as well.", "yep that's a great idea. i can create the issues tonight. i will add the label `test` to them.", 'fix testling config. testling needed a special html file to be able to setup mocha with the tap reporter (required by testling)', 'use ti-mocha and grunt-titanium to launch tests on simulators', 'fix line-endings-dependent tests for non-*nix systems. ```......................................................52 passing (8s)2 failing1) lines eachpos:   assertionerror: 388 === 393    at testeachposhelper (d:\\docs\\projects\\web\\recast\\test\\lines.js:73:12)    at exports.testeachpos (d:\\docs\\projects\\web\\recast\\test\\lines.js:89:5)    at context.<anonymous> (d:\\docs\\projects\\web\\recast\\test\\run.js:14:21)    at test.runnable.run (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runnable.js:194:15)    at runner.runtest (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:372:10)    at d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:448:12    at next (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:297:14)    at d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:307:7    at next (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:245:23)    at object._onimmediate (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:274:5)    at processimmediate [as _immediatecallback] (timers.js:330:15)2) lines charat:    + expected - actual    +<cr>    -    at lines.compare (d:\\docs\\projects\\web\\recast\\test\\lines.js:108:16)    at lines.lp.eachpos (d:\\docs\\projects\\web\\recast\\lib\\lines.js:497:17)    at exports.testcharat (d:\\docs\\projects\\web\\recast\\test\\lines.js:113:11)    at context.<anonymous> (d:\\docs\\projects\\web\\recast\\test\\run.js:14:21)    at test.runnable.run (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runnable.js:194:15)    at runner.runtest (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:372:10)    at d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:448:12    at next (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:297:14)    at d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:307:7    at next (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:245:23)    at object._onimmediate (d:\\docs\\projects\\web\\recast\\node_modules\\mocha\\lib\\runner.js:274:5)    at processimmediate [as _immediatecallback] (timers.js:330:15)```', 'this seems like a real problem outside of the tests. too.', 'allow tests to be run individually by mocha. ideally `mocha test/lines.js` would produce output similar to the "lines" section of the `npm test` output. this used to work with `whiskey`. and would be nice if it worked again.to be clear. i have no problem with sacrificing features like this temporarily to make progress quickly. switching to `mocha` was clearly a good move.', "also. since we haven't changed the syntax of the `test/*.js` files. `whiskey` still works fine! definitely would prefer to use `mocha` for everything. though.", "yep. also. if you don't like `--grep` syntax. it can be still possible to check `argv` for extra argument and call `submodule` only for this string. not for entire list. so it would become possible to run test suite by name.", 'added mocha example and doc', 'what command did you actually run?`npm install grunt-mocha-test`works fine for me', 'as you can see all tests except common tests rewritten in mocha. and only common tests still in nodeunit. and this is the next candidate for removing. because after years this file becomes huge and unusable. i want to split this file into several test: for relations/scopes. include. queries. adapter as currently done for validations. hooks and defaults.then adapters will be able to choose which tests to import from the main package.', "could you please add some test which will fail without this fix. i'm in middle of rewriting this module. so i need it to prevent regression. create separate file and use mocha (take a look at test/defaults.test.js as example).thanks!", 'note that i had to add `"test": "mocha --recursive"` to explicitly define the test suite runner script. since having a `scripts` property in `package.json` overrides the defaults.', "what's the plan on this? this is making travis + mocha-phantomjs completely useless for me as failing tests just hang until travis kills the task (meaning i get a 'broken' notification 30 minutes later)", "low priority but yeah i'd agree with that", "incomplete test summary in cygwin. when tests are running in an cygwin environment the test summary is incomplete.sampletest:``` javascriptvar expect = require('chai').expect;describe('testsuite'. function () {    describe('thing'. function () {        it('should be true'. function () {            expect(true).to.be.false;        });    });});```when running `mocha mytest.js --reporter spec` in the normal windows console this is what i get:(everythings fine. just imagine the color)```d:\\dev\\mochateststuff>mocha mytest.js --reporter spec  testsuite    thing      1) should be true  ? 1 of 1 test failed:  1) testsuite thing should be true:     expected true to be false```running in cygwin:(the summary is missing. also no colors)```meralanis /cygdrive/d/mochateststuff$ mocha mytest.js --reporter spec  testsuite    thing   1 of 1 test failed:```i did a little digging through the code and it appears that the `end` event of the runner isnt fired.from the spec runner:```runner.on('end'. self.epilogue.bind(self));```", "we're making use of the fix implemented in this pull request. and it fixes our problems with running through mocha-phantomjs as well as showing no negative effects with the standard in-browser html reporter. i think this could ideally be merged. and then work could be done to display the diffs in the html reporter as well going forward...", "qunit interface: adds support for built-in assertions. including expect(). this patch modifies the qunit mocha interface to support all of qunit's built-in assertion methods. including expect (the method that allows you to declare how many assertions are in a given test case).i realize this is somewhat against the design philosophy of mocha in that it ties assertion handling into the test runner. but in doing so it gains the ability to use the expect() method of qunit. which requires some minor integration with the test runner to be able to work.i've added tests. but since some of them are negative tests that are expected to fail. i haven't tied them into the build system though the makefile. as don't know if there's an existing paradigm for those conditions.feedback welcome of course!", "we weren't necessarily going for feature parity with qunit. i definitely don't see us ever coupling with an assertion lib", 'it could be an interface that we add to the mocha wiki page as third-party. basically just duplicate the qunit interface instead of extending it', "yeah. i'm fine with that approach.  so i would need to programmatically create a mocha instance and then overwrite the mocha.interfaces.qunit object with my own. right?", "it does work to override the mocha.interfaces.qunit setting.  it would maybe be nicer if there was some way to do that from the mocha command line. but this works.   i'm closing this pull request and i'll create another repo with just this overridden interface and write up an article for the wiki on how to use it.", 'ideally we handle it like the reporters where you can just do `--reporter npm-module-name` but `--ui my-qunit-stuff`. i dont remember us adding that yet though', "yeah there's been some discussion about this. it's a bit of a tough call. i dont want to bloat mocha with a bunch more options. i think we should just come to a conclusion on what's expected. i'm a bit torn between the two personally. we could also clutter the output a bunch more and add both but that's pretty ugly", ':d i really need to work in mocha using components', 'yeah. having it work like the reporters would be great.', 'im no help with windows unfortunately :( if you discover anything that looks suspicious let me know', "i checked which events get _actually_ fired when the reporter runs:``` javascript    var _emit = runner.emit;    runner.emit = function() {        fs.appendfilesync(path.join(process.cwd(). 'dbg'). arguments[0] + '\\n'. 'utf8');        _emit.apply(runner. arguments);    };```as it turns out either way the events get fired correctly:```newlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenernewlistenerstartsuitesuitesuitetestfailtest endsuite endsuite endsuite endend```it looks like it is an issue with the cursor moving thing that is done (like `cursor.cr();`) and that the cygwin console doesnt get along with it.thus this might be some cygwin configuration issue that turns up in any application that uses these console directives and is not mocha specific.", '+1  if not included in test timing. just a separate timing log for the before/after functions.', "in #700 this was brought up again. i think for now maybe aggregating them would be safest. this gets a little tricky since multiple beforeach's can be used even at the same level so describing them in the output would be very verbose unless it's opt-in", "http client test. considering the following code:``` javascriptvar http = require('http');var options = {  hostname: 'localhost'.  port    : 3000.  path    : '/api'.  method  : 'post'};function post(text. cb) {  var req = http.request(options. cb);  req.write(text);  req.end();}```and a mocha test. like this one:``` javascriptdescribe('http client test'. function() {  it('will make the request (first)'. function(done) {    post('foo'. function(res) {      throw new error('bar');    });  });  it('will make the request (second)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });  it('will make the request (third)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });  it('will make the request (fourth)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });  it('will make the request (fifth)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });  it('will not make the request (sixth)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });  it('neither will this one (seventh)'. function (done) {    post('foo'. function (res) {      throw new error('bar');    });  });});```the result is:```http client test  1) will make the request (first)  2) will make the request (second)  3) will make the request (third)  4) will make the request (fourth)  5) will make the request (fifth)  6) will not make the request (sixth)  7) neither will this one (seventh)x 7 of 7 tests failed:1) http client test will make the request (first):   error: bar2) http client test will make the request (second):   error: bar3) http client test will make the request (third):   error: bar4) http client test will make the request (fourth):   error: bar5) http client test will make the request (fifth):   error: bar6) http client test will not make the request (sixth):   error: timeout of 2000ms exceeded7) http client test neither will this one (seventh):   error: timeout of 2000ms exceeded```as you can see. after the fifth failed test (the first five). every subsequent test fails because of a timeout. they do not fail because of the throwed error. they fail with a timeout.generally speaking. after 5 test error involving an http request. every other test that also depends on an http request (in a async fashion. of course). will fail with a timeout. their requests will not even hit the server.any idea what's going on?", "never worked. mocha explicitly exit()s at the end so you don't have to gc all of node's junk (open sockets. intervals. etc). going to close just because this wont be feasible in the near future. however some people are convinced they _need_ this feature. even though i strongly disagree. we may add some sort of method that stops mocha from running in the first tick (we have a few issues for this open i believe)", "argh. i didn't realize this was mocha's issue box. google took me here and i got disoriented. it's your project. so it's all good. i wouldn't have meant to press my opinion so hard.i'm also considering a flatter style with a hierarchal naming convention. to reduce jumping around the file.", 'add option for passing a writeablestream to mocha on init for logging purposes. i\'m interested in running a mocha instance and capturing output in a writeablestream. right now mocha uses `console.log` to echo all its messages. to accomplish this. i have to listen to stdout and do some extra work. this could be made easier with an optional stream that the user passes in and simply default to `console.log`. for example:``` coffeescriptmystream = fs.createwritestream("path/to/my/file")mocha = new mocha  ui: \'bdd\'  reporter: \'spec\'  stream: mystream  # defaults to console.log```', 'is there any objection to mocha printing both the error message and the diff? diffs are helpful. but imho the message is essential.', 'i believe this is because i am using the grunt task simple-mocha. if i run using mocha from the command line. this works as expected.', "the problem is that `mocha.run` is synchronous in ie and asynchronous in all other (good) browsers. you can use the following code to have the same behavior everywhere.``` jsif (window.activexobject || !window.postmessage) {    (function() {        var timeouts = [];        process.nexttick = function(fn) {            timeouts.push(fn);            settimeout(function() {                if (timeouts.length) timeouts.shift()();            }. 0);        }    })();}watch(mocha.run());```this will also fix #502 at the same time. isn't life beautiful?of course it would be perfect if tj could commit the fix directly in mocha.", 'it never should have worked haha. mocha was never designed to nest test-cases. they\'re supposed to be nested within "suites" with the describe() calls etc', 'i think it would still be useful to provide the file name parameter that\'s in this pull request because some node modules also emit their own messages to stdout (and don\'t provide a method to disable them). this is the hack i settled on to get my coverage json:```mocha -r json-cov $file | perl -0777 -pe "s/(?s).*?{\\n/{\\n/" > $file-coverage.json```', 'using latest mocha for browser. i can access: mocha.reporters.base', "this is rebased. and i *think* it works because when i fix all the absolute `require('ember-electron/...')` paths to be relative i can run `ember electron:test` and it passes. but since a whole bunch of mocha tests are failing in master. i can't tell if this pr breaks any functionality that they are supposed to be testing.", 'reload.js expects testing environment to be qunit. add support for other testing libs. in its current iteration reload.js seems to check for env by looking for qunit. we should try to read the env in a different way to support other testing environments like mocha.', 'yaaaaaaaas. i remember chatting about this with felixrieseberg yonks ago.also. looks like this may be solved by #188? i was able to get rid of that check using `nodeprocess.cwd()`', "i think i'll open a pr to migrate the tests to `mocha` and `chai` and follow the same patterns you used for `node-elm-compiler`. using different frameworks is going to make it unnecessarily difficult to transfer learnings between the two projects.", 'doesn\'t enforce our space standard. please see `.jshintrc````{  "node": true.  "browser": true.  "esnext": true.  "bitwise": true.  "camelcase": false.  "curly": false.  "eqeqeq": true.  "immed": true.  "indent": 2.  "latedef": true.  "newcap": true.  "maxdepth": 8.  "maxparams": 4.  "asi": true.  "noarg": true.  "quotmark": "single".  "regexp": true.  "undef": true.  "unused": true.  "strict": false.  "trailing": true.  "smarttabs": true.  "laxcomma": true.  "mocha": true.  "node": true.  "globals": {    "matrix": true.    "debuglog": true.    "app" : true.    "$": true.    "_": true.    "t": true  }}```', 'can you change commit message to something like:```gh-665 add unit testsadd unittesting using mocha. chai and phantomjs.fixes gh-665```', 'mlzummo. jbrundage. dtsnell4. buunguyen - welcome to unit testing.anmoljagetia has just added a base unit testing framework (mocha. chai. phantomjs) which "we" will be augmenting as time goes on.  this is my first test case and it caught 1 missing class name...please review.', 'gordonsmith i cant get amchart pie to pass phantomjs mocha. even on preivous branches now that have already passed in the past (locally as well)', "chrismccord i've now completed a first pass at unit tests for the `socket` class. i'm pretty sure this gives us coverage for all the socket functions. either explicitly or implicitly. there may be a few pathways missing (like testing against a mock `activexobject` object). but this may be good enough as a start. i opted for duplication in areas like setup and teardown of tests in the interest of local readability and perhaps to help highlight dependencies and coupling for future refactoring. i'm interested in tackling tests other exported classes in `phoenix.js` perhaps in separate prs.looking forward to your feedback!", "you are my hero! thank you so much for paying down my lack of js test debt. this will make future iterations must nicer. looks great and i'm sure now we've improved the bus factor on the js client :) welcome to the club!", "w00t! thanks!i'd be interested in working on some js tests for `channel` next if you think that would be helpful.", 'yes. please do! that would be extremely helpful. we particularly could use tests on the push join/receive/timeout chains.', 'improve testing & fix broken tests. - [x] `mocha` is a bit more comfortable for management. use it instead of custom script.- [x] 1 test (testunexpectedexit) hangs after last commit with slimerjs support (shame on me).- [x] 3 other tests broken (did not digged yet).- [x] add pass with slimerjs testing.- [x] no global dependencies. add phantomjs & slimerjs to devdeps.- [x] automate with travis-ci', 'complete!', "so the idea for tests would be that the test writer would add .before (and possible .after) tasks that do there setup? that matches mocha nicely so i'm +1 on that.", 'tests. we need some tests for this library. what are your thoughts ebrelsford?', "yeah. i was just thinking this too. mocha would be great. and i'll probably have time to get going on this in a bit.", "travis ci integration. it'd be a fun practice to get travis ci working so that we know the latest build always works.this might go hand in hand with writing some additional mocha tests for the generator.", 'the basic mocha tests fail too.```....x 2 of 82 tests failed:1) flows allows me to retrieve default_currency:   error: parse error    at cleartextstream.socketondata (http.js:1584:20)    at cleartextstream.read [as _read] (tls.js:509:12)    at cleartextstream.readable.read (_stream_readable.js:320:10)    at encryptedstream.write [as _write] (tls.js:366:25)    at dowrite (_stream_writable.js:221:10)    at writeorbuffer (_stream_writable.js:211:5)    at encryptedstream.writable.write (_stream_writable.js:180:11)    at write (_stream_readable.js:583:24)    at flow (_stream_readable.js:592:7)    at socket.pipeonreadable (_stream_readable.js:624:5)    at socket.eventemitter.emit (events.js:92:17)    at emitreadable_ (_stream_readable.js:408:10)    at emitreadable (_stream_readable.js:404:5)    at readableaddchunk (_stream_readable.js:165:9)    at socket.readable.push (_stream_readable.js:127:10)    at tcp.onread (net.js:526:21)2) flows "after each" hook:   error: socket hang up    at createhanguperror (http.js:1473:15)    at cleartextstream.socketcloselistener (http.js:1523:23)    at cleartextstream.eventemitter.emit (events.js:117:20)    at tls.js:688:10    at process._tickcallback (node.js:415:13)```', "seems it must be this change as your repo is coveraging fine. strange thing is the the tests don't even start - i see no mocha output.", "getting the basic mocha html report from mocha-phantomjs. rather than using the docs report or other reporters. i'd like to get mocha's basic html report (the one that uses the nice css) from the output of mocha-phantomjs. the reason i cannot use mocha for my tests is that several of them require a headless browser to be successful.is there anyway to get the nice looking html out of mocha-phantomjs?", "> i'd like to get mocha's basic html report (the one that uses the nice css) from the output of mocha-phantomjsthis doesn't make any sense. the html reporter interacts with the dom; all other reporters output to a console. `mocha-phantomjs` outputs to a console.the default `spec` reporter is very nice looking.", "hmmm doh i put up v4 as an early beta but i thought it should be hidden and not the default... i'll look into that. it's faling as `phantomjs2` doesn't have built linux binaries.`npm install mocha-phantomjs<4` for now.", 'the cli. `mocha-phantomjs-core` is just phantomjs code. in fact i could make python and ruby packages out of it for those developers. as there is no node.js code there.', 'nope. running `elm-test` on the `upgrade-elm-test` branch is green for me on linux (using elm-test from master). i can an error on the `mocha *.js` part. but that\'s something different than what i\'m seeing on travis.i heard eeue56 had some "luck" reproducing this on linux? did you have to take any special actions to get that far?', 'test fail: "engine attach() should destroy upgrades not handled by engine". fails from time to time:```...........x 2 of 65 tests failed:1) engine attach() should destroy upgrades not handled by engine:   error: done() called multiple times    at multiple (.\\node_modules\\mocha\\lib\\runnable.js:177:31)    at done (.\\node_modules\\mocha\\lib\\runnable.js:183:26)    at runnable.run.duration (.\\node_modules\\mocha\\lib\\runnable.js:199:9)    at socket.eio.attach.destroyupgrade (.\\test\\engine.io.js:141:11)    at socket.eventemitter.emit (events.js:93:17)    at tcp.onread (net.js:418:51)2) engine attach() should destroy upgrades not handled by engine:   error: done() called multiple times    at multiple (.\\node_modules\\mocha\\lib\\runnable.js:177:31)    at done (.\\node_modules\\mocha\\lib\\runnable.js:183:26)    at runnable.run.duration (.\\node_modules\\mocha\\lib\\runnable.js:199:9)    at socket.eio.attach.destroyupgrade (.\\test\\engine.io.js:141:11)    at socket.eventemitter.emit (events.js:93:17)    at tcp.onread (net.js:418:51)```', "update mocha and use setup global beforeeach/aftereach. some simple fixes that should've been in #1373**changes:**- specify our own mocha so it's up to date- add helpers file  - global beforeeach setup and aftereach teardown  - fixtures- clean up setup/node.js - fixed a broken test (from old `this` behavior)", ':+1:', ':+1: will let you guys merge this one', 'fix mocha watch config. the watch task for mocha broke in #1377jmeas any idea how we can simplify this code?', 'thejameskyle can you explain how the watch is broken? it seems to be working fine without this fix for me atm.', 'samccone try saving a spec file and it will fail because they all depend on the `setup/helpers.js` file to run first.', 'ahumphreys87 does this remove `grunt-mocha-test` running with jsdom?', 'thejameskyle to answer your question in the previous pr. yes this does remove `grunt-mocha-test` but it still uses jsdom. i have changed any of that. just the method we use to run the tests', 'added timeout to delete tests. also removed hashlib from requires.tests were failing because while data was being retrieved. the delete tests got executed. added timeout to pass them. ever considered using a testing library like mocha to get rid of such issues?', 'see #33 instead', 'testing harness. this sets up a testing harness that runs `mocha` tests inside of electron. headless. yay for that.', "don't use electron compile within mocha tests.. take out the usage of electron-compile in the mocha tests.", "hey i added a lot of new tests. now i think it's awesome! :)it was my first time using mocha! smooooooooth! :)", "could you post the errors?also checkout  to be sure ist's not a registry problem.btw: we could change from buster to mocha + chai if you like", "if we are moving to globals i would rather just expose the functions as globals so they don't need to be imported (similar to what mocha does with describe / it).the other idea i've thought about was that the exported function name needs to match something or have some other property to differentiate it. haven't come up with something i like around this though", 'well. technically. i\'m not suggesting anything global per se. what i\'m talking about is a singleton that holds the state related to step definitions and hooks. and that singleton could be a "default instance". we could expose a proper instance-based api like so:```javascriptimport { definesupportcode. before. given } from \'cucumber\'const supportcode = definesupportcode(() => {  before(...  given(...  // definesupportcode could even expose the usual helpers to avoid explicit imports. similar to the current api:  this.when(...})```and calling the helpers directly as suggested previously would in fact call `definesupportcode` behind the scenes and keep a reference to that single instance that would be used by default by cucumber.this would give us the advantages of both approaches: * no more constructor automatically called* no need to call `this.xxx` all over the place* still able to programmatically pass isolated support code to cucumber (useful in an environment that do multiple runs. like a browser)* no globalsi\'m against introducing mocha-like global helpers. in mocha. it\'s (kinda) ok because those tests are meant to be quite isolated. focussing on small(er) units. cucumber. on the other hand. is used for acceptance testing. that often means loading a lot of things and polluting the global scope is more dangerous in that context. adding `import { given. when. then } from \'cucumber\'` at the top of your files to keep things clean and safe is really worth it. imho.', "thanks charlie. this is great!usually. you don't have many files that require cucumber helpers. for example. in cucumber pro. we have 5 stepdef files and 1 hook file. that's quite different from our mocha test suite that currently contains 85 files. as i said previously. mocha tests are supposedly more isolated. much more numerous and it's an arguably a nice convenience to have global mocha helpers.another nice benefit of the explicit `import`/`require` is that there are no assumptions about the location of support code files. it can be intermixed with other things without any risks.another one is linting: global helpers need special linting rules or disabling the linter in some places; that's annoying.so. i'd really like to be good citizens and not pollute the global scope :)", 'try toggle discard again. ```$ device=android mocha -t 60000 -r spec find-element-specs.js   apidemo - find elements -    mobile find       should scroll to an element by text or content desc (1900ms)       should find a single element by content-description (932ms)       should find a single element by text (1060ms)    find element(s) methods       should find a single element by content-description (4026ms)       should find an element by class name (992ms)       should find multiple elements by class name (1532ms)       should not find an element that doesnt exist (5209ms)       should not find multiple elements that doesnt exist (5161ms)       should fail on empty locator (1400ms)       should find a single element by id (5584ms)       should find a single element by string id (1576ms)       should find a single element by resource-id (1611ms)       should find multiple elements by resource-id (2742ms)       should find multiple elements by resource-id even when theres just one (926ms)    find element(s) from element       should find a single element by tag name (1273ms)       should find multiple elements by tag name (1511ms)       should not find an element that doesnt exist (5798ms)       should not find multiple elements that dont exist (6201ms)    xpath       should find element by type (923ms)       should find element by text (1035ms)       should find element by partial text (381ms)       should find the last element (1310ms)       should find element by xpath index and child (803ms)       should find element by index and embedded desc (5384ms)    find elements using accessibility id locator strategy       should find an element by name (693ms)       should return an array of one element if the plural "elements" is used (957ms)    find elements by -android uiautomator locator strategy       should find elements with a boolean argument (5611ms)       should find elements without prepending "new uiselector()" (1851ms)       should find elements without prepending "new uiselector()." (278ms)       should find elements without prepending "new " (1432ms)       should find an element with an int argument (1243ms)       should find an element with a string argument (403ms)       should find an element with an overloaded method argument (1484ms)       should find an element with a class<t> method argument (1470ms)       should find an element with a long chain of methods (1430ms)       should find an element with recursive uiselectors (1454ms)       should not find an element with bad syntax (5157ms)       should not find an element with a made up method (5095ms)       should not find an element which does not exist (5187ms)    invalid locator strategy       should not accept -ios uiautomation locator strategy (80ms)  40 passing (2m)```/cc paymand', 'looks promising. let me get back to you with the results of my smoke/unit tests tomorrow if there is no rush.', "ok. i'll look forward to your results. the master branch has failing tests so i will be merging this for now.", 'my smoke/unit tests pass. thanks for the fix! :+1:', 'awesome! thanks for helping with this fix.', 'fixed and extended mocha generator tests. the original tests were not working.furthermore. i added .travis.yml to enable travis ci', 'further mocha tests. although there are not many tests i think travis ci should be enabled to get at least some feedback', 'punchlinegroup or sweetpi please close this issue; resolved via petski/pimatic-mochad#3', "good idea. although i'd prefer testacular or some other multi-browser-test-framework instead of lock-in to support only the webkit engine (phantomjs) for testing", 'can i run a brunch app on travis-ci?. is it possible to run `brunch test` on travis-ci? is there a native `mocha` command that i can run?', "what needs done to make this happen?  i'm happy to attempt to implement. but would need a little direction", 'chrismcv nothing. just keep in mind that the result should be simple.', "see console.log output from brunch test command. when i run mocha in the browser then i can see `console.log` output from the code under test. but when running the `brunch test` command it doesn't. it will only show output from the test files. is there anyway to see the output when running from the command line?", '`alert` maybe', 'i plan to get rid of jsdom and use mocha-phantomjs which supports `console.log` and stuff nicely soon.see gh-463', 'using babel to compile and run mocha tests when certain es7 features such as class properties have been enabled. i would like to use babel to compile mocha tests when es7 class properties as well as other es7 features have been enabled. how do i do this? how can i turn on additional options when doing...``` js{  "scripts": {    "test": "mocha --compilers js:babel/register"  }}```', "similarly. i'd be interested in a way to not include the polyfill when `require('babel/register')` is used.i've made a few javascript libraries which are transpiled with babel's runtime flag before being published to npm. it's assumed that the global babel polyfill is not necessarily in use when people use the library. there are unit tests which operate directly on the es6 source by telling mocha to require babel/register. however that causes the polyfill to be used for the tests. this means my tests won't let me know if i accidentally make my code rely on things which are supported by the polyfill but not the runtime transform.", 'sebmck i get this error when running mocha tests - even if i agree that it would be best to avoid this kind of thing. a hard error prevents some valid use cases to run :(even if working. commenting the `throw` statement in lib/polyfill.js is hardly a good workaround.', 'thanks. looks good for me. great work. gronke !', 'i am still asking for adding `.travis.yml` file. because i want to see the build before merge', '> i am still asking for adding `.travis.yml` filedid i really commit an empty `.travis.yml` file  ... file content added now', 'only christian-bromann can do this.', "i think it's time for another breakpoint. feedback on the created test cases (in `./test/specs/`) is much appreciated to align our expectations on this reporter.", 'tests look good to me!', 'christian-bromann cool! what needs to be done to get this merged? or would you prefer me to rebase #8 against this branch and we continue there?', 'gronke nothing. please go ahead and merge. just gave you all the access rights.', 'christian-bromann awesome. some more tests will come with the pull-request #8 (_work in progress_)', 'christian-bromann  i am using mocha', 'yes. it would be nice to have some confirmation of this behaviour.', "v'll try to update once i tested it. (it's crucial for my implementation)", 'gmcnaught and ? kue-unique is working as expected ? can you share some code snippet how you did you tested ?', "unique did that trick. thanks.(for some reason when i first tried it. it seems like it's not working. and triggering the job only once)", 'write test cases. the tests are all organized in files which is great. but as the library grows it would probably be better to use a proper testing framework. and writing separate test cases.i suggest mocha. i think it is pretty simple and gets the job done really well.', "you are right. hence the `// todo: use a testing suite for testing` in jakefile.js ;)mocha is fine with me. i'm used to nodeunit but this doesn't support testing a complete directory recursively - which is what we have with math.js.there is one thing that bothers me: with math.js we compare a lot floating point numbers. which can have round-off errors. therefore i created a small utilty approx.js having functions `approx.equal` and `approx.deepequal` to test whether numbers are approximately equal. it would be great to have a test framework which supports this by default...", 'i agree with not adding dependencies. this would only be devdependency. so not problem...', "random more flexible args. now `random([size]. [max]. [min])` supports``` javascriptrandom()random(max)random(size)random(min. max)random(size. max)random(size. min. max)```and `randomint([size]. [min]. max)``` javascriptrandomint(max)randomint(min. max)randomint(size. max)randomint(size. min. max)```does that make sense?some tests are missing. but i am waiting for the mocha refactor cause now it's hard to read my test file", "ok super! i will do the merge after having everything refactored (currently halfway changing the source files into node modules). and after the refactoring i will change the test files to mocha. i'm not sure if there will be time left for that tonight.", 'cool! good job!!!', "oh. by the way. don't be surprised if the build fails... the tests for random functions obviously have a certain probability to fail :(", 'great! well done!!!yeah ... so if the execution speed is the same. the rest is not that important! maybe in the future. a nice to have feature could be to get a custom build of mathjs. so that you can keep it small in your webpage.i can take care of the mocha tests. or tell me if you really want to do it.']
["i'm not sure what you're trying to do? source maps are meant for inspecting / reading compiled code. but not for running it -- electron-mocha would just run the compiled code so i don't see how source maps come into play here? having said that. they should work just like they do in chromium.", 'ok. looks like this is general problem of mocha and bundlers. not specific to this project. will do more digging on the subject.', "not working. or not what i think it is?. this code:```it('gets ready'. function(done) {  require('electron').app.once('ready'. done)})```when run with electron-mocha gives a mocha timeout error (`ensure the done() callback is being called in this test.`).is this expected?", "yes that's to be expected: electron-mocha runs your tests in electron's main process. or. if you pass `--renderer` in a renderer process. in both cases. electron-mocha is responsible for starting and initializing electron. when your tests run. electron has already been initialized. so the `ready` event has fired before you start listening for it.", "cool. that makes some sense. this worked:```const {browserwindow} = require('electron')it('makes a window'. function (done) {  const options = { frame: false. height: 768. width: 1024. x: 0. y: 0. show: false }  const win = new browserwindow(options)   win.webcontents.once('did-stop-loading'. function () {    win.webcontents.savepage('out.html'. 'htmlonly'. done)  })  win.loadurl('')})```", 'unit testing infrastructure. included mocha as dev dependency and a few tests for the createlambdaproxycontext.this branch was created on top of the lambda-proxy-event.', 'wow. thanks for all the work.', "don't mention it.best way of making this plugin grow safely is to build a solid ground of tests.we can put them to run in a travis ci job.", 'just added hapi integration testing.', "enable handler mocks. not sure if you'll want this in but thought i'd offer it up just in case. i am using the plugin to do contract testing whereby the endpoints are spun up as part of a mocha test (by calling `start()`). these changes let me override the `require` of the handlers thereby enabling me to mock/stub dependencies - particularly those that do comms (i've been using `proxyquire` but you could use anything). anyway not sure if it would be useful to anyone else...", 'leonardoalifraco. thanks a lot for this! it rocks and so do you.', "thanks jgrigg. even if this is a bit too specific. if more users want this. i'll reopen :+1:", 'thanks!', 'we use mocha now.', "i'll fix this soon. but this can't be `auto-merged`. `npm prune` is not needed. and i'd like `npm test` to directly execute `mocha -r spec` so it works on windows as well as travis.", 'this would improve test coverage merge visionmedia ?', 'use local mocha instead of globally installed. this is not depend on globally installed `mocha`.', "if this idea is acceptable. i'll rebase my commits.", "npm does this automatically for you.  it's one of the advantages of using `scripts` instead of `make`.  the other advantage is cross platform support.", "i didn't know up to now. thanks :smile:", 'ah nice yeah the bdd ui would be nice. the filter tests were commented out since they were pretty volatile for a while but we should definitely get them working fine again', 'i rebased and sent another pr. #866', "windows: fs.watch() needs an error handler!. i noticed that the file watcher created in createfswatchinstance() is not getting an error handler. you need to add it though. otherwise errors while watching will bubble all the way up to the uncaught exception handler in node.a good test case to reproduce is having 2 folders. one with chokidar git repo and one with a small app that uses chokidar to watch the git repo folder. then just run the chokidar mocha tests while watching.eventually i am getting this:change c:\\gitdevelopment\\chokidar\\test-fixtures\\add.txtadd c:\\gitdevelopment\\chokidar\\test-fixtures\\add.txt{ [error: eperm. lstat 'c:\\gitdevelopment\\chokidar\\test-fixtures']  errno: -4048.  code: 'eperm'.  path: 'c:\\gitdevelopment\\chokidar\\test-fixtures' }unlinkdir c:\\gitdevelopment\\chokidar\\test-fixturesadddir c:\\gitdevelopment\\chokidar\\test-fixturesunlinkdir c:\\gitdevelopment\\chokidar\\test-fixturesat this time chokidar is actually crashed and does not report changes anymore.", 'hm you are right. i did not see this. however i think there is still a bug. if i set ignorepermissionerrors to true. i still see the error bubbling to my error handler. i would not expect this.', 'thanks!', 'consolidate and write new tests using mocha. the tests in forge are lacking and split up using several different libraries (and custom code). they need to be unified and able to be run in both a browser and node.js. we should move all of the tests over to using mocha and write new tests.', 'this was merged into `mocha` which has been merged into `dev`. so closing this.', 'this is coming in `mocha` branch. will be pushed and merged into `dev` soon.', 'this is closed in `mocha` via 1ab44b1 and will be merged soon.', "wemeetagain nevermind. i confirmed that's working (needed a small fix. done in `mocha`. so you don't need to worry about it).", 'rendering improvements. add lodash.isequal dependency and remove legacy codeadd mocha/enzyme/chai/sinon and remove jestadd spec and remove old spec', "raydog i was also thinking it would probably be worth moving to mocha. or maybe node-tap which isaacs' has been putting a ton of work into.i'd be happy to take that on as round two of this work. i think it can be done separately though?", "erinspice raydog. i've managed to get stuff running on travis. i'm going to go ahead and rebase/merge this. but let's open an issue to discuss moving things to mocha.", 'bcoe how do we add coverage reporting to the mocha tests?', "erinspice `nyc mocha`. rather than `nyc ./test/run.s`. once you've got everything moved over -- coverage should stay at `86%`. which is a good test that the port went well.", "erinspice isaac built that part of nyc. it's amazingly clever -- looks for the node bin and replaces it with a new node-bin that adds coverage tracking.as a representative from npm. can i vote for using package.json's scripts stanza rather than grunt for kicking off our mocha suite? once you delete the weird .sh file i added. i think you should be able to simply run `mocha` -- seems like the tests were written in a pretty darn portable way. even if rolling their own test runner.", 'support unit test bdd.tdd. use mocha. to write unit test for every module', "this is a tricky problem because `q` and other promise implementations wrap the `onresolved` handler in try/catch. which is a bad idea if you call a callback in the handler. for the exact problem you're running in to.one thing to do would be to defer the actual `done()` call:``` jsbrowser.init({browsername: 'chrome'}).then(function ()  {  settimeout(done. 0);});```that way. the `mocha.run()` call gets a new call stack and the error will bubble all the way up.", 'great job dima.. ill pull in and play with mocha a bit.', 'hey. how about adding tools like `mocha` and `chai`. and then starting making unit tests when new features added? :)', 'all tests for zone now runs with mocha if you use "npm run test-mocha".i\'m not sure about how you want the main and browser_entry_part to be.i did a little duplication to make the configuration easy. let me know if you want to me to change that.', 'housekeeping node tests. renamed to unit-tests. - `npm run unit-test`. will transpile and run all unit tests- `npm run unit-test-watch`. will chain tsc with watcher and run mochathe unit testing environment has been cleaned up.test watcher has been added.the tns-core-modules have been added as module path and tns-core-modules have been added as prefix so one can require `tns-core-modules/ui/core` or `ui/core` just like in {n} runtime.the `-common.ts` files have been cleaned up from platform specific types. so the compilation of the unit tests can be ultrafast (no compilation of platform declarations).', 'enhance test framework.  - nicer reporter (mocha) - adds jquery and materialize plugins for testing - adds test folder to linter runner - fixes some linting issues in tests', 'wip convert to typescript. ## todo before merging- [x] get rid of all `.default`- [ ] ~~add todo for all `:any`~~ (we can enable tslint rule later to prevent any)- [x] eliminate weird method map- [x] namespace in  data.ts- [ ] make output js  tabbed by 2- [x] make mocha know about ts line no. - [x] make `countdisplayname` a constant - [x] rename module usages from `axis` to `vlaxis` to avoid name conflicts- [x] get rid of all `exports.`- [x] use an enum for x. y. col. row. ...- [x] use an enum types- [x] get correct son schema- [ ] code coverage in ts?## todo if we merge- [ ] describe the schema in ts- [ ] enable more tslint rules (and fix issues)  - [ ] replace `var` with `let` and `const`- [ ] fix istype in compass (and vlui. ps. vy if applicable)', "all tests passing. there are some refactorings/ annotations we can do but i'll leave those for later. let's make a decision whether we want to follow this typescript path or not.", '> get rid of all .defaultyou mean in the tests?', '> you mean in the tests?> in the main code for sure.   and in the tests (if possible).', 'kanitw i think we are ready. opinions?', 'yaay', 'hah. yeah. i just saw the "it" and assumed i knew what it was. never even heard of mocha. apparently there\'s like 2 million javascript test frameworks now.', "so tap parser says one failed test. that i can't attribute to any test defined by me when running that last test that succeeds... something feels off somehow. didn't have any problems with mocha here...", 'found my issue. i was rejecting a promise for all commands that did not trigger :) i originally implemented this behaviour to do stuff in case a command is not specified.i did so with "q" back in the time when promises that were rejected but not catched did so silently without causing any trouble... now i switched to native promises (and from mocha to tap) and this behaviour popped up.so not tap\'s fault. sorry for your taking some of your time. mea culpa :)', 'how can i specify the source directory?. i am using```mocha --require coffee-coverage/register \\       --compilers coffee:coffee-script/register \\       -r html-cov --bail test/ > coverage.html```how can i tell it to check all files in the `src` directory for coverage?', 'your tests in `test/` should `require` files from your `src` directory and test them.', "so if a `src/` file isn't `require`d. then it won't be covered?", 'correct.', 'pkoretic - this is running the existing mocha tests. or with a custom script? if the latter. please post it here.', "fix(test): result is not defined on error. just passing throw your code. i find it very hard to test...are there any reason not to use mocha or jasmine to deal with the test environment ?wouldn't it be cool to use mocha compilers to test the code in traceur and 6to5 for example ?", "just passing throw your code. i find it very hard to test...are there any reason not to use mocha or jasmine to deal with the test environment ?wouldn't it be cool to use mocha compilers to test the code in traceur and 6to5 for example ?", 'cool thanks', 'chore(test): use mocha. following #167 and #247~~here is a start with half of the tests.wait for it :)~~', "nice! thanks for working on this. just let me know when it's ready for review.", 'k almost ready', 'ready for review guybedford :+1:', 'ok let me see if i can get something going then too.are you meaning self-polyfilling for the loader or for promise? you think system should be separated?', '"self"-polyfilling for everything. what you think about separating the different key parts :- ie8 stuff //optional- promises- es6 loader api- systemloader layerinstead of writing everything in the same module.it comes from the idea that the polyfilling level comes with the browser level. in my case (i already have a polyfill layer for my targeted browsers and it\'s bothering me to see the _es6-module-loader_ having a partial duplicate of it on it\'s own...)so for me **es6-module-loader must be es6 loader api + systemloader layer only** :tada: what you think ?', "i agree with you in principles. but i'm not sure i see the practical benefit actually - the ie8 stuff is very minimal. it's just a few lines and not really polyfills as they are local functions and all the code has to take ie8 support into account.then there is already a build that separates the promises polyfill.then in terms of the loader v system loader. there is no use case to separate them either. loader and system go together.the goal is to make this very easy to get going - that's why we value coupling. already the `register` extension and others are separated into systemjs.what would you like to be able to do that you can't do today?", "i'm still trying to wrap my head around how the _es6-module-loader_ is working...i want to know what's actually necessary to make it work. and i think i'm slowly getting it know.thanks", "sure. glad to hear you're getting into it. happy to answer questions where i can!", 'grunt + qunit', 'grunt to start up server. and mocha to test with. you could also use a testrunner like karma', "a few remakes to note. just tried the last keystone with yo and it rocks ! but i have a few remakes to note:- user model:  what append if i remove it ?  it could be great if keystone create a default models if it is not declared.- flash message middleware:  what append if i remove it ? it can be a database issue if messages are pushed on session but never popped ?  a default behavior with the ability to customize it can be useful.- wath about yo sub-generator for usual module like blog. model. grunt. glup. test.... as i do not have to decide at the beginning what i need.  for instance `yo keystone:make-test` could generate a makfile with dependencies to jshint. mocha. blacket. and a simple tests for homepage with supertest in addition of my existing project.- is dotenv a dev dependency ?- i usually use supervisor for dev. maybe can be useful with a `npm start` config ?- for tests i need to run keystone in silence mode but can not find option.- if a model filed is required it must be initial or i can't create documents.", 'html reporter never updates it\'s progress. and never sets the "finished" class on the #mocha div . we use this in our ci to tell when the tests finish. is there a workaround?', "fushi can you elaborate? i'm not quite sure what you're talking about", 'the tests are done in that picture. but the completed percent shows as 0%. and the mocha window is still yellow. because <div id="mocha"> doesn\'t have class="finished"', 'fushi this looks like you are not using the default reporter. can you provide a repository to reproduce your issues?', "that is the default reporter. as far as i can tell. i've done nothing but `ember install ember-cli-mocha` and `rm -rf node_modules && npm install`.then i run `ember serve` and browse to `/tests/`.can you show a screenshot of what you're expecting?", "fushi i'm pretty sure that this has worked at some point in the past. but apparently it no longer does. i've been able to reproduce this in a fresh new app.", "> we use this in our ci to tell when the tests finish.why don't you use `ember test` instead?", "we're moving from ember (non-cli) to ember-cli. and the current method of build/test for ci is the following:build on build serverdeploy to dev environmentrun selenium from build server using a grid of workers. against dev environment url with `&grep=` get arguments.i'm not opposed to changing that. i just don't yet know how we'd go about parallelizing test execution in the new way of doing things.the main issue i'm having is that i need the test results back on the build server. but i'm not sure i can run anything graphical there (and phantomjs doesn't seem to run any of the integration tests)", 'use "ember-mocha-adapter" from "ember-mocha" npm package. /cc rwjblue teddyzeenny', 'fix mocha tests. node tests were erroring out on this. need to have "./connection"', 'ack', 'thanks for contributing. 282 tests break with this patch. you can run the tests by running "mocha". which you can get with "npm install -g mocha". can you investigate why the tests are breaking. and then issue an update to your pr with fixes?', "that's pretty bad haha. i'll investigate later today. it may turn out that this patch is more trouble than it's worth and i'll just do things the right way so i don't have to moneypatch the parser. the problem is that my encoding is nonstandard to both the bitcoin protocol and this library - it isn't the library's problem to deal with.sorry for missing the mocha config. the class hierarchy of this project is kind of weird - it took a couple of days to understand where all the tentacles go. i think. on top of a paragraph in the readme on testing. it would be prudent to move all the code to a lib/ folder to separate it from the structural things like mocha and jshint config.i love the idea of bitpay getting behind the development of a library like this. and i think it has a lot of potential. with that said. i have more more unsolicited opinions if you're interested :) i was writing my own library but heard about this last week and decided to save some work for myself.thanks for taking the time to look at this - i plan on contributing more.", 'oh. i got response from deebloo. and thanks. so i will close this.> angular-cli using mocha for its own unit tests> jasmine for the generated project> that is the angular-cli source code> > out of the box the generated angular-cli projects are using jasmine', "robwormald - fair enough.  turns out bundling is typically the only problem i need help with - testing etc. is easy enough calling special purpose tools like mocha from npm's `scripts`.asynchronous module loading is a cool idea in theory but is one of those things that i've never actually needed in a real app. the bottlenecks have always been somewhere else. maybe one day i'll need it - who knows finally for the codegen stuff - i guess i need to wait to see it in action before i understand the value. it's not really in my feature list as a user of the framework right now.", "prevent mocha test failure from issuing grunt warning. when i'm running grunt-mocha-test in a watch. i want to be able to keep running it every time a file changes. if grunt-mocha-test calls `grunt.fail.warn()`. it'll kill the watch as well. unless i run the task with `--force`. is there a way to prevent this from happening? if not. would you be amenable to a pr implementing that?", "are you using grunt-contrib-watch?there's only a couple of places where grunt.fail.warn is called and that's when there is a problem clearing the require cache and i've not seen that happen for a while. do you have an example of when you run into this?", "separate config for grunt tasks?. is there a way to set a separate config for different tasks?  for example. when running locally. i want `ui: 'bdd'. reporter: 'spec'`. but running in jenkins i just want: `reporter: 'tap'`also. is there a way to engage the mocha `check-leaks` feature via `mochatestconfig`?  i tried `options: { ignoreleaks: false }` and tried to create some global variables in the tests. but it does not seem to be working.", "not sure about the leaks stuff but if its not in the programmatic options then it won't currently be supported", 'just checked the docs - looks like it should be possible to add support for this. will look into it', 'awesome -- thanks - i will test it out tomorrow.  fyi the growl task seems to already work. if i add it as an option: `... growl: true.`', "oh really. i hope i didn't just break the growl option :o", 'custom launcher errors in ci mode. i\'ve configured a custom launcher as follows:``` yamllaunchers:  node:    command: "mocha -r tap test/*_test.js"    protocol: "tap"launch_in_dev:  - node  - chromelaunch_in_ci:  - node  - chrome```i\'ve introduced code in my tests that causes a javascript referenceerror. you can replicate the same with something as simple as `f = new foo()` without defining `foo`. when i run this with mocha by itself. i see the stacktrace and mocha exits with an exit code of `1`. as expected. when i run the same mocha command as a custom launcher via testem. the stacktrace is not shown and testem exits with an exit code of `0`.[addition]i forgot to add that this is happening in **ci mode.** not in dev mode.[/addition]can anyone shed light on why this is the case? this ends up silently swallowing errors that prevent my tests from even running. which is highly problematic.', 'i tried to setup the same scenario. but i am not able to reproduce - i do see the stacetrace. see gist', 'whitecolor i created this gist to try to replicate the issue but could not:  you try it and see? although. i am printing out terminal color escape codes. i wonder whether that can cause some terminals to choke.', 'i just tried it out. and things got a bit weirder.the setup in your gist does work for me as well. however. if i mv `error_test.js` into a `test` folder and edit the mocha command in the launcher to account for the placement of the test file:```mocha -r tap test/*_test.js```that does not work as expected in my setup. it swallows the error.thoughts?', 'interesting. i will dig deeper into it. maybe a job for promises?', 'yes. thanks. it works =) close the ticket if needed.', 'thanks! i will close this. but i will try to get my fix into mocha.', 'opichals no really. i want to know :)', 'to watch the tests not to exceed specified time.as i think about it it might be useful to have per-suite and also per-testcase timeouts to be able to validate performance regressions.', "gotcha. it think this is better captured on the client-side and then sent back to the server side for reporting. which means work will have to be done for each framework i have to support. but i think it's worth it.", 'well. if the timing is measured between the socket messages received. the work should be generic if i am not missing something.', "we could do that but it's not very accurate.", 'in case it helps. i took the time to document my use case in a video overview that outlines why i chose to test in the browser. and why asynchronous assertionerrors in mocha are important for my project:', ":+1: for this feature. for exactly the same reason. i'd like to have some complex code that decides if a test should be skipped. and have the test output show a skip. not a pass. i'd be ok with calling `skip(); done();`regarding skip() magically getting out of the test case -- how about throwing an exception just like when an assertion fails?", "actually. i'm now :-1: for this. simpler to just change your test runner to use the tap reporter manually (plus use  instead). plus you might want to use a different reporter in the browser or log something else to the console", "perhaps the ci thing is leaving us with a little bit of a red herring here (i think we use them in quite different ways).i think it presents a cleaner simpler abstraction to reporters and is advantageous because of that. and i think it would be nice to have the ability to run the reporters as separate command line apps and pipe data from the test runner to the reporters (e.g. for aggregating multiple tests or using non-mocha test frameworks with mocha reporters) and i can't see any way of doing that without simplifying the format to be json serializable.", 'dmayo3 are you using mocha-phantomjs?', 'with mocha-phantomjs the codes runs in a browser environment but output goes to the console.', "oh right. forgot about those people haha. i've never seen numbers but yeah i've seen that. i definitely dont want to inline a bunch of serialization or json stuff. this would be less gross if mocha were implemented with components right now but for now i'm find with just the regular tostring. they'll figure it out", "haha! ok ok. i won't pester you about it. i really just want mocha to support being passed a stream object for output. and having it default to standard out if none is given.i don't plan on this being built in to exclusively support grunt. i'll just toss you a reply here when i'm done and if you think it might help. i'm happy to issue a pull request. if you only use make in your exploits. then hats off. i'm not so fortunate... gyp is also json-ish like grunt. and actually i think msbuild and ant are both more intuitive with their xml based configs. probably scons is the only thing i really enjoyed building with. but it's a beast and slow compared to make or ninja.", "document difference between `mocha debug` and `mocha --debug`. visionmedia: it took me a while to figure out how to get mocha to run with node's built-in debugger client to run (vs node-inspector / webkit web inspector). so i thought this docs clarification might help others stumbling over the same thing.", 'seems i can\'t pass bail option with options to mocha constructor? i\'m using grunt-simple-mocha and can pass options only via grunt config. because my test folder named "tests" and if i recall correctly mocha only loading mocha.opts from "test" folder.', "mocha doesn't have anything to do with the http lib sorry. you'll have to ask elsewhere if the request itself is failing. though keep in mind mocha exists when tests are complete. thus you would have to put this code in a `it(function(done){  request here. invoke done() once complete })`", '( and a corresponding v bump for mocha eventually would help :) )', "you'll have to step through with node's debugger or investigate the stack trace there. doesn't look related to mocha sorry! some sort of race condition in that lib", "update mocha. that's really old", "im open to suggestions on improving the output but i dont want to get all crazy with customization. it's easy to integrate new reporters with mocha so you can copy/paste it and go from there if you like", 'added a `.bail()` method in f0b441c', 'davisford yea wouldn\'t surprise me. node itself doesn\'t publicly facilitate clearing the module cache. so anything we get "working" is a hack. another reason why i dont like watchers. but i wont have time to look into this for a bit', "i don't think the styles will be too much of a problem -- the mocha layout and css is already pretty well scoped. and the markup fairly minimal. i added it to a single page app with a white on dark layout which drove most of the changes.personally. i think the combination of this with testacular will be pretty powerful. by running the tests directly in the app. there's no barrier getting in the way trying to send events to a browser over websockets. for example. if a test fails. you have your app open in your browser with the state that the failure occurs so that it's easy to debug. you can also hook into your app. so listening for routing events. for example. which keeps the test lightweight and fast. and then you can run them through testacular to drive the browsers for continuous integration.", 'i am having the same trouble. it\'s very difficult to get a breakpoint set inside a test. i\'ve been trying to wade through the mocha source to try and get into a function that will let me set a breakpoint on my test. but i still haven\'t had any luck.i\'ve tried setting a "debugger;" in my source. but it still does not get caught in time.+1 for chris rock\'s suggestion.', 'i ran into the same problem. but worked around it by forking (oh no!) mocha and pasting the jsdiff code into the empty diff.js file. radamant is correct.', 'after digging in the code of both mocha and node. i implemented a simple workaround: it the way to go?may it be possible to automate it in a function call like `removeuncaughtexceptionhandler` in the mocha api?', 'expected/actual output is poor without color. example:<pre>% mocha --compilers coffee:coffee-script -c     1 of 3 tests failed:  1) filterdata should work for nested hashes:            actual expected            1 | {      2 |   "outer": {      3 |     "middle": "value"      4 |   }      5 | }{}</pre>in this very simple example you should be able to tell that there is a non-empty hash and an empty hash. looking at just this output you cannot tell which of the two is expected and which is actual.maybe you could do diff-style +/- when color is off.', 'unable to run tests in private network . pleaseeeeee help me fix this blocking issue. i have deployed a sample contract to a private blockchain. embark deploy privatenet works perfectly fine. when i run test. it fails with the following message. i guess. it is not able to find the blockchain and the account. i am running the command embark spec and embark spec privatenet.error outputd:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract>embark specsecp256k1 bindings are not compiled. pure js implementation will be used.secp256k1 bindings are not compiled. pure js implementation will be used.conferenceprimary account address is : nulltrying to obtain conference address...    1) "before all" hook  0 passing (3s)  1 failing  1) conference "before all" hook:     error: the string "invalid address" was thrown. throw an error :)      at runner.fail (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runner.js:226:11)      at runner.failhook (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runner.js:262:8)      at d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runner.js:307:16      at done (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runnable.js:287:5)      at hook.runnable.run (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runnable.js:322:5)      at next (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runner.js:298:10)      at immediate._onimmediate (d:\\aaa\\xxx\\blockchain\\installed\\embark\\firstcontract\\node_modules\\mocha\\lib\\runner.js:320:5)', 'thanks a ton!!!!. i am unblocked and the simulator scenario works!!!!. i will fix my test cases now. **i have one more issue on which in need your help**. i really want to connect to the node that runs my contract (not the simulator) on the private network. the accounts there have enough ether. **what changes do i need to make**?', "thank you!! it is working. i am able to test against the privatenet node. i would continue to use the simulator to test. it's a great tool.", "add socket.js tests. this pr adds a few mocha tests for the `socket` object in `phoenix.js`.i'm opening this pr as a potential starting point for building out a suite of unit tests for the javascript functionality provided by `phoenix.js`. though only just a few tests. there are already a few key decision points where i'm looking for feedback before i get too far.though `phoenix.js` touches a few browser apis. i decided to stick with the node.js context for the tests for simplicity. this means introducing a few libraries. e.g.. `jsdom`. `jsdom-global`. `mock-socket`. to mock out those dependencies. we could certainly run the tests in a browser-based runner. e.g.. karma. testem. etc.. instead (or do both!). though the approach i went with was consistent with the existing presence tests. looking for comments.i've only added a few tests to get a proof-of-concept going for the mocking approach. if we like this direction. let me know your thoughts on how to tackle remaining tests: merge small sets of tests as i go vs build out an entire suite in one pr. certainly. if you have any comments on the tests i've written so far. i'd be happy to address.another decision would be when to add the node tests to the travis build.finally. there's never enough love to go around. so thanks for all your hard work on phoenix!", "this looks great! and we surely could use unit tests for proper client coverage. > i've only added a few tests to get a proof-of-concept going for the mocking approach. if we like this direction. let me know your thoughts on how to tackle remaining tests: merge small sets of tests as i go vs build out an entire suite in one pr.please continue with the tests. using this pr. and we can merge when we're in a more complete state. thanks so much!", 'model tests. create mocha tests for:- [x] chat model- [x] author model- [x] room model', "testing?. how do you run the tests? i don't see any instructions for running them. the syntax looks like mocha. but every test failed.", 'it\'s `mocha .` and make sure you have a db that we can run the tests against called "massive":```createdb massive```', "if you didn't create the user `rob` with password `password` and superuser permissions on the massive db. that would definitely be a contributing factor.", 'or just replace that with valid connection info. also. use `mocha` without the dot - sorry my bad :).', 'did some testing. the only things that can be safely removed are the mocha references and the empty concat and uglify task blocks. everything else is used. including the references to htmlmin. cssmin. concat. and uglify in the task declarations at the bottom.', "help wanted: testing framework. live-server has some tests in the `tests` folder (currently using mocha and supertest) run with `npm test`. but they are really bare bones. i have little experience about node.js testing frameworks etc. so some pointers/ideas/contributions would be appreciated. specifically these things:- i'm more interested in testing broader user-facing functionality (both the command line tool and the node api) than javascript source level unit tests (the implementation is just a few functions which i think would be rather difficult to meaningfully unit test)- is it somehow possible to automatically test behavior with a web browser / websockets (e.g. that the live reload works and css reload happens without page reload)?- what is a good way to test command line apps from node in a cross-platform way? the current tests in cli.js feel hacky (use a timeout in the live-server side to exit the server etc.). i'd want to at least run live-server with some params. touch some files (to test watching). kill the server and capture output for examination. most likely one test after another (not parallel) in order to not interfere with each other.", 'when `mocha-phantomjs` switches over to `mocha-phantomjs-core`. it will get this work.', "> (put another way. while phantomjs offers those command line options. i don't see how to supply them via mochaphantomjs().)unfortunately it's because you can't - see #186 however the alternative way to do this has already been stated in the thread:```phantomjs --ignore-resource-errors --ssl-protocol=any lib/mocha-phantomjs.coffee <page> <reporter> <config-as-json>```", "interesting...i'm a fan. call me a noob but i haven't seen this pattern before...i imagine it cleanly transfers over to the mocha-chai world?", 'meh', 'right... now looking at the implementation. it filters by certain sub-directories and not spec name? might be easy to pipe a --grep="settings" option into mocha... i can make a pull request if interested?', "theoretically you can run exactly the same specs using this method as you might using the --filter way.just depends on your preference and if you want the ability to target a director as opposed to spec names...i prefer to use the grep option as it's consistent with mocha and more powerful... also you might want to consider if users will be confused by two alternate ways to run specific tests.totally up to you.", 'automatically running a bare mocha reporter on brunch watch. hey guys. i propose an additional flag to `brunch watch` command called `t` to automatically run mocha tests after each compile and output them in the terminal. alternatively we can add a `w` command to `brunch test` to do the reverse. but that would also mean adding `s` as i am developing locally using the built in server. any concerns with ether approach?', 'gh-378', 'get rid of jsdom crap. mocha-phantomjs ftw', '+1.. or casperjs integration', "i've experience so much pain and time wasted with jsdom and trying to get testing going. this will really help our team!", "there already exists a browser test suite. `$ make test-browser` will build all the tests files for delivery to the browser. then you can open `test/browser.html` and it's a standard mocha setup (no idea if it even still passes. been a while since i ran it and it's not a part of ci. hence this issue). no idea about browser support. whatever passes i guess :stuck_out_tongue:", 'the idea would be to still have it in mocha. just using karma to run the tests in the browser instead of some other setup. (karma only takes care of starting browsers. loading mocha and your code and then reporting about it)', '/shrug not sure what it will save us ... plus sebmck has already solved a fair number of issues with mocha-fixtures that we would have to redo :taxi: :fire: :e-mail:', 'seems like a nice addition! it would be much easier if wdio-mocha-framework would expose the current allure instance. however i was not able get it to work correctly. in standalone examples it works like a charm. but when running via wdio. everything is different.', 'integration tests. - launch webdriverio multiple times and verify the allure reporter xml output- in `test/fixtures/specs/` different test scenarios are defined that are used by `mochatest`### test scenarios- "before all" hook- "after all" hook- screenshots- suites', 'good job. gronke i am not sure that this could be called unit tests (it looks more like integration). but it is still good thing.', "> i am not sure that this could be called unit tests (it looks more like integration). but it is still good thing.you're right. we should them call them integration tests. imo it's more interesting seeing webdriverio in action than mocking the basereporter and building real unit tests. even if the tests should be written like unit tests. i would not use this reporter without webdriverio. so it feels better to include it in the tests than mocking it.  **update**: renamed to integration tests [?]"]
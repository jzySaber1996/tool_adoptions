["medikoo that problem already exists if you use npm to fetch client side packages. what if the package your trying to use doesn't exist in npm? for instance. you got a client-side project that is using async. jquery and some jquery plugins. you will end up managing the jquery plugins yourself because those aren't in the npm registry. in this case npm is not covering all packages and you end up manually handling them. if you use bower. you got a much variety of endpoints to fetch packages. you have git endpoints. local repositories. url's. tarballs and zips. in the example above. bower would be able to fetch async. jquery and the jquery plugins.in my projects. i manage client side packages with bower and node packages with npm (build scripts. mocha tests. etc).", 'calling lambda-local as a node.js module. i want to be able to call lambda-local from another node.js program. as a module.right now it only supports command line execution. my immediate need for this comes from not being able to write mocha tests against the `lambda-local` execution. but i am sure it will be useful in many other situations too.', 'i suggest you start with a minimal example first; in your case something about the babel setup does not work and overwriting all the dom globals is problematic. too. obviously.without digging deeper. i can only say that using babel via `--compiler` works for me with electron-mocha but my babel configuration is limited to features which are not supported by electron already. electron is basically node 5 and chromium 49 -- that means you get almost everything from es2015 for free.', 'handle --compiler before --require options. fix #57 this moves handling the `--require` option into the mocha file. to make sure that `--compilers` have already been loaded. jprichardson can you confirm that this is ok?', "wondering about if there's a way for us to build in the instrumentation into electron-mocha itself. i would hate to require that people go through multiple explicit build steps in their setup.", 'utack', 'i could not understand what inukshuk explained. can someone please post some steps how i can make istanbul work with electron-mocha? thanks', 'i was able to instrument my source code but i am not able to figure out how i can use this to run my tests in electron-mocha. i am piping my files from test folder onto electron-mocha. where should i add the option to use instrumented code for coverage?', "fix exit code. if we destroy the only window. electron seems to quit with an exit code of 0. so mocha's success/failure won't be communicated properly.we don't really need to worry about destroying it anyways. since we're about to exit.", 'this seems reasonable. inukshuk what are your thoughts?', "hrmph. i'm not sure i can make this and #56 play together nicely. but i'd argue that this _is_ more important. since test reporting is currently broken for ci use cases.", "alright. so this _works_. but note that as of #56. we're no longer cleaning up `browserdatapath` at all since the call to `win.destroy()` exits. so it might be worthwhile to simply revert that pr and delete this one.", "i think joshaber is spot on here. reverting #56 would take care of the bigger issue for now. though i'm curious as to why clean up is being skipped at all. i haven't looked at it yet. but the first thought i had was that electron just exists cleanly when all windows have been closed (i assume) so perhaps we'd just have to handle the all windows closed event and exit from there (though clean up may have to be a sync function not sure)? i'll take a look at it later today!", 'hmm.. does  "... && node bin/electron-mocha test/main ..." work?', "i think that was it: if i just listen for `all-windows-closed` then the clean up takes place. i think the overall cleaner solution is to use `app.exit` instead of `process.exit` -- according to the docs this should force close all open windows. so i would hope that the issues addressed by #56 are taken care of this way as well. this way we can move clean up to `app.on('quit')` too. see #62 -- what do you think?", 'lgtm', 'awesome. thanks inukshuk!', "nzakas i'm using expect.js with mocha in shift-parser currently. it supports deep equality test and it supports visual diff as well. not sure if it's mocha's functionality or expect.js's. let me try to reused the current runner first.", 'fix the tests for the build. to be able to have an effective ci process we need to get to a point fast where the build is passing. to achieve that we have to fix the mocha tests that are currently failing or remove them if they are not necessary or helpful any more.', "flomotlik this is great to hear. our team's most urgent expectation is test-ability. at the moment we are using `serverless-mocha-plugin` (which had some bugs that i created a pull request for). even though this plugin creates a nice template there are gaps remaining. such as how would you mock end-points (s3 for instance). test internal functions etc. looking forward to your team's work in the test area. thanks.", 'use tape and testling. instead of mocha', 'upgrading mocha and chai.', 'adding the mocha harness', 'ok. sounds good. i started a test suite using mocha in `./nodejs/`. should be trivial to expand on that now to consolidate the test suite.', "when i was in there looking around a saw a lot of easy improvements. #612 stuff i didn't do was output something if it failed to connect to the database (check out `bin/sequelize` around where it says `var sequelize       = new sequelize ...` because it would be around there where you could check to see if the db connection is any good.if you know anything about outputting text (i use `console.log` and that's it) maybe some of that fancy ncurses stuff you see in test suites (like the mocha reporters? i haven't looked into how they do that) it could be made much better.", 'thanks beatfactor!', "update gruntfile.js: mochatest through watch should run in 'test' env. mochatest task needs to be run in 'test' environment. to avoid db flushing when mochatest is called by watch. first call 'env:test' task before 'mochatest'", "in my opinion. a mocha loader would be great. currently i am preferring grunt over webpack for just two reasons: you do not need to write a build.js but a simple configuration file (gruntfile). and it comes with support for mocha. if building my web project from node source and still running tests etc. on node was that easy. that'd be a huge plus for webpack and a dependency less for my projects.", "don't see anything wrong. but also never used mocha with testling", 'an alternative to this is to do some rewriting of the code. so that a normal javascript test framework can be used(eg mochajs). need some dependency injection to simulate filesbut hey. there is not a lot of logic in this project. so this the shell file will work', 'copy over waterline tests. better yet. let them live in waterline. but call them programmatically with mocha.besides testing the waterline core (todo). we should also be running our adapter test suite on at least the following adapters by default:- sails-dirty (in memory)- sails-dirty (persistent on disk)we also might exploring leaving a hosted remote instance of other data stores running for easy testing. e.g.- sails-mysql', "we have a good test suite for our active record implementation right now. but there's not a lot for the other core features.  i've got a story to create an integration testing suite (#139). but it's probably not going to be for a couple of weeks unfortunately.  we're using mocha", 'if you run this test independently:```    > mocha test350```everything passed ok. that is why i passed it to release. so. this is a problem of interference one test to another. but ok. anyway it is a bug.', 'the ember core team decided it would like to consolidate the officially supported testing repos in community-controlled repos to improve visibility and control. at the same time. we want to continue to recognize the importance of the investment that switchfly made to get these projects off the ground.i have contacted pchen12 from switchfly to discuss moving `ember-test-helpers` and `ember-mocha` to the `emberjs` org. and `ember-cli-mocha` to the `ember-cli` org. rwjblue has already moved `ember-qunit` over to `emberjs`.', 'add mocha to package.json', 'oh. let me recap. the thread i posted is about webdriverjs and not wd.however. the behaviors also applies to wd.when one operation fails in wd. mocha is unable to go further. and fails independently of the `--bail` option.', "i was unsure myself. i've never used mocha or should.js before. i just browsed to the test/ directory (via http/https of course) to see if there's a possibility. and voila ...", "looking at the docs for --require and knowing how require works i think this might be a path issue. it's also possible that the require option is handled differently within mocha to other options so i will look at that too", 'actually i see now how blanket hooks into mocha - maybe it would be better integrated', 'i think of it like if you allready using `grunt-mocha-test` after updating you will get additional task `coverage` (or whatever name). and you can run it without any additional work (with default settings).', "just wanted to add what i've done since yesterday. by the way. i noticed you commented on jeffrey way's tutorial. that's what i'm working on and having this problem.at the end of the tutorial mocha starts complaining about global leaks but the problem seems to go away when i manually open chrome to localhost:7357 . not sure if or what that means.i enabled logging by adding the --enable-logs and --v=1 args to the args array in browser_launcher.js and when running testem it was showing warnings about problems with some extensions.long story short. i ended up removing all extensions and then ended up completely removing chrome and re-installing it. still getting the same behavior.fyi the latest installer installed chrome.exe to c:\\program files\\google\\chrome\\application\\chrome.exe instead of the usual place so you might be needing to update the list of paths. i tried adding the path to the array of locations in browser_launcher.js just to see if it worked and it didn't so i copied the application folder over to appdata\\local\\google\\chrome\\application  and that works  for 'testem launchers' and for manually running chrome.", 'does the issue come from the use of program.parse another time. or from the need to augment argv before parsing in itself?', "the need to manipulate argv based on a flag that commander parses. parsing twice might have some strange side-effects im not sure. haven't tried. maybe it's ok but that's still pretty hacky", 'debug just failed tests. i was wondering if there could be an additional option for vows that would allow debugging but only of failed tests.this would allow me to have mocha running in one window spitting out all the results but another terminal window(/pane) could be responsible for letting me debug against the failed tests.this would allow a workflow:- modify the code or tests- mocha is watching and triggers the tests- the new step is triggered for any failed tests- debug the first failed test and hopefully then fix the code/tests which triggers a run of all tests.', "yeah wouldn't be mocha related we're not altering messages", 'we could separate stdout and stderr (not sure why we are not atm actually...) in which case you can just display stderr. or optionally use `--bail` which exits on the _first_ error', 'prevent logo overlap on long file list. when the list of files is very long. the list of files can extend past the bottom of the screen. and the mocha logo overlaps them.', "yeah perhaps. ideally i didn't want any custom mocha behaviour there but i can't think of a reasonable way around .showdiff", "sounds good thanks.bail is useful for sure. one of the many useful features in mocha. however when i do bail i sometimes end up wanting to debug the failed test (or the tests in that file). i know i can debug the entire test suite easily but it adds more friction especially when i've got breakpoints. anyhow as long as the input going to stderr had the name of the file containing the failing test i should be able to do what i want.", "executing a test run fixture before any tests. one feature i'd find useful is the ability to run a file before any tests. the code in that file being a test run fixture that does some sort of setup.i realise i can get this behavior by requiring the fixture file from each of my test files. but i'd prefer not to have to not least as it makes moving the test files around harder (paths to the fixture ending up broken).i was thinking this could be controlled using another mocha option. only slight complexity i can think of would be if the test fixture file was inside the directory in this sort of situation:```mocha -r spec spec/ -w -g --recursive --runfixture spec/testfixture```", "i dont think it would be worth complicating. fixtures are so arbitrary do i'd rather delegate all of that to the user", "you can do `it.only('name` to only execute that one test. or `describe.only(` for that suite", "get you. at the minute i've just got a test run fixture file at the root of spec directory so it gets picked up first which works (other than when running individual test files directly). ta.", 'i just put initialization stuff in ./test/support or similar and require() that in each test so it still works standalone', 'closing for now', "this fails to highlight to the tester when a test has actually been skipped; calling `done()` early implies that the test has passed. it hasn't.", "yeah that's somewhat annoying with node. i tend to just minimize nesting anywhere possible now. our app is just lib/\\* with hundreds of modules in there. no nesting and no nested tests within each", "yeah that's true. it would be simple to add but it does seem like a strange feature to me at least", 'it would be really useful when testing optional dependencies. for example an abstraction layer supporting several adapters for different environments. currently we work around this in the following way:``` javascript(condition ? describe : describe.skip)("optional foo tests". function() {});```but this loses some of the natural readability of the tests.', "yeah that is a good feature. maybe i under-use it because i find if i'm using the watch feature and use describe.only then it does indeed just run the one test. but if i remove the only again it keeps just running the one test...but i'm thinking that's probably a bug and i'll log it separately.", 'hmm yeah maybe. i dont use watch so i miss that stuff', 'we must not be resetting the `mocha` state related things like .grep()', 'the "exports" ui does not match the commonjs spec (though it is pretty close). which says test keys should start with the string "test".  test functions should also take an `assert` module as an argument.  i\'m not sure that the "exports" ui could be altered while maintaining backwards-compat for mocha.we\'re currently experimenting with this at olark.  i actually did not intend to create this request upstream (yet). meant to do that on our own fork.  my bad :)', "mocha doesnt have anything to do with assertions so that's up to should / expect / chai etc", "if i forget to pass a callback in an express app. my response will hang and end up in my logs. if i forget the callback in mocha i'm not notified at all. a test framework with false positives is one i can't use.", "calling `done()` should error if it's not defined but yeah mocha wont wait around for it. hmm.. trying to think if there's a cleaner way to solve this", "added doctype to template. i added a doctype to the browser template that is used with `mocha init`. the reason for this is that the recommended assertion library chaijs. doesn't work in quirks mode in internet explorer. in other words. using mocha init with chaijs out of the box doesn't work in internet explorer. this resolves that problem.thanks!", "yeah this is reasonable. i like it. i think we should change mocha's bin to use find as well. or at least add that flag. because often you may have something like `describe('foo(bar)'.` where you dont really want to escape all the parens", "hey. i'm awaiting this to be merged as i'd quite like to use the framework that serialseb has developed using this functionalitywhat work is outstanding to make this happen? can i do it?", 'htmlreporter: fix "property \'appendchild\' of undefined".. ie9 throws the following error on the large.js test:> (mocha.js:2072) unable to get value of the property \'appendchild\': object is null or undefined.which is caused by the stack.shift() call earlier.', 'seems a little odd that only one browser would get this. since the shifting is just based on events we control this seems like duct-taping another problem', "this seems a good thing to have. especially with ci environments (currently i have some slow stuff on travis and i'd like to know what's issue).is this better implemented as a new reporter. which might extend dot. or modifying the base reporter?", "at this point a custom reporter (or built on json / json-stream) would be the way to go. i'd prefer to stay away from adding too much complexity to what we have already unless it seems really necessary. i'd like to remove some from core as well and put them in npm. like teamcity. i've never even heard of that ci so we should move some out", 'separate package', 'is that already updated on npm? i just installed mocha and im still getting global leaks detected: stats. report', 'i have the same error.', 'i think this is a dup. but currently we bail if a hook fails because we assume subsequent calls will fail as well. in the future we can change this to skip only that suite. but for now it emits "end" and mocha exits.', 'do you have a piece of code to show ? i\'m doing the following (using mocha) :``` javascriptbefore(function(done) {  this.browser    .visit(\':3001/contact\')    .then(done. done);});it(\'should redirect to 404\'. function() {  assert.equal(this.browser.statuscode. 404. "didn\'t get 404. instead got " + this.browser.statuscode);});```', 'yes. and mocha tells me all tests pass green and the command ends normally.', 'will do.if you want to fix travis-cl in the main yeoman package. i think all you have to do is increase the mocha timeout.', "make sure you have mocha installed: `npm install mocha -g` and then have postgres installed locally. create a login that matches our test login (or change it to suit) and then run `mocha .` - don't forget the dot :).", 'closing the issue as there is no further response.', "you are using schemaless urls with the `file:///` protocol. so it defaults to file. you won't be able to use those urls on a flat file - you'll either need `http` or `https` explicitly. or use a local server.btw this has nothing to do with `mocha-phantomjs` but just how the browsers work. if you open it up in a browser you will see the images not load.", 'fyi if you ever need to provide additional phantom args. `phantomjs lib/mocha-phantomjs.coffee <page> <reporter> <config>` is something we support (semver wise)', "i assume running locally produces the same result? try `phantomjs --remote i debugger sport=9000 lib/mocha-phantomjs.coffee tests.html spec` for debugging.phantomjs definitely has its quircks. and by quircks i mean big bugs. if you are doing heavy dom manipulation and verification. or relying on more recent dom functionality. you might not want to use `mocha-phantomjs`. sauce labs is really awesome if you haven't seen them.", "> maybe all flags that aren't recognized by mocha-phantomjs should be passed throughyes we should really be doing this. and would love a pr to do so. thanks!", 'if someone wants to take a stab at the refactor i think it will help a bunch. and will not be thrown away. the switch to mocha+chai is just syntax sugar.', ':thumbsup: for mocha-chai... mocha+chai+sinon = bliss!i do love a refactor', 'i think the issue is that i added "mocha" as a require. not fully groking how the system works.  i\'ll try to add some examples later.you should create a github site for this project. it\'s super useful!', "thanks! i've tried to add support for `require('mocha');` in test cases. but it's not trivial to do.", 'mattwynne aslakhellesoy looking at tackling this week and wanted to get your thoughts on this.', 'good news because i have no time to do that right. sorry.', 'closing this as i working on this in another branch and just taking a bunch of code from mocha.', "i've been getting this problem intermittently too. sometimes is works fine. sometimes zero tests. sometimes i get errors saying that some of the scripts are not available in the mocha test run. all this has started happening and getting worse as the project i'm working on grows in size.what actually happens is that 'file_list' triggers it's 'ready' event multiple times. so the directories are scanned for files as they should be. what's supposed to happen is when the files are all added. there's a gap bigger than 65ms. the file list. triggers a 'ready' event. the code gets compiled and the tests are run.occasionally for me there will be a gap bigger than 65ms between files being added before they are all added. so if there are 300 files. maybe a 'ready' event gets triggered after 200 of them. which causes the compile. and the tests to be run when not all of the code files are included.so that's the problem. i'm not sure about the best way to go about resolving it.allowing the reset_time to be configured would probably be quickest. but that is a work around.altering the way the file system is scanned so that it explicitly walks every sub directory before triggering a 'ready' event the **first time** it runs (but sticking with the reset_time for watching) seems like a better solution. so maybe a change to chokidar so it can say when all the files are initially found.what are your thoughts paulmillr ?edit to add that this isn't a problem when running the project up in a browser. even if the first compile doesn't include all the files there will be another one just after which probably will. if i understand it correctly doing brunch build without a watch should have the same problem. but i've never noticed it.", 'sebmck : i know that the issue for babel itself is solved. but how did u end up solving the problem in general? (i am trying to debug some mocha tests)', "cr0ck : could you please specify slightly more detailed how the working setup now looks like?i have tried with `mocha --compilers js:babel/register` as well and the sourcemaps don't seem to work.now i have copied your file. but how am i telling mocha to `read it` before the tests?", 'how are you running `mocha`?', "multi process scheduler. firstly: did a simple clean up of tests so they pass 90% of the time.  -- there seems to be a situation with mocha where the connection to redis can be closed and an event fire. which causes tests to fail that i did not resolve.secondly: added redlock around the 'every' and _onjobexpiry to force only a single worker that gets the key expiration method to schedule the next run of the job. this may need an improvement / modification for clustered redis scenarios. as redlock needs to know all of the redis servers it needs to get quorum from on a lock.", 'this should resolve #24 and #21', 'doowb my suggestion is that we change the output to _nothing_ instead of a dot for now. so that it\'s not broken and arkkimaagi can do what he is describing without problems (right?). that way this: ```href="{{root assets}}img/apple-touch-icon-57-precomposed.png"```would return:```href="img/apple-touch-icon-57-precomposed.png"```instead of:```href=".img/apple-touch-icon-57-precomposed.png"```> to add actual unit tests to this. would take some more refactoring since it would require pulling the asset path calculation out into a utility function that can be tested.yeah. i think this will be good when you can get to it. and it really should be done for all of the pre-defined variables in assemble. i started preparing some nodeunit and mocha tests yesterday in anticipation of this.']
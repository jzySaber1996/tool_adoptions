["i agree with everything satazor  said!> - should the end event be always fired? regardless of success or failure?i would say that the end event needs to be always fired. unless there is an error. in which case. the error event is fired.> one possible solution is to remove the data argument (optional) in the end event and pass a true/false boolean instead.ideally. the end event should emit something like `.emit('end'. null[. args])`. this is very subjective because you would use the error event to catch up and bubble up errors. but it makes things easier to always have the same signature (such as passing in the mocha `done` handler straight through the end listener)also. maybe consider bubbling instance of errors or null instead of true / false booleans.", "i'll close this for now as this isn't an electron-mocha issue.pezholio i suggest that you create your own window stub for now. there is something about the browserwindow prototype's native binding that's throwing off sinon here. you will probably get the same error if you try to inspect the prototype yourself -- sinon obviously runs into the same issue when walking the prototype for stubs to create.", "i don't know the exact scenarios where the app exit is not happening. i see electron gpu process and main process still running when i run tests (we have 100+ unit tests. and this issues happens only if i run all the suites together and not individual suites). so. i'm guessing it is a leak in electron which is causing this issue.to avoid electron-mocha failing due to this issue. i tried regular quit which works and doesn't change the functionality as far as i can say.", 'so when this happens. electron-mocha exits. but there are some processes left? or does electron-mocha just hang (`electron-mocha --renderer` should open one main and one renderer process -- and they both continue to run?)', 'all the tests are main process tests. tests create internally browser windows and close them. electron-mocha just hangs and electron main and gpu child process of main continue to run.', "ok thanks. i'm a bit hesitant to merge this. because i think `app.exit()` should just close all windows regardless -- i would expect tests which open windows to close them again so. ideally. electron-mocha would only ever close the window it opened itself. jprichardson thoughts?", "chandrachivukula juturu can you try to close each window opened by a test after the test has finished? if there is a specific reason that makes this infeasible. you could also register a global after filter and call `app.quit()` there. personally. i think every test that potentially opens a window should make sure the window will be closed after the test. if the issue persists even though the every window has been closed. i'd be more open to working around this directly in electron-mocha.the reason why i'm hesitant is simply that we currently don't know what exactly causes this problem (can it happen for any window or only if the window process acts in a certain way. e.g.. forks another process or something like that. etc.). if we add this workaround and the next electron release fixes the cause of this problem (whatever it is). we wouldn't be aware of it and we'd keep maintaining the workaround code even though it would be superfluous at that point.so. like i said. if this can be worked around by closing the windows after a test. then we don't have to work around it in electron-mocha (we can keep this issue open for a while. in case someone else stumbles onto this).having said that. it would help a lot if you could reliably reproduce the issue. because then we could report it as an electron issue and track its progress.", "hi michmich i've interest a work in a test suite for magicmirror. i think is somethings we need.i can do develop somes tests. this patch propose the use mocha for the test suite.i would like to hear comments and if  we can work a list set of test to do.", "global leak detected: i. got mocha error because of the missing 'var' statement", 'neither do it; it passes for me locally so must be just a random travis thing.thanks!', "i see that you are using mocha and expect.js for peer.js. may as well use the same here.i'd be happy to implement tests to catch-up if you like. i'd just need to know what tests should be written up to meet your current expectations. probably a good idea to open an issue for each currently missing test to be sure their fully described and can be discussed.", "rename bender-js assert plugin naming to test-runner. i found that naming yuitest runner an assert plugin is little bit confusing. technically this plugin is a test runner not just assert library. shouldn't be this called `runner` or `test-runner` not `assert`? also the same for configuration option `assertion`. etc.it's an early stage of bender develop so it shouldn't be much work with refactoring - or at least it's easier now when there is only one runner - but there will be other runners like mocha or jasmin.", "well. that's true. but we use both test runner and assertions. so it would be great to find a common name for that kind of plugins... but i can't think of any at the moment. so i guess we'll have to follow the convention from other testing frameworks and rename such to runners.", "i've been thinking about this again and calling it `runner` might be confusing since it also includes assertions. so maybe `framework` fits better here?", 'sounds ok to me. sorry for closing - wrong button.', 'settled on mocha.while there are no actual tests. the test architecture seems to be working and has been pushed to the `mocha` branch along with documentation. so i will close this issue. this will be merged into `dev` very soon (since #35 depends on this being closed). and then hopefully into `master`.separate issues can be created for adding individual tests.', "annetheagile hello there. we are already using mocha for our unit tests. mocha can't be used for performance tests as we would like to have them though.", "new relic is nice for testing certain things. but it's also a bit over complicated. and i'm not certain that it lets us run tests as part of the build.ideally what i'd like is a set of benchmarks for particular parts of the application done using a tailored tool like benchmark or litmus. i'm sure this _can_ be done using mocha. but it seems like we're trying to bend the tool to our will. rather than using something designed for the purpose.", "erisds . thank you for more detail on what you want. so is this ticket a placeholder so sometime you can buy benchmark/litmus [i'm  not familiar with either]? i am a big agile fan. so it seems like it would be nice to use tools we already have [mocha] to get a tiny bit of the value and then see if we can use the data effectively? in the open source spirit. i am not the fastest hacker. but i would really love to try to benchmark a tiny bit of the system with mocha. it seems like you have all the infrastructure for me to try out a bunch of things and then your ci machine will tell me if i got it right. i love that! in any case. perhaps you also know which parts of the system you'd like to check? maybe putting that here would also be a good start.", 'i\'m not sure what you mean by "sometime you can _buy_ benchmark/litmus". they are both free open source tools.if you wanna have a go with mocha. by all means give it a shot. the thing to look at benchmarking. imo would be the api?', 'updated error handling on all mocha tests. - switch to using catch- added error handling where missing', 'package: remove unnecessary pathing. npm automatically includes `~/node_modules/.bin` in the path when running scripts; so there is no need to specify this.it will not revert to the globally installed `mocha` unless the package is missing.', 'thanks for the simplification dcousens', 'i think a massive test refactoring probably deserves its own pull. for instance if we were to switch to mocha then #1054 would be much easier.', 'cool. yeh those are good things to fix. my worry is that any way we do something differently inside the tests gives us more scope for things to break in production but pass in tests. i just tested this and its working good. will merge this so can unblock any amd stuff and start looking how to get rid of the shim and the crufti do want to move test suites. qunits node runner is fustrating. mocha and tape didnt have great in browser runners last time i tried them though', 'switch to mocha. the browser runner is pretty much the same (but looks nicer). the node runner on the other hand actually tells you what tests it uses. as mocha actually includes a qunit shim. the main issue will be dealing with all the weird cruft in the tests. should make #1021 much easier.', "not sure what you mean by passing db names. won't all environments have exactly 2 adapters http and local?", "i havevn't looked into that one much. it uses tape right?", 'yup. test definition is clean (dont really like default mocha specs). its small and hackable. theres a bunch of browser scaffolding already done for us which is nice', 'gonna assign myself for this since i am gonna work on it today and hopefully dont duplicate each others work i am fairly sold on karma being the test runner. some complications but looking into it now. and if karma is the test runner then mocha integrates nicely. qunit might suffice too. i actually like qunit specs. just not the runner. in particularly the node one', 'so theres a little if (node) cruft in these tests. however its not much and there needs to be a pretty hard requirement that we test the actual `pouchdb-nightly.js` package. we may be able to get best of those worlds by browserifying the test code. will take a look at that after i look into the browser runners. any ideas welcome', 'so first problem. between karma and mocha its fairly annoying to be able to pass in the couchdb host', "no problem. colin-- the real issue is a lack of tests. as you mentioned.  tests for assets would be awesome (we're using mocha).  the orm is pretty well covered. but we've still got a lot of coverage to knock out.  even if we just write tests as we fix bugs. that's a great start!  i've starting work on routing tests and i'll continue as i have time", 'update mocha-env-reporter and source-map-support', "mocha intermittent timeouts. we seem to be having intermittent issues with the mocha tests timing out; including the travis ci builds failing due to these timeouts. this has been disrupting the flow of contributions with our pr's builds failing. personally. when i've committed changes to a pr. i have been crossing my fingers that no tests timeout. i suggest to implement suite level timeout increases; described here  this could be a permanent or temporary solution. either way. it may save us a lot of headache when running through the ci builds.i have a branch that i've been testing this with. and it seems to have solved the issue with timeouts. i'm setting the timeout using `this.timeout(10000);` for each server test suite (just once per file). it's a small change to each server test file. i wanted to open the discussion before i submitted a pr. perhaps there is a bigger issue at play here. rather than just latency issues. any thoughts. or concerns over this approach?lirantal ilanbiala codydaig rhutchison trainerbill simison bastianwegge jloveland", 'sgtm', "lirantal ok great. because i'm currently getting frustrated since i'm testing the tests atm :)", "shouldn't this be about creating a sandbox for every test and making sure no requests or timeouts are running after every single test-suite finished rather than just increasing the timeout?", 'bastianwegge can provide an example. or proposed alternative solution?i realize that my proposal may be just a temp solution. right now. these timeouts are reaking havoc on the travis builds. so it would be nice to get something out quickly.', '"testing the tests" - that\'s nice :)', 'mleanos  obviously. we can have both. instead of mochajs default (2000). we can have a default value that fits better meanjs needs. and if needed we can adjust the timeout for a specific test.', "more info:the method containing nodemailer was being called from mocha/chai. after frustration in email. i went on to another part of the application. attempted another library (unirest). and encountered the same issue. not sure how mocha is jacking the callback up. but it is.sorry for the ping. i'll close this.", "unit testing framework for keystonejs projects. it's not clear how to write unit tests for a keystonejs project.we should create a mini-framework for testing that initialises keystone then allows tests to be run (which could be written using any of the available frameworks. e.g. mocha)", '+1the lack of unit tests for the source is one thing that i find makes it difficult to make some of the changes i need.but this is just as important. i want to easily create unit tests for my models. and some more bdd style to drive the browser too. i have found issues for example with using  not liking the tinymce scripts.', "## persian civil engineering centralhi. i'd like to share a google maps link with you. link:---mohamadreza rezaei listrustgmail.", "## persian civil engineering centralsherkate shahrak haye san'ati pasdaran blvd. sari. iran link:---mohamadreza rezaei listrustgmail.", 'converted the tests to run with mocha. as testdata is large its a problem with mocha that saves output until all tests are donei am looking into jasmine too. but i have a feeling we must change the output from the exsisting (woking) run.js to be a format that can be parsed and then fake run the pares results from the test as mocha tests. but then - what is the point of having the mocha to do nothing but formatting the output. i think i make the output from the run.js be a test report in markdown ready to copy paste into the wiki', "that's the idea!  to be fair to you. i think that `ember-cli-test-loader` was only just recently added to npm so using bower like this was the right approach. but. we can do better now!once this lands. i'm going to figure out how to pull in `ember-mocha-adapter` through npm. i feel like it's a bad experience for a user to install an addon and then have their `package.json` or `bower.json` polluted with additional dependencies.  it makes it hard to remove and update things. too. since they never explicitly added them in the first place.a bonus perk of doing this is that. with the community migration away from bower. this addon no longer requires it.  i think it would be smart to eliminate it as a need from the high-profile addons like this one as much as possible.", 'split "chai" code into ember-cli-chai . mocha is often used in combination with chai. but it is not required and some people might prefer other assertion libraries. should we split this library into `ember-cli-mocha` and `ember-cli-chai`?/cc dgeb rwjblue stefanpenner alexlafroscia', 'seems fine to me. but the default out of the box experience should be "it works" imo.', "ehh. i personally feel like that would be unnecessary.  having to install one thing is simpler for most people and using chai with mocha is probably what most people want.  if they want to use another assertion library. they can always do so and just ignore chai. it's not like having extra test assets affect that apps' payload size.", 'alexlafroscia the problem is that we would have to do a major release for every major release of mocha _and_ chai. also we could still add `ember-cli-chai` via blueprint by default.', 'i see. i was hoping that. if we added support for a named amd module to chai. then we could just do the without the wrapper', "in the case of `ember-cli-chai` you'd just have to run `ember install ember-cli-chai` and then you're done.in the case of amd-compatible chai you'd still have to import chai into the test vendor tree somehow and it would be quite a bit more complex for our users. also there wouldn't be a good place to put the instructions on how to do that since they don't really belong in the chai repo itself.", "right. right. kind of a shame it isn't easier to consume an amd-exporting dependency without a wrapper add-on. but you're right about the ease of the `ember install ember-cli-chai` being worth it.", "promise-chain + wd-tractor + mocha-as-promised + chai-as-promised. wow.```    describe('slow rendering'. function() {      beforeeach(function() {        return browser.get(urlroot + 'app/index.html#/repeater');      });      it('should synchronize with a slow action'. function() {        return browser          .elementbyid('addone').click()          .elementbyngrepeater('foo in foos'. 1. '{{foo.b}}')            .text().should.become('14930352')          .elementbyid('addone').click()          .elementbyngrepeater('foo in foos'. 1. '{{foo.b}}')            .text().should.become('24157817');      });    });```", "how to tell grunt-mocha-test to use the mocha.opts file from the test folder?. in mocha you can use a file called `mocha.opts` to provide cli arguments to the mocha process. this file needs to be in the `test` folder.anyway. if i simply put``` javascriptgrunt.initconfig({  mochatest: {    files: ['test/**/*.js']  }});```into my `gruntfile.js`. mocha complains about `suite` not being defined. although the file `mocha.opts` exists and contains the line:```--ui tdd```any ideas on how to solve this?it looks as if mocha. if run by grunt-mocha-test. does not load this file at all.", "first of all: thanks a lot for replying _that_ fast :-)!i understand the issue you have. and your suggestion of using```ui: 'tdd'```works perfectly. the downside is that if you also want to be able to run `mocha` from the command line. you need the additional `mocha.opts` file with the very same options inside.but anyway. it works.thanks for your help :-)!", "your welcome.yeah. i can see the problem. i'm tempted to raise an issue against mocha - it seems to me that attempting to fix that here would be very fragile if mocha ever changed its options (i think this is already a problem with the way i dealt with --require)", "mocha (node) doesn't throw visual errors (exceptions). if while running node.js mocha tests there some exception happens it is not visible it in the testem's output. just information that tests faild.", "i've seen that before. i presume you are using `protocol: tap`?", 'no. just "mocha": {            "command": "mocha -r spec  --colors"   }', 'per-testsuite timeout missing (tests-start - all-tests-results time interval limit). there is a per-launcher process timeout option if i understand that correctly.it would be useful to be able to limit the time of individual test suites (between the tests-start and all-test-results events) which would be agnostic to the test runner used (mocha. jasmine. etc..)', 'what is your motivation behind this?', 'would you mind putting up a gist that demonstrates the bug?', 'what code do you want to see? 1) i install testem globally.2) testem.json: {    "launchers": {        "mocha": {            "command": "mocha -r spec  --colors"        }    }.    "launch_in_dev": ["mocha"]}3) i have folder and one test file test/test.js:describe(\'test\'. function(){    it(\'should pass\'. function(){        "st".indexof1(\'x\')    })})4) if i run testem i have next output:--------------+     mocha    |      x       |              +-----------------------------------------------------------  ? 1 of 1 test failed:  test5) if i run just mocha. the output is: .  ? 1 of 1 test failed:  1) test should pass:     typeerror: object st has no method \'indexof1\'      at context.<anonymous> (so i would like to see testem outputs such errors too.', "thanks for the additional details. i'll take a look.", "> ps. can we use something like thatit looks awkward without modifiers. > mocha allows skip usage in the test body. when you use this.skip() inside a test. it throws a special exception. that aborts the test.it's an option. but it's inconsistent and it's easy to shoot yourself in the leg with that approach.> also. in mocha you do not declare before and after directly in the it test block. you use another describe block.yeah. but we don't have nested fixtures yet. also from my own experience you don't need a new fixture for some tests. you just need some test-specific teardown code and introduction of new fixture feels awkward in that case.", "but this decision shouldn't be taken by mocha. if the user wants to return an integer instead of an error let him to do so. mocha is used for testing. not for validating the user's code correctness. it's my opinion.", "there are many many ways to handle error handling. we're just going with the common/obvious one. this is why promises etc are not supported either. it would cause mocha to become very complex if we support everything.", "i'm hitting the same issue as blackjable: i can connect via node-inspector. but no test script files show up in chrome's debugger.", 'mocha stops `--watch`-ing when a custom `--compilers` produce error.. right now mocha will just outright crash if there is a compilation error in one of the test files.this includes changes during a `--watch` session as well. thus breaking your tdd flow.red -> edit -> green -> edit -> red -> edit -> syntax error -> mocha crashed.mocha should stills watch your files and restart even when there is a compilation error.you can test this with the coffee script compiler.', "+1i've just set up mocha with growl. and it is 99% when working on a small screen.editor takes up 100% of the screen. save - boom. growl says yay or nay.in many cases it's enough with an indication of the reason+line to figure out the error. without having to switch to the test-window.", 'anyone found the solution?', 'git clone the repo and there is the .tmbundle file.', "there's always more than one way to do it.", "i'd drop down to assert() for that sort of thing personally. but closing since its not mocha related", '/me does not test compiled languages. pull-requests accepted', "turns out this issue doesn't concern `--compilers` at all.it is just a simple matter of `require()` error when the test file have a syntax error.", "that would work. coffee script cli has `--nodejs` option which is for that. but mocha right now. accepts `--debug` and `--trace_gc` node options directly. so it's not the way it was done before.", 'i think from technical perspective everything is fine within mocha. also node-inspector is working fine. anyhow the problem is. that node-inspector starts to debug the complete mocha source code. a simple solution would be. that mocha sets a auto-breakpoint at the first test case.', "can support custom reporter for client side unit test. for now. if i use some reporter extensions. it can work well when run server-side unit test. but for run client side unit test. it can't find the correct reporter extensions.such as for mocha-lcov-reporter. when run client side unit test. it can't be loaded at all.please support a way to allow the client side unit test to load the custom reporter extensions.", 'yup you can do `mocha.setup({ reporter: constructorofmyreporter })`', "(how it gets into the browser isn't up to mocha. no magic npm stuff etc)", 'optional mocha.opts path as first argument. node mocha path/mocha.opts', "yes. you're right. but in ie7```script438: object doesn't support property or method 'indexof' mocha.js. line 3850 character 5```", "i still get this error...it's still in mocha.js", "yup. i guess my point about it seeming confusing was in regards to domains not actually catching any errors. which has nothing to do with mocha. it belongs on the node issues. but `domain.on('error')` fails to prevent the error from still being 'uncaught'. which is inconsistent with eventemitter.emit('error') not throwing if there's a listener.", 'that would just be a method reference error. if we can reproduce that with the latest mocha re-open', "yeh. me too. i'm almost done building a mocha-ci server to help with the problem :smile:", "./mocha.js is a build file. you'll need to edit ./lib", 'about runscripts. hello!!im using mocha with zomibie with runscripts = true . it is catching exceptions from external scripts. like facebook box. etc. there are way to run only local scripts?thanks for help!', "> i should also mention that the error appears when trying to use mocha to> test the rest endpoints; my test script (in test/testtest.js. where test is> a peer to server and client) looks like this.that's a completely different issue then. please open a new gh issue and copy your last two comments there. i'll remove them from here once you do that. in order to keep the discussion focused.", "so i took a look into this finally. the way we configure mocha at the last minute actually doesn't work with the slow feature. as test has it's own slow option (mochajs/mocha#402). which the default is cloned when the tests are loaded. so it's too late to set the overall default given our current mechanism of configuring.", "i'm having the same issue.  also using gulp-mocha-phantomjs.", "not true - we are using it in our `gruntfile` at work. remove the `--`:```    mocha_phantomjs: {      all: ['tests.html'].      options: {        'ignore-resource-errors': true      }    }```", 'tracking the long term fix for this in #186', 'the `mocha.opts` file is currenlty ignored by mochify. i think it should be read by mocaccino and the browser code needs to be configured with the values (just like the reporter).care to look into it?', "it sets the timeout for polling the logs. at some point. min-wd thinks the script does not respond anymore. that is what this timeout is.in your case. you want to change the mocha timeout value. it might make sense to couple these values in mochify.reading the `mocha.opts` file should happen in mocaccino since that is where mocha get's configured.", "i'm happy to help on the cucumber-js side. our logic for adding and removing the uncaught exception handler is pretty simple. before each step starts running we add it. after it stops running we remove it. this is very similar to what mocha does.", "appium testsuite bails when beforeeach hook fails. when the beforeeach hook for a single test fails. mocha bails out of the entire suite--in fact. not just the entire suite. but the entire test run. this is a pain because we get through most of the test run and then mocha bails without us knowing what tests didn't run.instead of this. we should somehow mark the individual tests as failed. or at worst. bail out of the specific suite with the bad beforeeach. but the entire test run shouldn't stop.sebv. can you look into this?", 'more information: when i run this without `--recursive` it seems to go to the next `describe` and not bail out entirely. which is what i would expect.', 'various fixes. - upgraded mocha and use mocha from node_modules/.bin- added timeout to uninstallapk in adb and test reset- bugfixes', 'nice. thanks', 'looks like testacular have mocha support these days', "even then. it's only the `gulp mocha` command that triggers the issue. all of the other application commands import properly. i will follow up shortly.", ">  it would be cool if babel could look inside the `package.json` file and find config options therei'm hesitant on adding an additional way to specify options. the goal of `.babelrc` was to consolidate it.> the mocha compiler option doesn't support thisit should. if it doesn't then it's a bug.", "just to clarify. what didn't work was to give babel command line options via the `mocha --compiler` option. using a `.babelrc` works. i would just prefer to have everything at the same place."]
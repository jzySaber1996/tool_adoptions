["if i may pitch in on this last message. specifically the part about decorating the `chai` object:a official way to access an exported value of each plugins from within the test suites. like `chai.plugins.myplugin.<..>` would be nice for programmability.also an export of the various utils. something like `chai.util.inspect()`. this could also hold helpers like `isarray`. `objectdiff` etc so we can use them when debugging failing tests instead of carting them around ourselves (and without dual import issues when writing cross node/browser tests). and maybe it's interesting to think about the ideal way to present a custom multi-line error message. its easy to just pass it in as the regular message but this does clutter reporter 'spec' or 'list': you'd want a single-line in the spec-tree-structure and a detailed report below (like the way the string-diffs and stack trace show up). i'm not sure if this is possible with the current mocha though: i tried mashing things in the actual / expected values and disabling `showdiff` but mocha will always diff strings.", "allow colored/chalked output from tests. i'm in the process of porting my tests from mocha to nightwatch. and discovered that any output written to `console.*` or `process.std*` by my code (not nightwatch itself) is stripped of colors before it reaches my terminal. i tried explicitly setting `disable_colors` to false. but it had no effect.", 'can you provide an example?', "``` jsconst chalk = require('chalk');module.exports = {  'test1': function() {    console.log(chalk.red('hello') + chalk.blue('world'));  }};```digging through the chalk docs. i found that i could run `force_color=1 nightwatch` and get the expected output. so at least there's a workaround.", 'i checked the output of the build process and everything seems to work as expected which is great.unfortunately two things are still broken: - the tests still don\'t run under grunt.- no coverage information is collected.to get the test running again:- in gruntfile.js change `mochaoptions: [\'--compilers\'. \'js:babel/register\'].` to `mochaoptions: [\'--compilers\'. \'js:babel-register\'].`- create `.babelrc` in project root with following contents```{  "presets": [ "es2015" ]}```i have no idea why converage information is not collected.', "ah. i didn't run `grunt test` is isolation. i've updated the gruntfile to use the same configuration file as the other tests use (the `mochahook.js`). i'll look into why coverage is failing.", 'reset total test counter in reporter init. when using mochawesome multiple times in one process - for example tests are executed on web server. then total tests counter is not reseted before each run. first run has correct count of tests. but second round is starting when previous ended - which means statistic is broken.i am a newbie in forks and pull requests. maybe something important is missing now..', 'can you explain your use case a little more?', 'yes.i have written simple http server in node. this server executing mocha tests (tests are part of server code) and collecting mocha results. for test reports. there is mochawesome.but there is a problem. when mocha tests are executed in same process as server. it looks like there is only one counter init / reset - at the server start and total test counter is increasing from this point. what i need is reset this counter for each mocha run in one process. and maybe there should be someone else in future. for who it should be useful too.', "if you are running these tests in a server process aren't the other metrics incorrect as well?", 'total tests. totally passed percentage. totally failed percentage. totally skipped tests.but with reseting counter in init. all these metrics looks good.', "cool. thanks :)we'll deal with the customfds later. i guess (or mocha will?)", "that's how mocha works unfortunately.", 'switch tests to promises. mocha has [support for promises]( i need to go through all the tests and return the horseman chain rather than using the `done` callback.', 'i have done this. because i am neurotic.', 'auto generate api.md using mocha. found a nice way to generate docs from tests. maybe we can combine this with #104', 'cc victorbjelkholm richardlitt', 'oh. that does sound awesome! so we can have really clear and focused tests (better for development) that we also can use to generate the api.md? sounds to good to be true! :)', 'so very excellent :+1:', 'this looks pretty awesome! this just grabs it from the tests file?', 'how do i access this file: `test/tmp-disposable-nodes-addrs.json`?', 'richardlitt not sure what you mean with "access"', "dignifiedquire i updated with your requests. for multiple test files i parse the karma config's files for the mocha task. let me know if you want that handled differently", "it's mocha. throws an error if you pass anything other than an `error` object to `done`.i'll give it a try. cheers.", 'sails leaking globals. running my mocha tests it detected multiple leaks: error: global leaks detected: config. hook. moment. user. project. _. async. sails. both user & project are models and should be "leaked". same goes for sails.  but the rest shouldn\'t be leaked right?', 'use sails.log instead of console.log(). it would be great if we could replace this `console.log()` with a `sails.log`. in my integration tests this `console.log` call messes up the output of the mocha reporter by printing new lines to the console. if its replaces with a `sails.log` i can disable these newlines along with other log output by setting the log level to `error`. thanks.', 'now i see this is not bug restify-plugin or mocha. but script is failed and test not show this.', 'update .travis.yml.. - add `travis_retry`- remove mocha from `npm install`- remove `gem update --system`- add `--no-document` in `gem install`', 'lgtm.', 'ps: i am using grunt-mocha-webdriver ...', 'does this support for mocha?', "filipesilva sorry about the late reply - i think it's actually only available in the mocha plugin. not karma itself.", "this seems like a setup problem with mocha. i can't say why this is not working. but i don't think it is due to my code. can you try again?", "colors option not working. i tried for w hile to get this to work but couldn't. my workflow implied that grunt mocha test is run from a subprocess and color was disabled even with colors: true in the options.i found out that setting``` jsmocha.reporters.base.usecolors = true;```does fix the color rendering.is it possible to patch in some way grunt-mocha-test so it sets this variable accordingly to options.colors?thanks a lot...", "problem about the path of test files. if i cd in the folder where is the js file containing the test. and i execute mocha in this folder. the test will success.if i execute grunt-mocha-test with grunt. the test fails.i think the problem is related to the path.i don't want to move the files containing test cases.how may i solve this issue?", 'did you specify the location of your test files in the gruntfile? maybe you could post your gruntfile and i can take a look', 'do any of your tests run?', 'sorry. my fault. i was forgetting the node_env variable.now the tests fail for an other reason. i will investigate more :-)thank you', 'ok no problem', 'maybe if i could figure out how to test it :s', 'you could set this to true only if options.colors = true.', 'i already have this code```  var mocha = new mocha(params.options);```and this```      if (mocha.options.colors != null) {        mocha.reporters.base.usecolors = mocha.options.colors;      }```and i have not seen any issues with colors - can you supply a more explicit test case?i am wondering if this is an issue with mocha or possibly even by design.', "i've looked into the mocha code and it looks like it disables colors if the process is not running in a tty. it also looks like mocha doesn't allow you to override this behaviour with the colors option which is really only used to disable colors (tbh i'm not sure about this).as such i wouldn't want grunt-mocha-test to treat the colors option differently to the underlying mocha implementation.in my own tests i use subprocesses using exec but colors work as this has valid io streams (i think).", 'ok cool. thanks', 'visionmedia do you have any reaction to the latest change?', 'yes. i suppose some could get furious. but i agree --watch should deprecate.to make it less traumatic someone could create a mocha-watch package by simply combining mocha and nodemon.(that would allow integration with other apps. ides. notifiers. to move to that project)', "the fact that promises are annoying _here_ isn't indicative of promises being annoying; it's just indicative of the impedance mismatch between nodeback `(err. res)` apis and promise apis. because they're not the same. there's some annoyance involved whenever the two need to interact.the most common cases of interaction are encapsulated in the promise libraries to cut down on such annoyance - `q.nfcall` is a prime example of this - but mocha tests are not a common promise/nodeback interaction scenario. mocha's api only expects nodeback interaction. so promise interaction suffers from impedance mismatch. adding support for the promise style. which honestly is not that complicated (even given its need for black magic. `mocha-as-promised` completely achieves support in under 200 lines. and support in mocha core would be _much_ shorter). would eliminate this impedance mismatch.", 'mcollina just came across this thread and was wondering if you had found or implemented anything that solved your issue.  we have the same need.  thanks!', 'geekdave i decided it was too much effort for a very little gain :).', "is there any update on this? stepping through the mocha source line by line until all of my project source files are loaded is terribly painful.i could always not run mocha command-line. instead `require('mocha')` in my own wrapper. but what a waste.", "i'd like this too. we have tests that are only sensible if you have a local identd running. i'd like to be able to skip the tests when we detect that identd is not running.", 'bump.this would be especially useful for the karma js mocha adapter.  currently it reports just results back to karma.  i would like the debug view to show tests in progress using the test reporter. and this would allow that fairly simply.', "i had try to do like your code. but done is 'done'. semantic is not match.so i extend to function.prototype.or may like this:```test('foo'. function (done.dotry) {  myasyncfn(function (err. result) {    dotry(function () {      expect('mocha is good').to.equal('mocha is awesome');      done();    });  });});```", "shouldn't we just allow options to be passed for the reporters and then individually support alternative output types on the reporters?", "modify --watch option: continue when syntaxerror. i want 'mocha --watch' continue when sytaxerror.becase rerun is a little difficultiy. (running happy!)but was stopped 'mocha --watch' when sytaxerror.this modify ...- when command start and sytaxerror then stopped.- when 'mocha --watch' runninng and modify js sytaxerror then running.", 'fb55 i suspect the problem is simply wrong execution of the growl "repoter"``` shfeedy:cssselect felix$ mocha -r growl --watch```should be ``` shfeedy:cssselect felix$ mocha --growl --watch```i do agree however that the api to growl should probably be a reporter just like any other...', "i'm currently working on a test setup using just mocha and chai (without karma & test-bundler). hoping this will fix the issue. i will post updates here once i got it working.", "we switched to using the test bundler because mocha require hooks (`mocha --require js:babel-core/register`) don't allow for a bunch of custom. webpack specific stuff. e.g. with webpack you can import images. but if you do that the tests won't run which is really annoying.the idea is to have a webpack compiled build that's testable. which is what the `test-bundler` achieves. this means that we're using our standard setup which in turn means that whatever works in our apps works in our tests.if you have any better ideas of how to do this feel free to submit a pr. i have yet to figure out a better solution! (the current one was thought up by the amazing jbinto)---to answer to the question at hand. i've sadly never seen that error before. so i can't really help you with it! :confused: since it seems like an environmental problem on your end. i'll close this issue for now--feel free to comment again if you think this is something we can fix from our side!i hope you'll figure it out soon...", "mocha and arrow functions. in your tests you use arrow functions with mocha. but it's not advised by mocha's doc: you know something that i don't?", "til. i didn't know `this.timeout` was a thing in mocha and have never used it before! lets remove all arrow functions from the tests since they're an antipattern!", "oh no. that's a shame i'll close this issue for now. lets revisit this when glob support is in eslint! thanks for notifying me of this. let's keep an eye on this!", "seems really dirty. to be honest. i'd keep it as-is for now. it hasn't killed anybody yet so i don't think it's that critical :grin:", 'in my opinion. i think it should be a centralized. but free to write section. the structure could be on two level. the first one divide the file between every generators used on the project. and inside this a simple key-value store.``` json{  "yeoman-generator" : {    "foo": "bar"  }.  "generator-bbb" : {    "test_framework" : "mocha"  }}```for an api. i\'d like it this way:``` javascriptthis.storage.get("test_framework");this.storage.set("test_framework". "qunit");this.storage.set({  "test_framework": "qunit".  "module_style": "commonjs"});this.storage.getall(); // returning a hash```i think reading the `package.json` would be the best way to determine the name of a generator.', 'running `yo` just list 2 generators : backbone and mochawhere can i find the latest rc ?', 'i updated browsertests so that they work again (tests use mocha now). i can indeed run them on my own machine. but testling.ci is not building anymore. what could be the problem there? the service hook is there.', 'nothing was ever specifically disabled - it should just work.', "ah right. i forgot about that :) i'll remove it shortly.", "i should rephrase that - i was very tired when i wrote it.i've written a chrome plugin and having seen a couple of libraries for converting the chrome api to promises. it seemed like a great time for a rewrite. i unit tested it with jasmine and i feel i can do much better with mocha and sinon.i can see i am perhaps being stupid and that all i need to do is stub the promises returned from your chrome promises library.", 'just merged. thanks!', ':clap: megawac', "ok let's go to the extreme and compile doom to run on the browser. and then you want to use tools like mocha to test it. do you think it is going to work well? you see. ultimately  it's not about what is the language is compiled to. it's all about the stage where humans write it. what you gain in design/tooling. you'll loose it in testing/flexibility. this is an old arguments and to write server code. dynamic language have an history of being better. to write front end app the balance tends to be in favor of strongly typed language. but even in this space the tendency is reversing with languages like rust.i would agree that js is not the easiest language to work with on server side. but in spirit. using transpilers like ts and to a less measure coffeescript is using a different language. the subset of generated javascript  is smaller than what you can write in pure js.", "cannot send?. im not sure if this is sendmail. or my own code. but it's working on my live site right now.  i am trying to run a mocha test with my web app. and i am not getting any feedback from the callback of sendgrid.send({payload}. callback);  i have a console.log() in there. and nothing is happening... and... the email never got sent!.. payload looks fine.", 'i just dont want to upgrade my live api with this code now because it aint working.', 'hmm. is there some config setting in your mocha tests that is different possibly? i just re-ran the libraries tests and hand tested with a demo app i have and the sends are working just fine.', 'can you paste more context of that code - some of the code surrounding it. that might help.', 'let me try running the app itself (and not through the mocha test)...', "kk. i'm about to get on a plane so i might not be able to respond right away", "looks like it worked from within the actual app... that is weird.. look like it's time to debug on my end.... thats weird. thanks.", '> comment originally made by jonathan samines (jn.samines)hi! fraudster is a module which i use to mock a "require" dependency. i removed fraudster. and even tried to execute my test without "gulp mocha" and only using mocha and the --compilers flag. but i got the exactly same error.']
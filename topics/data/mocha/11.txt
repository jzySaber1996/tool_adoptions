["i do find the error log quite easy to understand.```lambda category handler should return the correct number of categories: ```mocha stuff...beginning of the lambda-local logs...```error: errorerror: ------```an error has occurred.```assertion {  __flags:    { ssfi: [function: proxygetter].     lockssfi: undefined.     object: 8.     message: null } }```error log.end of lamba-local logs.```  [?] lambda category handler should return the correct number of categories: 8ms  [?] color lambda should return a list of colors: 4ms  2 passing (19ms)```end of mocha logs.lambda-local has only also printed the error that happens during the execution of your handler. `err`. before passing it to the callback.have you tried to print `err` and see if it's empty or not ?what is weird is that your test pass...", "the reason for `--require-main` is to run as early as possible (to allow modifications before electron's `ready` event). that's why it is loaded before all other mocha options. i'm inclined to leave it this way. but what is the reason for using `--require-main` instead of `--require` in your case?", "it's the same as in 'regular' mocha: it runs your code before the tests start in the process in which mocha runs. so by default. you'd use `--require` as you would in mocha.`--require-main` is for special situations where you need to run something in main either before the ready event. or if you're testing in `--renderer` but still need to run some code in main.", 'indeed not :-/ i just run mocha.edit: fixed.', 'can\'t create breakpoints when in a "[vm]" file. i use mocha to run tests using it\'s `--watch` argument so that i can keep a node-debugger session open as i make changes to the code. when the code changes. mocha reloads and reruns the tests. when it hits a breakpoint node-inspector shows the file that changed in a new tab with a label that starts with "[vm]" (eg. "[vm] basecontroller.js 497"). this works great. except for the fact that i can\'t set breakpoints in this new tab.is this expected behaviour? is there any way we can avoid this. because it makes debugging quite cumbersome.i should add as well that we\'re using traceur to transpile our source code when it\'s `require()`ed.', 'this is in enhancement list - #187', "w33ble it'd be good to integrate eslint-mocha-plugin in the first place. it has that `no-exclusive-tests` rule which is a good one since we kind of rely on `.only` to debug individual tests. they should throw a lint error just to ensure they don't get committed.", 'add handlerejections support. adds #921 however. needs some work because. although this seems like it should work. i don\'t think the tests are right since no log is created in fixtures.it almost feels like it\'s silently not working. but i can\'t figure out how to get past mocha\'s "make sure your promise resolves" in order to test that it rejects. (and handled by winston)hope this is a good starting point for someone. leaving the wip tag to indicate it shouldn\'t be merged without double-checking the testing.', 'thanks for this pr!  looks very nice!   folks have wanted this feature for a while.re: tests. do you have e.g. a plain node.js file you or i can run to see if this works or not (i.e.. outside of mocha or any other test frameworks)?  i can take a look at the mocha stuff but it would be helpful to have some starting point for testing.  thanks again for your work on this!', 'would this work:`sls offline --exec "mocha"`(assuming `mocha` as test runner)', 'either i do not understand what idea grzegorz had or this is not a bug in bender core code. i am not exactly sure what grzegorz meant by "unified among all test frameworks".this bug is actually connected with bender- adapters. like bender-mocha. bender-yui. etc. on results screen. bender displays what has been passed to it throught adapters. while adapters get their data from test frameworks. there is nothing we can do in bender code. i will open appropriate issues in those repositories.i am leaving this issue open until all our adapters are tweaked.', "this is mocha's thing.if you want full stack in errors you have to put this in bender.js configuration:```mocha: {    fulltrace: true}```since this resolves error number 2 (and had to be changed in cke5) and error number 1 is connected with cke5. i am closing this issue.", 'i can get behind it. still. this is benderjs-mocha issue.', "i think we can use self.i don't know exactly how to test it in mocha.under node-webkit we have global and window context. scripts running under window.result: es6-shim will patch global instead of window.", "test: use promises-es6-tests for additional promise tests. the promises-es6-tests suite is a spin off of work that i did for getify 's getify/native-promises-only module.eventually the goal with the tests is to convert them to test262 format and submit them for review and inclusion in test262 for ecmascript.at present they are a set of tests in mocha. using a style and runner very similar to promises-aplus (in fact. stolen from promises-aplus/promises-tests ). but covering es6-draft-specified features such as promise.all. promise.race. etc.es6-shim promises pass all the tests currently in promises-es6-tests./cc ljharb", 'this passes for me on the command line. but it\'d be nice to have a way to also test it in `test/index.html` - when i add the script tag. it fails because of the "require". so it\'d likely need to be browserified.', "i will look into that; i only tested from command line and didn't thinkabout the browser tests.[edited to remove email cruft]", 'added a `bundle/promises-es6-tests.js` to my package. and included it in both kinds of browser tests.  verified that the tests run in browser.still passes on command line.no longer uses caret in package.json.', 'smikes continues to be a test-suite rockstar!', "i checked this pr out. `npm install`ed. and the path `node_modules/promises-es6-tests/bundle/promises-es6-tests.js` didn't exist - so it didn't load in the html file.", "the ci failure is: collections set has an iterator that works with array.from works with the full set:i don't see how that's related to the change i just pushed.  any ideas?", "yes. that seems to be a flaky test. i've opened #268 to track that.", 'closing in favour of visionmedia/mocha#1065', "ummm.. i haven't tested it. but from what it looks like your test starts running before your server is fully set up. `describe('test'` gets started before `app.get` can finish and start the server.put the server setup up in a `before` or `beforeeach` function. set the sql connection there and call `done` when full setup is done.", "js:babel-register doesn't seem to this code. i get an error with default arguments. constructor(url. options = {}) {unexpected token =even if i use the require method. it's still es6 and still errors.i'm running mocha via:mocha --require babel-polyfill --compilers js:babel-register", 'let me take a look into this and see if i can reproduce it.', 'ohhh. thats a good point aearly we should test all the modules after we run the compile step. can we configure the `index.js` file mocha uses to test?', "`support/build.test.js`  is where we test the build right now.  it just does a quick smoke test to verify things seem okay in all the formats we support (single files. monolithic. es6).  if you want to beef up the testing of `dist/async.js` i'd do it there.otherwise. when working on the code. you'd have to run the  build before your ran the mocha tests. a bit of an annoyance.", "i think it's overkill to run the tests on all the various ways we distribute it. (single file. individual modules. single es6 modules. es6 index. etc..)  i think if we verify the build puts thing in the right place. that's good enough. if one full suite of the tests passes.take a second look at `support/build.test.js` -- it makes sure things gets put in their right places.  i'd rather not overcomplicate our general mocha tests.", "the `results` object is specific to the nightwatch runner and so it's not available when using mocha.", '+1 but would also love to see the mocha.grep() command added so we can run individual tests across multiple files.', 'this is an annoying bug.  my workaround is to try to run the test file by itself instead through the runner.  this normally gives me enough syntax to debug the issue.  once i get the error "describe" is undefined (since i use mocha). the test will run.  i shouldn\'t have to do that. but alas. coding...', "i'm afraid not. nightwatch is not involved at that point anymore. the best you can do is to send the string that is in the `describe` by setting it in the `beforeeach` hook. like so:``` jsdescribe('some test'. function() {  var title = this.title;  beforeeach(function(client. done) {     client.options.desiredcapabilities.name = title;     done();  });});```", "tags are a nightwatch specific feature and cannot be easily used with mocha. i'm afraid this is not something we're looking into adding for now.", "tags are a nightwatch specific feature and cannot be easily used with mocha. i'm afraid this is not something we're looking into adding for now. the implementation here only deals with the bdd ui. but mocha has a number of interfaces which won't work.", 'this is a sample `nightwatch.conf.js`:```module.exports = (function(config) {  config.test_settings.mocha = {    "test_runner" : {      "type" : "mocha".      "options" : {        grep : /demo\\stest/      }    }  };  return config;})(require(\'./nightwatch.json\'));```', 'this exist in the mocha-nightwatch module.', 'this exist in the too old glob module used by mocha-nightwatch.', "well. i _was_ a bit surprised that the `configure` calls from different files were invoked successively. it was my understanding that mocha tests are processed serially. but then again. the _tests_ are processed serially. the initialization code within the modules isn't necessarily.so a clean setup code for each test is a must (as i understand it).especially given that `i18n` exists as a global instance and successive `configure` calls always interact with that instance. at least that's what i understood. it was a bit confusing stepping through the code yesterday ;dsomething that i found really weird was the scope you can register to. like. when i pass `register:somescope` to `configure` and i _then_ call `__`. i get to `i18n[method].apply(request. arguments)` from the `applyapitoobject` setup. so the previously supplied `somescope` will become `this` in the following `__` invocation (and. thus. none of the own member variables of `i18n` will be available for access. `getlocalefromobject` will return `undefined`). `__` then calls into `translate`. but `translate` loses the `this` reference (just like `getlocalefromobject`) and is executed in global scope as well.i might not have analyzed it properly. but something weird is going on. if you enable logging and run the existing tests. there are several `no locale found - check the context of the call to __()` messages being logged.", 'fixing failing tests. recent changes within ember changed the behavior of how ember handles errors within testing.  this corrects the behavior until the fix lands either within ember or ember-mocha.', "i'm still ironing out some teething problems to do with tests running before the auth request returns a token. once i have a fully working solution. i'll set up a pr :)edit: i was running `grunt serve` and `grunt watch:mochatest` simultaneously. they appear to have been interfering as they reloaded on save. lesson learnt!", 'refine the structure and build flow. * using webpack 2 now* replaced mocha with jest and enzyme* check lint before commit* support yarn* fixed some bugs......in this vacation.', 'use mocha tap reporter only for travis ci', "mocha unit tests broken by hookfor anglar-xl:repository in app/index.js. ```this.hookfor('angular-xl:repository'. {    args: 'awesome'  });```commenting out the above on ~line 41 allows the tests to pass both locally and on travis.", 'fix for travis build issues in app index. fix for issue #145 mocha unit tests broken by hookfor anglar-xl:repository in app/index.js', "in development. you can append `.only` to an `it` or `describe`. like so:```it.only('something'. function() {  // test});```you can also run just one file with:```sudo gulp install -g mochamocha test/somefile.js ```", "it's not something we're looking at right now. no. i suppose it would need a custom compilation plugin like what we do now for karma. and some other stuff.", 'ping sherlock1982', 'singleton created in \'require:\' option is not available. but works via mocha.opts. first. thanks for grunt-mocha-test-- we use it daily and appreciate the effort that\'s gone into it. in our test suite we are currently using \'require\' mocha.opts to load a file which populates a singleton. which all the tests can then then access. however. when add the equivalent to our gruntfile. it doesn\'t work:```require: \'./test/lib/bootstrap.js\'.```by adding a "console.log" statement to our bootstrap file. i can confirm that grunt is actually finding and loading the file. but the singleton created by that file is not available to other tests as before. i read in another issue that the implementation of \'require\' is "simulated". could this behavior difference be a side effect one of the differences? thanks!', "given that you're including bootstrap.js is this client side code? if so have you been testing client side javascript with mocha (browser stuff) and now using grunt-mocha-test for server side nodejs stuff. if so it could be the difference between how modules are loaded in the browser and nodejs. in nodejs modules are required with the require keyword and globals will only be accessible outside the module if declared without the var keyword. apologies if i'm way off the mark and you already know this stuff.", 'what i\'m doing is all server-side. we just happen to have a file named "bootstrap.js" which needs to run once. before all of our tests. the particular singleton module has a getinstance() method. which looks an object in the module file (but outside of any function) to see if there\'s already a key with a particular name. if not. one. is created. since the file is "required" once. and then stays in memory. the next time the module uses this singleton module and calls getinstance(). it gets the singleton back. this works in other cases. but not with grunt-mocha-test. and i\'m not sure why.', "ahah. i took a look at the code. didn't realize the require was reimplemented. it looks like node_path is not handled. node_path is a special environment variable that is a path to local packages. instead of saying `require('lib/my-module')` or `require('../lib/my-module')`. you can use `node_path=lib` and `require('my-module')`. we should add that to the possible paths to resolve modules to.", "i'm not sure what you mean when you say the require is reimplemented. are you referring to the require option. this is indeed based on the original mocha command line implementation of `--require````program.on('require'. function(mod){  var abs = exists(mod) || exists(mod + '.js');  if (abs) mod = resolve(mod);  requires.push(mod);});```however i don't see how the implementation here would deal with node_path differently. are you trying to require a file with the require option that is referenced relative to node_path?a concrete example (gruntfile and other source) would be helpful in analysing this", 'can you provide an example of this - it will save me time trying to reconstruct one from your description. gruntfile plus sample test should do.', "totally fair. i'll see what i can do.", 'log mocha errors to grunt console. when running mocha from command line. i see this error:(function (exports. require. module. __filename. __dirname) { ui.view_rhmi.widreferenceerror: ui is not definedwhen running the same code via grunt-mocha-test. i see only that the task has failed.', 'update: this only happens when running mochatest:coverage from the examples. with mochatest:test everything is fine and logged.', "it's taken a while for me to figure out the issue here. in the examples `mochatest:coverage` is run in quiet mode as you don't want the coverage report output to console but instead just want to capture it to a file. in the example the idea is to run `mochatest` thus running both tasks. first the tests are run in `mochatest:test` and the coverage data is captured. next the capture data is processed using `mochatest:coverage`. due to the way `mocha` works with the require cache. the tests are actually only run once in this scenario.hope this clears things up for you.", "ok i can reproduce this now but i think it will take some investigation - don't suppose you have any ideas? i still don't know what you mean by reimplementing require.", "hmm i think the problem might be that setting the node_path variable at runtime may not be enough for nodejs. my guess is that it has to be set before the process is started. i have confirmed this by setting node_path before running grunt.as such i'm not sure this is really an issue and more a feature of how nodejs deals with node_path (ie. it adds the path on process initialization and does not check it on every call to `require`)i have tried setting `require.paths` instead but it seems that this functionality has been removed from nodejs:```error: require.paths is removed. use node_modules folders. or the node_path environment variable instead.```running out of ideas now", 'yeah. i was just ignorant of what your require code was doing. i shouldn\'t have said "reimplementing require". i had never seen the internals of node\'s require or any code like it. so i saw you setting up paths and such and assumed you were somehow reworking it.so i assume your task runs on the same process as grunt and does not spawn a new one. right? i only ask because i have a grunt-express-server task that handles node_path fine. but i am thinking maybe that\'s because it spawns a new process. just guessing though.', "yep. i can confirm. you would have to spawn another process using child_process for this to work. with that module you can pass process_env to the child_process and it will treat it as if it were part of the initial environment. i wonder if i could set up mocha in a `grunt-contrib-watch` task and pass the option `spawn: true` to start it in a new process? i'll try that in the morning.", "i'm quite surprised it works like that in grunt watch. unless you're also running `env:test` before running watch.maybe i will add a gotchas section to the readme.adding a spawn option would be a fundamentally different approach and it may make more sense to just wrap the mocha executable in a grunt-shell task.", "yes. i'm running grunt-env before it as well. yeah. unless it's easy to hook up with grunt.util.spawn. using another grunt task seems like the way to go.", 'it also occurred to me that `grunt-shell` could also be used to spawn a grunt sub process while setting the environment too. after all you may not always want to use watch', "automated integration tests are really important to a lot of application teams. you could argue that service-level integration tests probably don't need to run in multiple browsers. and teams that are doing this kind of testing can do it via mocha or jasmine today. but then they'd have to configure testem _and_ another framework if they want to use testem for their unit tests.i'm also interested in using testem as a vehicle to manage cross-browser functional testing. but i can't do anything along those lines without the proxy to let me talk to my web server.", 'thank you mirisuzanne. a note for other contributors. i used the mocha integration code. jest uses `describe` and `it` as well. the true tests do run however jest does not detect it as an integration test and gives me that error above.', "i've thought about how to get mocha working with `node` but it would get dirty since people require `mocha` to run the test runner programmatically and requiring `mocha` to get a test runner set up in the background for you would conflict with that.i could probably get it to work with `require('mocha/describe')` and have that bootstrap a mocha test runner with the default settings if the program was not run with the `mocha` executable but run with `node` directly. that way the programmatic `require('mocha')` api to create and run your own test runners won't be effected.", "run mocha programmatically with harmony. i would like to run my tests using harmony programmatically.as far as i understand it is currently not possible.it should be possible to pass it in as an option as follows:var mocha = require('mocha').  mocha = new mocha({    harmony: true.  })", "after reading this. as well as tj's reasoning for not supporting promises. i'd like to offer my :+1: to this pr.although mocha can be made to work with async tasks. as discussed here. it is certainly not elegant. and. it's not that js is short of async functions that inclusion of promises would be an overkill. i hope that it's included in the future. so that ugly hacks are not required. :)", "prefer `process.stdout.write` in xunit reporter.. falls back to `console.log` if `process.stdout` is unavailable.this is another stab at fixing #1068 without breaking existing use cases. it works for me on my own ~1500 test suite in both chrome and through mocha-phantomjs. and the mocha self-tests pass.i was having an issue with mocha-phantomjs not picking up the reporter output when used with the `--file` options because of the use of `console.log`. switching to `process.stdout.write` (when it is available) fixes the problem for me.this also seems to have picked up a completely unrelated change to another file that hadn't yet made it into the mocha.js snapshot. (see diff.) is this sort of thing a regular occurrence?", '+1 - very good pr imho', ':+1: raynos docs are in the gh-pages branch.', "travisjeffery thanks! i've added a pr into the gh-pages to document this feature.", "+1 to the idea and to using duck-typing on .then instead of adding additional method names.however one thing this doesn't handle is the case where the promise is rejected with something that's not an error. domenic handles null or undefined in mocha-as-promised. but not the case where reject is called with something else like a string. imo this should also be detected and turned into an error.", "> domenic handles null or undefined in mocha-as-promised. but not the case where reject is called with something else like a string. imo this should also be detected and turned into an error.hmm interesting point. the only reason i handle null and undefined specially is because otherwise mocha assumes it's a success. otherwise i pass through any non-error values to mocha. which yells at the user for me.", "copy and paste the mocha.js you have and send a link to a project reproducing the problem. mocha's working. it's something wrong on your end and i can't help you without any info.", 'i\'d prefer if mocha simply supported all node\'s arguments instead of trying to filter out the "bad" ones.', "support --harmony in mocha.opts. my code works only with `node --harmony`. so i can't run my tests simply by running `mocha`.```$ mocha  error: unknown option `--harmony'```", 'you have to use `mocha --harmony`', 'remove invalid optimization in checkglobals().. fixes #1015there was an attempt at an optimization in checkglobals that would short circuit globals checking if the number of tolerated globals was equal (correcting for a couple of expected differences) to the number of actual globals.  this is completely invalid since you could tolerate a new global in your mocha config and the system under test could add a different global but not the new global you are tolerating.  (or you could tolerate two new ones. and two new unexpected ones could be added by the system under test. etc.)  in these cases. the unexpected globals will not be reported by mocha and tests will pass when they should not.  so basically the code was assuming that the system under test would definitely add any globals that were added to the list of tolerated globals through mocha config.i added a test for the simplest failure case. saw it fail. and then removed the offending code.', "documentation around glob patterns and shell expansion. not sure if it is needed. but it may be worth noting that due to shell expansion. glob patterns should be wrapped in single quotes on unix shells.`mocha -w -r should 'test/**/*.js'`using that would render the --recursive flag useless.", 'visionmedia any comment on this? i believe this was affecting other people as well. so it seems it would be worth carrying the fix. unless you had a different approach in mind?', "`should.js` relies on `try`+`catch` of `assertionerror`'s for negative assertions. unfortunately. this doesn't play out nice with `mocha.throwerror` because the error will be reported to mocha even if you catch it later.", 'vduggirala $ mocha test.js --timeout 30000', "the original pr's api is consistent to how the rest of mocha api looks.", '+1 promises are now in the standard. like it or not. they are here now.my promise-based code is a pain to test with mocha. throwing in a rejection callback is just not working. as exceptions are caught in promise internal code. failed tests end up timing out...', "this is a small but incredibly useful change. are there any remaining concerns preventing it from being merged?i've been following it through a couple incarnations and have been using it for strongloop's internal ci for a while.", 'this is still broken for me as well.  it caches files outside of the tests.', '`mocha.generator = co` or sth', "can't see this being mocha's issue. it's not compiling to js properly", 'i always wondered. can\'t this "coffee-script/register" thingy be specified in the comments section in the source file itself? something like a shebang or modeline. but interpreted by mocha itself when requiring...', "lucassus that example works fine when you only have one nested level of derived values. but if you tried to make `kind` depend on another variable set in an inner `context` there is no combination of `before` and `beforeeach` that will make it work.visionmedia. to pose dpehrson's question again: are you opposed to having `let` or something like it in mocha? i'm investigating whether it's possible to build `let` outside of mocha using the api it provides. if it isn't. would you at least be amenable to a pr that adds the api required to allow a `let` that is not included in mocha?", "i don't think it's necessary to add to mocha directly personally. much like express's `app.configure()` was really just a glorified `if` statement. it raised more questions than it was worth. even though it does look a bit nicer", "i don't see any reason to have it in mocha", "that's unfortunate. it prevents mocha from scaling to larger applications with more complex test suites.", "visionmedia that's fair. as to my second question. would you be opposed to (hopefully minimal) changes to mocha's api to allow someone to build it outside of mocha?", "ah. yeah i see. definitely not something that should be in mocha. when it's effectively just:``` jsvar container = function(){  var memo;  return function(){    return memo || (memo = new ember.container());  }}();```which is easy in user-land and has no downside. or just using beforeeach with `this.container = new stuff` would be similar", "so i've come around to the idea that tap is a terrible format and you need some kind of json object format.:+1:", 'the other thing i would add to my list of requirements for reporting is that success or failure should be detected/indicated purely through the use of the exit code.  i.e. it should be possible to have a failing test. but overall state that tests pass. and visa versa.  all cis should depend purely on the exit code.', "alissa-ferro this is out of detox's scope.  the test runner's job to decide if it should bail or continue to test the other tests in the suite. if you want tests to continue running remove the `--bail` from your `mocha.opts`", "the current docs are correct. we don't use callbacks to notify mocha we're done. we use async-await instead.", 'so actually. this is how i currently run the test suite`node_modules/.bin/mocha e2e --opts e2e/mocha.opts  --configuration android.emu.release --grep :ios: --invert --loglevel verbose`', "discussion: migration to jest. jest has recently gained a lot of attention after significant improvements that were made to it. i think it would we should migrate towards it. i've had a number of issues with mocha. builds arent parallelized. async/await isnt supported. and it requires much more configuration out of the box. the migration wont be a lot of work because its api is very similar to that of mocha. it also comes with support for react out of the box.", "ia khochu v neio perenesti tol'ko kostyli `if window.mocha`. da. navernoe eto ne dolzhna byt' ne sborka. a prosto nekotorye veshchi nuzhno delat' v stub'akh", '``` bash  303 passing (8s)  1 failing  1) cli does not throw when being executed:     error: timeout of 2000ms exceeded      at null.<anonymous> (c:\\projects\\modernizr\\node_modules\\mocha\\lib\\runnable.js:158:19)      at timer.listontimeout (timers.js:119:15)```unrelated failure. afaict.', "what to do mocha tests time out. i didn't find any error in test.", 'i feel like that might not quite be the correct behavior for the use-case of providing a "done" function.given the documentation link above. the use-case documented is that you wait for an element to be available in the dom. according to the above comment. however. it sounds like this will run the callback argument simply if there are no more events in the event-chain to trigger. regardless of whether or not this done argument returns true.i think either the documentation should be updated to make this clear. or (and this is my preference) the logic-of-events should be this: if there is a done function. run it until it returns true or the waitduration is reached (regardless of whether or not there are events in the event-chain). if it does not return true during the waitduration. the callback function should be passed an error explaining that the duration was reached and the done function was false.this would allow people to use it in tests to wait for a condition to be met before trying to make assertions about the page they are on. (which is what i am trying to do. and what the documentation shows as a use-case.)', "the point of the completion function is to tell `wait` there's no point in processing more events. this is opposed to trying to use timeouts. so you get dependable tests that wrap up faster. it's an advisory. not an assertion.all assertions should come after the `wait`. not during it.", "i feel like that is typically the purpose of a callback (to do something after the called function workload is done. and not during it).given your above explanation. i still feel like the documentation needs updated. it should probably make it clear that it will fire the callback when the event-chain is empty. or when the done callback returns true - whichever happens first (and that is the important part.) it should say something about the event-chain being empty superseding the provided done function. this is the misleading paragraph:```you can also tell the browser to wait for something to happen on the page by passing a function as the first argument. that function is called repeatedly with the window object. and should return true (or any value equal to true) when it's time to pass control back to the application.```that doesn't appear to be true if zombie detects that the event-chain is empty.do you have an example of how i would achieve my above use-case? should i simply loop until my condition is met or my mocha test times itself out?thanks for all the help. by the way! i was (obviously) totally misunderstanding the purpose of this function.", '`foreach(fn)` vs `file.readfile(filename. fn)`. the only convention is that. when you have a continuation callback. if an error occurred during processing. you pass it as the first argument. otherwise it\'s undefined. try throwing an error from the completion function. you\'ll get an error in the callback.> `... something to happen on the page ...`> that doesn\'t appear to be true if zombie detects that the event-chain is empty.the way "something happens on the page" happens is. an event gets queued and then the browser processes it (which may in turn queue more events). when the browser runs out of events to process. we reach a state in which "something happens on the page" is never going to happen.if there\'s a disagreement about what\'s going to happen (you say something. the browser insists it\'s nothing). you may want to look at the reason for disagreement first.', 'comment updated to reflect. more accurately. what i am seeing.', "there is no rendering. so no dom layout ticks. but all dom updates will complete within an event.also. you can't `wait` on a websocket or sse at the moment. there's an open issue related to that.", 'ah! maybe that is my real issue then... my application connects the socket. and then fills out the html page with the data it gets over-the-wire. which i was trying to wait for with the above examples...', 'so. with the fact that you cannot wait for socket events. does that also mean that socket events which trigger dom layout changes are also not registered?i am seeing the same exact html from `console.log(browser.html());` before my test is run. and after the `pressbutton` is called with a `settimeout(function () { console.log(browser.html()); }. 5000);` in the `pressbutton` callback...', 'ok. thanks for all the help with this!', "option to silent stdout. or at least the yeoman ascii art when testing generator. i want to isolate my rspec output during an 'npm test' of my generator. so i'd like an option to hide the output of yeoman during the multiple test runs. notice. this is not `npm test > /dev/null` because i want mocha's output. just not the ascii art and file list.sorry if this is basic. but a search for stdout/tests for yeoman revealed alot of noise about how it used to check stdout instead of fs in tests. and not an answer to how to silence it.", 'simply create a custom environment and mock methods that are outputting content to the console (like `adapter#log`).', "hemanth is assigned on this issue and should bring something up sometime :)btw. don't use `--silent` - that was a bad idea as it would break usual ui with prompts. better implement a global flag like `yeoman_mute` or something similar... up to you.", 'sboudrias sure. will work on it.', "yes. it's reported on the latest. hunting down that `undefined` :cactus: **update 0:**  `$ mocha <any_test>.js --reporter list --timeout 50000` is resulting in the same.", 'live web site with authentication. is it possible to run `mocha-phantomjs` on a site which requires authentication (for example form based auth. or windows)?', "may be i'm not good at explanation. sorry. yes. i need to run tests in a browser on some site. but this site requires windows authentication.  thanks for your advice. will take a look into different options.", 'healyje. you should have much more logging with debug environment variable. i see you are using mocha. and os x. if you are running a single test. please run your test like this ```debug=* mocha test/mytest.js```that should give you a lot more output.', "healyje in this instance. if you are not getting an error when running a single test. i've seen this before. and someone reported similar issue and resolved it by creating own test bootstrap. mocha doesnt seem to be sandboxing all of the individual tests. which is causing conflicts. i would have to mess with this a bit. hopefully will get to it this weekend.", 'report colours. how do you modify the report colours?i use the solarized theme in my terminal (urxvt) and the report colour 90 is the exact same colour as the background. so any text that is this colour is invisible.  this appears to be a very common problem and has quite a bit of coverage on the mocha issues pages.  i have found the easiest way to fix this is to simply require colors from mocha/lib/reporters/base and change any colours set to 90 to 92 (or whatever) at the start of the tests.  however. when using mochify. this hack does not work. can you let me know how i need to modify this hack to make it work with mochify?thanks.', 'the `mocha` global is available in mochify. this allows you to modify `mocha.reporters.base.colors`.alternatively. you can disable color entirely with `--no-colors` on the command line. or `{ colors : false }` in the api.', 'that was easy. thanks.', "in <code>test/server.spec.js</code>. after i commented line81-84. ```serviceaccount: {    'private_key': 'fake'.    'client_email': 'fake'}```which is the serviceaccount part. everything seems to work fine with command `mocha`  but i'm not sure whether this is the point. just my findings.", "this would fix a bug when using babel-register with mocha. right now mocha doesn't provide a way to set things like ignore/only. the `.babelrc` does. but without this fix babel-register does not read these values from the `.babelrc`."]